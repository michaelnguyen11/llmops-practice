{
    "artifact_data": [
        {
            "id": "4eb69ac6-f8d3-4766-8e49-0246f9d2e9a8",
            "content": "Building ML System Using the FTI Architecture Introduction to the feature training inference FTI design pattern to build scalable and modular ML systems using MLOps best practices. SubscribeSign in Share this post Decoding ML Building ML systems the right way using the FTI architecture Copy link Facebook Email Notes More Building ML systems the right way using the FTI architecture The fundamentals of the FTI architecture that will help you build modular and scalable ML systems using MLOps best practices. Paul Iusztin Aug 10, 2024 16 Share this post Decoding ML Building ML systems the right way using the FTI architecture Copy link Facebook Email Notes More 2 Share This article presents the feature training inference FTI architecture to build scalable and modular ML systems using MLOps best practices. Jim Dowling, CEO at Hopsworks, proposed the design 1, 2 . We will start by discussing the problems of naively building ML systems. Then, we will examine other potential solutions and their problems. Ultimately, we will present the feature training inference FTI design pattern and its benefits. We will also understand the benefits of using a feature store and model registry when architecting your ML system. The problem with building ML systems Building production ready ML systems is much more than just training a model. From an engineering point of view, training the model is the most straightforward step in most use cases. However, training a model becomes complex when deciding on the correct architecture and hyperparameters. That s not an engineering problem but a research problem. At this point, we want to focus on how to design a production ready architecture. Training a model with high accuracy is extremely valuable, but just by training it on a static dataset, you are far from deploying it robustly. We have to consider how to ingest, clean and validate fresh data training vs. inference setups compute and serve features in the right environment serve the model in a cost effective way version, track and share the datasets and models monitor your infrastructure and models deploy the model on a scalable infrastructure automate the deployments and training These are the types of problems an ML or MLOps engineer must consider, while the research or data science team is often responsible for training the model. Figure 1 Components of an ML system. Photo from the Google Cloud Architecture documents Figure 1 shows all the components the Google Cloud team suggests that a mature ML and MLOps system requires. Along with the ML code, there are many moving pieces. The rest of the system comprises configuration, automation, data collection, data verification, testing and debugging, resource management, model analysis, process and metadata management, serving infrastructure, and monitoring. The point is that there are many components we must consider when productionizing an ML model. _Thus, the critical question is How do we connect all these components into a single homogenous system ?_ We must create a boilerplate for clearly designing ML systems to answer that question. Similar solutions exist for classic software. For example, if you zoom out, most software applications can be split between a database, business logic and UI layer. Every layer can be as complex as needed, but at a high level overview, the architecture of standard software can be boiled down to these three components. Do we have something similar for ML applications? The first step is to examine previous solutions and why they are unsuitable for building scalable ML systems. The issue with previous solutions In Figure 2, you can observe the typical architecture present in most ML applications. It is based on a monolithic batch architecture that couples the feature creation, model training, and inference into the same component. By taking this approach, you quickly solve one critical problem in the ML world the training serving skew. The training serving skew happens when the features passed to the model are computed differently at training and inference time. In this architecture, the features are created using the same code. Hence, the training serving skew issue is solved by default. This pattern works fine when working with small data. The pipeline runs on a schedule in batch mode, and the predictions are consumed by a third party application such as a dashboard. Figure 2 Monolithic batch pipeline architecture Unfortunately, building a monolithic batch system raises many other issues, such as features are not reusable by your system or others if the data increases, you have to refactor the whole code to support PySpark or Ray hard to rewrite the prediction module in a more efficient language such as C , Java or Rust hard to share the work between multiple teams between the features, training, and prediction modules impossible to switch to a streaming technology for real time training In Figure 3, we can see a similar scenario for a real time system. This use case introduces another issue in addition to what we listed before. To make the predictions, we have to transfer the whole state through the client request so the features can be computed and passed to the model. Consider the scenario of computing movie recommendations for a user. Instead of simply passing the user ID, we must transmit the entire user state, including their name, age, gender, movie history, and more. This approach is fraught with potential errors, as the client must understand how to access this state, and it s tightly coupled with the model service. Another example would be when implementing an LLM with RAG support. The documents we add as context along the query represent our external state. If we didn t store the records in a vector DB, we would have to pass them with the user query. To do so, the client must know how to query and retrieve the documents, which is not feasible. It is an antipattern for the client application to know how to access or compute the features. If you don t understand how RAG works, we will explain it in future chapters. Figure 3 Stateless real time architecture In conclusion, our problem is accessing the features to make predictions without passing them at the client s request. For example, based on our first user movie recommendation example, how can we predict the recommendations solely based on the user s ID? Remember these questions, as we will answer them shortly. The solution the FTI architecture The solution is based on creating a clear and straightforward mind map that any team or person can follow to compute the features, train the model, and make predictions. Based on these three critical steps that any ML system requires, the pattern is known as the FTI feature, training, inference pipelines. So, how does this differ from what we presented before? The pattern suggests that any ML system can be boiled down to these three pipelines feature, training, and inference similar to the database, business logic and UI layers from classic software . This is powerful, as we can clearly define the scope and interface of each pipeline. Also, it s easier to understand how the three components interact. As shown in Figure 4, we have the feature, training and inference pipelines. We will zoom in on each of them and understand their scope and interface. Before going into the details, it is essential to understand that each pipeline is a different component that can run on a different process or hardware. Thus, each pipeline can be written using a different technology, by a different team, or scaled differently. The key idea is that the design is very flexible to the needs of your team. It acts as a mind map for structuring your architecture. Figure 4 Feature Training Inference FTI pipelines architecture The feature pipeline The feature pipelines take as input data and output features labels used to train the model. Instead of directly passing them to the model, the features and labels are stored inside a feature store. Its responsibility is to store, version, track, and share the features. By saving the features into a feature store, we always have a state of our features. Thus, we can easily send the features to the training and inference pipeline s . As the data is versioned, we can always ensure that the training and inference time features match. Thus, we avoid the training serving skew problem. The training pipeline The training pipeline takes the features and labels from the features store as input and outputs a train model or models. The models are stored in a model registry. Its role is similar to that of feature stores, but this time, the model is the first class citizen. Thus, the model registry will store, version, track, and share the model with the inference pipeline. Also, most modern model registries support a metadata store that allows you to specify essential aspects of how the model was trained. The most important are the features, labels and their version used to train the model. Thus, we will always know what data the model was trained on. The inference pipeline The inference pipeline takes as input the features labels from the feature store and the trained model from the model registry. With these two, predictions can be easily made in either batch or real time mode. As this is a versatile pattern, it is up to you to decide what you do with your predictions. If it s a batch system, they will probably be stored in a database. If it s a real time system, the predictions will be served to the client who requested them. As the features, labels, and model are versioned. We can easily upgrade or roll back the deployment of the model. For example, we will always know that model v1 uses features F1, F2, and F3, and model v2 uses F2, F3, and F4. Thus, we can quickly change the connections between the model and features. Benefits of the FTI architecture To conclude, the most important thing you must remember about the FTI pipelines is their interface The feature pipeline takes in data and outputs features labels saved to the feature store. The training pipelines query the features store for features labels and output a model to the model registry. The inference pipeline uses the features from the feature store and the model from the model registry to make predictions. It doesn t matter how complex your ML system gets. These interfaces will remain the same. Now that we better understand how the pattern works, we want to highlight the main benefits of using this pattern as you have just three components, it is intuitive to use and easy to understand each component can be written into its tech stack, so we can quickly adapt them to specific needs, such as big or streaming data. Also, it allows us to pick the best tools for the job as there is a transparent interface between the three components, each one can be developed by a different team if necessary , making the development more manageable and scalable every component can be deployed, scaled, and monitored independently. The final thing you must understand about the FTI pattern is that the system doesn t have to contain only three pipelines. In most cases, it will include more. For example, the feature pipeline can be composed of a service that computes the features and one that validates the data. Also, the training pipeline can be composed of the training and evaluation components. The FTI pipelines act as logical layers. Thus, it is perfectly fine for each to be complex and contain multiple services. However, what is essential is to stick to the same interface on how the FTI pipelines interact with each other through the feature store and model registries. By doing so, each FTI component can evolve differently, without knowing the details of each other and without breaking the system on new changes. Conclusion In this article, we understood the fundamental problems when naively building ML systems. We also looked at potential solutions and their downsides. Ultimately, we presented the FTI architecture, its benefits, and how to apply it to modern ML systems. This article was inspired by our latest book , _ LLM Engineer s Handbook. _ If you liked this article, consider supporting our work by buying our book and getting access to an end to end framework on how to engineer LLM RAG applications, from data collection to fine tuning, serving and LLMOps LLM Engineer s Handbook LLM Engineer s Handbook Cover References Literature 1 Jim Dowling, From MLOps to ML Systems with Feature Training Inference Pipelines 2023 , Hopsworks blog 2 Jim Dowling, Modularity and Composability for AI Systems with AI Pipelines and Shared Storage 2024 , Hopsworks blog Images If not otherwise stated, all images are created by the author. 16 Share this post Decoding ML Building ML systems the right way using the FTI architecture Copy link Facebook Email Notes More 2 Share PreviousNext Discussion about this post Comments Restacks Top Latest Discussions No posts Ready for more? Subscribe 2024 Paul Iusztin Privacy Terms Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Notes More This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts en",
            "platform": "decodingml.substack.com",
            "author_id": "c7167a83-e446-428d-9a2e-6e88a6b8bc6f",
            "author_full_name": "Hiep Nguyen",
            "link": "https://decodingml.substack.com/p/building-ml-systems-the-right-way?r=1ttoeh"
        },
        {
            "id": "f9f6fa8b-ce11-44ac-9d03-11990f6d8944",
            "content": "LLM Agents Demystified by Li Decoding ML Hands on ReAct Agent implementation with AdalFlow library SubscribeSign in Share this post Decoding ML LLM Agents Demystified Copy link Facebook Email Notes More LLM Agents Demystified Hands on ReAct Agent implementation with AdalFlow library Li Jul 27, 2024 14 Share this post Decoding ML LLM Agents Demystified Copy link Facebook Email Notes More 1 Share Hi, all! I m Li Yin, Author of AdalFlow and ex AI researcher MetaAI Find me on LinkedIn Handy links AdalFlow Github Open in Colab _AdalFlow is an LLM library that not only helps developers build but also optimizes LLM task pipelines. Embracing a design pattern similar to PyTorch, AdalFlow is light, modular, and robust, with a 100 readable codebase._ _There are many tutorials that show users how to call high level agent APIs, but none of them explain how it really works in depth. This is where the AdalFlow library aims to make a difference._ _In this blog, you will not only learn how to use the ReAct Agent but more importantly, also understand how it was implemented and how you can customize or build your own agent with AdalFlow._ _Let s get started!_ _Image source , credits to Growtika_ Introduction _ An autonomous agent is a system situated within and a part of an environment that senses that environment and acts on it, over time, in pursuit of its own agenda and so as to effect what it senses in the future. _ _ Franklin and Graesser 1997 _ Alongside the well known RAGs, agents 1 are another popular family of LLM applications. What makes agents stand out is their ability to reason, plan, and act via accessible tools. When it comes to implementation, AdalFlow has simplified it down to a generator that can use tools, taking multiple steps sequential or parallel to complete a user query. Table of Contents 1. What is ReAct Agent 2. Introduction on tools function calls 3. ReAct Agent implementation 4. ReAct Agent in action 1 . What is ReAct Agent ReAct 2 is a general paradigm for building agents that sequentially interleaves thought, action, and observation steps. Thought The reasoning behind taking an action. Action The action to take from a predefined set of actions. In particular, these are the tools functional tools we have introduced in tools. Observation The simplest scenario is the execution result of the action in string format. To be more robust, this can be defined in any way that provides the right amount of execution information for the LLM to plan the next step. Prompt and Data Models _The prompt is the most straightforward way to understand any LLM application. Always read the prompt._ AdalFlow uses jinja2 syntax for the prompt. DEFAULT_REACT_AGENT_SYSTEM_PROMPT is the default prompt for the React agent s LLM planner. We can categorize the prompt template into four parts 1. Task description This part is the overall role setup and task description for the agent. task_desc r You are a helpful assistant.Answer the user s query using the tools provided below with minimal steps and maximum accuracy.Each step you will read the previous Thought, Action, and Observation execution result of the action and then provide the next Thought and Action. 2. Tools, output format, and example This part of the template is exactly the same as how we were calling functions in the tools. The output_format_str is generated by FunctionExpression via JsonOutputParser . It includes the actual output format and examples of a list of FunctionExpression instances. We use thought and action fields of the FunctionExpression as the agent s response. _You will be easily visualize the whole pipeline later by simply_ print react . tools r if tools TOOLS for tool in tools loop.index . tool endfor TOOLS endif output_format_str 3. Task specification to teach the planner how to think . We provide more detailed instruction to ensure the agent will always end with finish action to complete the task. Additionally, we teach it how to handle simple queries and complex queries. For simple queries, we instruct the agent to finish with as few steps as possible. For complex queries, we teach the agent a divide and conquer strategy to solve the query step by step. task_spec r TASK_SPEC For simple queries Directly call the finish action and provide the answer. For complex queries Step 1 Read the user query and potentially divide it into subqueries. And get started with the first subquery. Call one available tool at a time to solve each subquery subquestion. At step finish , join all subqueries answers and finish the task. Remember Action must call one of the above tools with name. It can not be empty. You will always end with finish action to finish the task. The answer can be the final answer or failure message. TASK_SPEC We put all these three parts together to be within the SYS SYS tag. 4. Agent step history. We use StepOutput to record the agent s step history, including action This will be the FunctionExpression instance predicted by the agent. observation The execution result of the action. In particular, we format the steps history after the user query as follows step_history r User query input_str Step History if step_history STEPS for history in step_history Step loop.index . Thought history.action.thought , Action history.action.action , Observation history.observation endfor STEPS endif You 2 . Introduction on tools function calls In addition to the tools provided by users, by default, we add a new tool named finish to allow the agent to stop and return the final answer. def finish answer str str Finish the task with answer. return answer Simply returning a string might not fit all scenarios, and we might consider allowing users to define their own finish function in the future for more complex cases. Additionally, since the provided tools cannot always solve user queries, we allow users to configure if an LLM model should be used to solve a subquery via the add_llm_as_fallback parameter. This LLM will use the same model client and model arguments as the agent s planner. Here is our code to specify the fallback LLM tool _additional_llm_tool Generator model_client model_client, model_kwargs model_kwargs if self.add_llm_as_fallback else None def llm_tool input str str I answer any input query with llm s world knowledge. Use me as a fallback tool or when the query is simple. use the generator to answer the query try output GeneratorOutput _additional_llm_tool prompt_kwargs input_str input response output.data if output else None return response except Exception as e log.error f Error using the generator e print f Error using the generator e return None 3 . ReAct Agent implementation We define the class ReActAgent to put everything together. It will orchestrate two components planner A Generator that works with a JsonOutputParser to parse the output format and examples of the function calls using FunctionExpression . ToolManager Manages a given list of tools, the finish function, and the LLM tool. It is responsible for parsing and executing the functions. Additionally, it manages step_history as a list of StepOutput instances for the agent s internal state. Prompt the agent with an input query and process the steps to generate a response. 4 . ReAct Agent in action We will set up two sets of models, llama3 70b 8192 by Groq and gpt 3.5 turbo by OpenAI, to test two queries. For comparison, we will compare these with a vanilla LLM response without using the agent. Here are the code snippets from lightrag.components.agent import ReActAgent from lightrag.core import Generator, ModelClientType, ModelClient from lightrag.utils import setup_env setup_env Define tools def multiply a int, b int int Multiply two numbers. return a b def add a int, b int int Add two numbers. return a b def divide a float, b float float Divide two numbers. return float a b llama3_model_kwargs model llama3 70b 8192 , llama3 70b works better than 8b here. temperature 0.0, gpt_model_kwargs model gpt 3.5 turbo , temperature 0.0, def test_react_agent model_client ModelClient, model_kwargs dict tools multiply, add, divide queries What is the capital of France? and what is 465 times 321 then add 95297 and then divide by 13.2? , Give me 5 words rhyming with cool, and make a 4 sentence poem using them , define a generator without tools for comparison generator Generator model_client model_client, model_kwargs model_kwargs, react ReActAgent max_steps 6, add_llm_as_fallback True, tools tools, model_client model_client, model_kwargs model_kwargs, print react for query in queries print f Query query agent_response react.call query llm_response generator.call prompt_kwargs input_str query print f Agent response agent_response print f LLM response llm_response print The structure of React using print react , including the initialization arguments and two major components tool_manager and planner . You can visualize the structure from our colab. Now, let s run the test function to see the agent in action. test_react_agent ModelClientType.GROQ , llama3_model_kwargs test_react_agent ModelClientType.OPENAI , gpt_model_kwargs Our agent will show the core steps for developers via colored printout, including input_query, steps, and the final answer. The printout of the first query with llama3 is shown below without the color here 2024 07 10 16 48 47 react.py 287 call input_query What is the capital of France? and what is 465 times 321 then add 95297 and then divide by 13.2 2024 07 10 16 48 48 react.py 266 _run_one_step Step 1 StepOutput step 1, action FunctionExpression thought Let s break down the query into subqueries and start with the first one. , action llm_tool input What is the capital of France? , function Function thought None, name llm_tool , args , kwargs input What is the capital of France? , observation The capital of France is Paris! _______ 2024 07 10 16 48 49 react.py 266 _run_one_step Step 2 StepOutput step 2, action FunctionExpression thought Now, let s move on to the second subquery. , action multiply a 465, b 321 , function Function thought None, name multiply , args , kwargs a 465, b 321 , observation 149265 _______ 2024 07 10 16 48 49 react.py 266 _run_one_step Step 3 StepOutput step 3, action FunctionExpression thought Now, let s add 95297 to the result. , action add a 149265, b 95297 , function Function thought None, name add , args , kwargs a 149265, b 95297 , observation 244562 _______ 2024 07 10 16 48 50 react.py 266 _run_one_step Step 4 StepOutput step 4, action FunctionExpression thought Now, let s divide the result by 13.2. , action divide a 244562, b 13.2 , function Function thought None, name divide , args , kwargs a 244562, b 13.2 , observation 18527.424242424244 _______ 2024 07 10 16 48 50 react.py 266 _run_one_step Step 5 StepOutput step 5, action FunctionExpression thought Now, let s combine the answers of both subqueries. , action finish answer The capital of France is Paris! and the result of the mathematical operation is 18527.424242424244. , function Function thought None, name finish , args , kwargs answer The capital of France is Paris! and the result of the mathematical operation is 18527.424242424244. , observation The capital of France is Paris! and the result of the mathematical operation is 18527.424242424244. _______ 2024 07 10 16 48 50 react.py 301 call answer The capital of France is Paris! and the result of the mathematical operation is 18527.424242424244. The comparison between the agent and the vanilla LLM response is shown below Answer with agent The capital of France is Paris! and the result of the mathematical operation is 18527.424242424244. Answer without agent GeneratorOutput data I d be happy to help you with that! n nThe capital of France is Paris. n nNow, let s tackle the math problem n n1. 465 321 149,485 n2. Add 95,297 to that result 149,485 95,297 244,782 n3. Divide the result by 13.2 244,782 13.2 18,544.09 n nSo, the answer is 18,544.09! , error None, usage None, raw_response I d be happy to help you with that! n nThe capital of France is Paris. n nNow, let s tackle the math problem n n1. 465 321 149,485 n2. Add 95,297 to that result 149,485 95,297 244,782 n3. Divide the result by 13.2 244,782 13.2 18,544.09 n nSo, the answer is 18,544.09! , metadata None The ReAct agent is particularly helpful for answering queries that require capabilities like computation or more complicated reasoning and planning. However, using it on general queries might be an overkill, as it might take more steps than necessary to answer the query. 5 . Optional Customization Please refer to our tutorial for how to customize ReAct to your use case. References 1 A survey on large language model based autonomous agents Paitesanshi LLM Agent Survey 2 ReAct https arxiv.org abs 2210.03629 3 Tool Tutorial https lightrag.sylph.ai tutorials tool_helper.html API References components.agent.react.ReActAgent core.types.StepOutput components.agent.react.DEFAULT_REACT_AGENT_SYSTEM_PROMPT 14 Share this post Decoding ML LLM Agents Demystified Copy link Facebook Email Notes More 1 Share PreviousNext A guest post by LiAuthor of AdalFlow, Founder at SylphAI, ex AI researcher at MetaAI. Github liyin2015 Subscribe to Li Discussion about this post Comments Restacks Top Latest Discussions No posts Ready for more? Subscribe 2024 Paul Iusztin Privacy Terms Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Notes More This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts en",
            "platform": "decodingml.substack.com",
            "author_id": "c7167a83-e446-428d-9a2e-6e88a6b8bc6f",
            "author_full_name": "Hiep Nguyen",
            "link": "https://decodingml.substack.com/p/llm-agents-demystified?r=1ttoeh"
        },
        {
            "id": "ca8a920f-d86b-43fe-88d7-becab64f550c",
            "content": "Data Ingestion Architecture for ML and Marketing Intelligence Building a highly scalable data collection pipeline for AI, ML and marketing intelligence leveraging the AWS cloud, Python, data crawling, and Docker. SubscribeSign in Share this post Decoding ML Highly Scalable Data Ingestion Architecture for ML and Marketing Intelligence Copy link Facebook Email Notes More Highly Scalable Data Ingestion Architecture for ML and Marketing Intelligence Leveraging AWS Ecosystem and Data Crawling for Scalable and Adaptive Data Pipelines Rares Istoc Jun 27, 2024 14 Share this post Decoding ML Highly Scalable Data Ingestion Architecture for ML and Marketing Intelligence Copy link Facebook Email Notes More 3 Share Today s article is written by our guest , Rares Istoc , a veteran with over 7 years of experience building scalable software and data engineering systems in the industry. Here is his LinkedIn. Machine learning without data is like a chef without ingredients all the skills but nothing to cook. These days, everything circulates around data, from personalized ads to streaming recommendations. Data drives decisions in business, healthcare, and sports. Without it, apps would be clueless, smart devices would be dumb, and predictions would be nothing more than guesses. In this digital age, data is the lifeblood of innovation and efficiency. Ok, but why another article about data ingestion? There are many ways to build data ingestion pipelines, and with all the new tools created over the last decade, selecting the best ones can be challenging. The answer often depends on your project s specific needs. In this article, you ll explore an end to end solution for marketing intelligence. Using AWS s ecosystem, you can create a scalable data ingestion pipeline for data crawling and integrate it into various analytical processes like sales, competitor analysis, market analysis, and customer insights. I ll also present the challenges encountered while building this solution. Finding a complete working solution is tough, with most answers scattered across the Internet. You can access the full solution code on GitHub . _ IMPORTANT NOTE Before diving into this solution, you must be aware of the legal implications of ingesting data from some data sources, like social media pages, so we can make sure nobody goes to jail. Please read the terms and conditions of each major platform these will restrict you from crawling user profiles and private pages._ Table of Contents 1. Architecture Overview 2. Implementation 3. Challenges Pitfalls 4. Local Testings 5. Deployment 1 . Architecture Overview This is what we are about to build Here are some non functional requirements I ve aimed to achieve with this architecture Scalability The solution can process many pages simultaneously and easily add more, handling growth at any time. Maintainability Adaptability Each component is designed for easy modification and expansion without significant development time. Components Overview Scheduler Triggers crawler lambdas for each page link. Crawler Extracts various posts and information from the page link. If unfamiliar with crawling, look it up before proceeding. Details will follow in the implementation part. Database MongoDB is used for our data lake storage, housing posts for later use. It excels at handling semi structured data. The complete flow the scheduler triggers a crawler lambda for each page, sending the page name and link. The crawler extracts posts from the past week, storing the raw content, creation date, link, and name. The scheduler waits for all lambdas to finish, aggregates the posts from the database, and sends them to ChatGPT using prompt templates to generate reports. 2 . Implementation In this section, I ll provide a detailed overview of the main components, breaking them down with code samples and explanations. 2.1. Scheduler I ll not focus much on the reporting part, though you can find it here along with all the code shared in this article. The main focus is the scheduling part, the entry point of the system where the flow starts and is orchestrated import json import os import time from datetime import datetime, timedelta import boto3 from aws_lambda_powertools import Logger from aws_lambda_powertools.utilities.typing import LambdaContext from src.constants import PAGE_LINK from src.db import database from src.utils import monitor logger Logger service decodingml scheduler _client boto3.client lambda def lambda_handler event, context LambdaContext correlation_ids for link in PAGE_LINK response _client.invoke FunctionName lambda , InvocationType Event , Payload json.dumps link link , logger.info f Triggered crawler for link correlation_ids.append response ResponseMetadata RequestId logger.info f Monitoring len correlation_ids crawler processes while True time.sleep 15 completed monitor correlation_ids correlation_ids c for c in correlation_ids if c not in completed if not correlation_ids break logger.info f Still waiting for len correlation_ids crawlers to complete now datetime.now posts list database.profiles.find date gte now timedelta days 7 , lte now , logger.info f Gathered len posts posts if not posts logger.info Cannot generate report, no new posts available return reports generate_profiles_report posts logger.info Generated new report! The scheduler acts as a scatterer, iterating over a list of page links and invoking a crawler asynchronously with the InvocationType parameter set to Event, ensuring the scheduler won t block for a single page. It stores each lambda s correlation ID in a list and waits for all lambdas to finish, with a 15 second wait time, adjustable based on your crawler s average completion time. Finally, it finds all crawled posts and sends them to the report generation phase. 2.2. Crawler Here I ll break down the actual crawling process import abc import os from datetime import datetime, timedelta from itertools import takewhile, dropwhile from typing import List, Dict, Any import instaloader from src.crawlers.base import BaseAbstractCrawler class BaseAbstractCrawler abc.ABC abc.abstractmethod def extract self, link str, kwargs None ... class InstagramCrawler BaseAbstractCrawler def __init__ self, link str, proxy None self.link link self.loader instaloader.Instaloader self._until datetime.now self._since self._until timedelta days 7 self._proxy proxy def extract self, kwargs List Dict str, str Any parsed_url urlparse self.link if self._proxy os.environ https_proxy self._proxy.__dict__ .get http profile instaloader.Profile.from_username self.loader.context, parsed_url.path.strip .split 0 posts takewhile lambda p p.date self._since, dropwhile lambda p p.date self._until, profile.get_posts return content post.caption, date post.date, link self.link for post in posts I ve defined a main abstraction point for all crawlers, establishing a common interface that all derived crawlers must implement. Each subclass must provide its implementation for the extract method, ensuring reusability and uniformity. import re from src.crawlers.base import BaseAbstractCrawler from src.crawlers.instagram import InstagramCrawler class CrawlerDispatcher def __init__ self None self._crawlers def register self, domain str, crawler type BaseAbstractCrawler None self._crawlers r https www . ? .com .format re.escape domain crawler def get_crawler self, url str BaseAbstractCrawler for pattern, crawler in self._crawlers.items if re.match pattern, url return crawler else raise ValueError No crawler found for the provided link dispatcher CrawlerDispatcher dispatcher.register instagram , InstagramCrawler To promote and call each crawler automatically, I ve built a dispatcher that selects and instantiates the correct crawler class based on the provided link. This acts as a registry and factory for the crawlers, managed under a unified interface and structure. Advantages Flexibility Scalability Allows easy addition of new domains and specialized crawlers without modifying the existing codebase. Encapsulation Modularity The dispatcher encapsulates the logic for determining which crawler to use, making the system modular and allowing each crawler to focus on its core business logic. from datetime import datetime, timedelta from aws_lambda_powertools import Logger from aws_lambda_powertools.utilities.typing import LambdaContext from src.crawlers import dispatcher from src.db import database logger Logger service decodingml crawler def lambda_handler event, context LambdaContext link event.get link logger.info f Start extracting posts for link crawler dispatcher.get_crawler event.get link posts page, correlation_id context.aws_request_id for page in crawler.extract now datetime.now existing_posts database.profiles.find date gte now timedelta days 7 , lte now , name link , projection date 1 existing_posts post.get date for post in list existing_posts posts post for post in posts if post.get date not in existing_posts if not posts logger.info No new posts on page return logger.info f Successfully extracted len posts posts database.profiles.insert_many posts logger.info f Successfully inserted data in db The main entry point assembles the link from the event body, selects the correct crawler, and starts extraction jobs. After extraction, it checks for existing posts to avoid duplicates and adds new posts to the database. 3 . Challenges Pitfalls 3.1. Running headless browser instance with selenium in lambda runtime environment This caused the most headaches. The Lambda execution environment is read only, so writing to disk requires using a temporary file, complicating automatic binary driver installation. Therefore, you need to install the driver directly in the Docker image and reference it manually in Selenium s driver options. The only usable driver for this setup was the Google binary driver in my case. FROM public.ecr.aws lambda python 3.11 as build Download chrome driver and browser and manually unpack them in their folders RUN yum install y unzip curl Lo tmp chromedriver linux64.zip https edgedl.me.gvt1.com edgedl chrome chrome for testing 119.0.6045.105 linux64 chromedriver linux64.zip curl Lo tmp chrome linux64.zip https edgedl.me.gvt1.com edgedl chrome chrome for testing 119.0.6045.105 linux64 chrome linux64.zip unzip tmp chromedriver linux64.zip d opt unzip tmp chrome linux64.zip d opt FROM public.ecr.aws lambda python 3.11 Install the function s OS dependencies using yum RUN yum install y atk cups libs gtk3 libXcomposite alsa lib libXcursor libXdamage libXext libXi libXrandr libXScrnSaver libXtst pango at spi2 atk libXt xorg x11 server Xvfb xorg x11 xauth dbus glib dbus glib devel nss mesa libgbm ffmpeg libxext6 libssl dev libcurl4 openssl dev libpq dev COPY from build opt chrome linux64 opt chrome COPY from build opt chromedriver linux64 opt COPY . pyproject.toml . poetry.lock . Install Poetry, export dependencies to requirements.txt, and install dependencies in the Lambda task directory, finally cleanup manifest files. RUN python3 m pip install upgrade pip pip install poetry RUN poetry export f requirements.txt requirements.txt pip3 install no cache dir r requirements.txt target LAMBDA_TASK_ROOT rm requirements.txt pyproject.toml poetry.lock Copy function code COPY . src LAMBDA_TASK_ROOT src The main idea in this Dockerfile is that I manually downloaded the Chrome driver and browser and unpacked them in a location where they can be accessed by Selenium, which usually would ve done this directly. This is a mandatory step for the Lambda environment. Since everything is read only, in the next code sample I ll show you how point Selenium to the correct driver and browser locations from tempfile import mkdtemp def init_driver self options Options Setup drover binary location manually options.binary_location opt chrome chrome Run browser in headless mode options.add_argument headless new options.add_argument no sandbox options.add_argument single process options.add_argument window size 1420,1080 options.add_argument disable dev shm usage options.add_argument disable gpu options.add_argument disable popup blocking options.add_argument disable notifications options.add_argument disable dev tools options.add_argument log level 3 options.add_argument ignore certificate errors options.add_argument no zygote options.add_argument f user data dir mkdtemp options.add_argument f data path mkdtemp options.add_argument f disk cache dir mkdtemp options.add_argument remote debugging port 9222 self._driver webdriver.Chrome service Service opt chromedriver , options options, I hardcoded the driver and browser locations in the Dockerfile. Additionally, I pointed several folders e.g., user data dir, disk cache dir to temporary directories to prevent Selenium from creating them automatically, which would cause errors due to Lambda s disk limitations. 3.2. Aggregate Empty Pages My initial monitoring algorithm was basic, looping over lambda invocation correlation IDs and checking the database for generated posts. However, it encountered an infinite loop when no new posts were created for some pages. import datetime import re from typing import List import boto3 _client boto3.client logs def monitor correlation_ids List str finished now int datetime.datetime.now datetime.timedelta days 1 .timestamp 1000 response _client.filter_log_events logGroupName aws lambda crawler , startTime now, filterPattern REPORT RequestId for event in response events match re.search r REPORT RequestId s , event.get message if match correlation_id match.group 1 if correlation_id in correlation_ids finished.append correlation_id return finished Here, I search through all log streams for each lambda generated in that current day and look for the message, which usually has this format _ REPORT RequestId _ correlation_id . This indicates that the lambda has reached the end of its execution, and I can mark which correlation IDs have finished. 3.3. Avoid being blocked by social media platforms This was a pity error the kind you would ve spent days on and the solution was to watch it from a different perspective. Popular social media platforms implement many anti bot protection mechanisms to prevent crawling, from request header analysis to rate limiting to IP blocking. And because we run our browser in headless mode to mimic realistic user browser interaction, and all our crawlers send requests under the same IP address to multiple pages at the same time repeatedly, this screams, please block me. To address this, I ve used a proxy to mask my IP address and location import os class ProxyConnection def __init__ self, host str None, port str None, username str None, password str None, verify_ssl bool False self.host host or os.getenv PROXY_HOST self.port port or os.getenv PROXY_PORT self.username username or os.getenv PROXY_USERNAME self.password password or os.getenv PROXY_PASSWORD self.verify_ssl verify_ssl self._url f self.username self.password self.host self.port def __dict__ self return https https .format self._url.replace , , http http .format self._url.replace , , no_proxy localhost, 127.0.0.1 , verify_ssl self.verify_ssl To address this, I used a proxy to mask my IP and location. Paid proxies like SmartProxy offer a pool of rotating IPs, assigning a different IP to each crawler, mimicking regular user behavior. Additionally, using a proxy allows finding a country without access restrictions to public pages, ensuring smooth crawling. 4 . Local Testings To prove this works, I wrote a makefile containing some simple commands for crawler and lambda. The problem is that I ve only managed to test the crawler locally. Since the scheduler spins up crawlers, they should be already deployed on AWS. local test crawler Send test command on local to test the lambda curl X POST http localhost 9000 2015 03 31 functions function invocations d link https www.instagram.com mcdonalds local test scheduler Send test command on local to test the lambda curl X POST http localhost 9000 2015 03 31 functions function invocations d Now, most people, when testing lambda functions on a local environment, use AWS Lambda RIE Runtime Interface Emulator , which allows you to test your lambda function packages in a container. Basically, this emulates a lambda execution environment on your local machine. As you can see, I ve managed to do this without using the emulator, which slightly simplified my environment. You can use these commands to test each component. For example, if you would like to test the crawler, go into your terminal and use this command make local test crawler As you can see, the crawling process has started, and for this page, we ve found three new posts in the last seven days 5 . Deployment The deployment process is defined in our GitHub repository under the ops folder, where you can explore the whole solution written in Pulumi. You can play with the Makefile. It contains all the necessary commands to make your infrastructure up and running. Conclusion In this article, we ve explored a complete end to end robust solution for building a Highly Scalable Data Ingestion pipeline that can leverage existing data from multiple crawlable sources for various processes like ML training, data analysis, etc. We ve gone through specific challenges you might face and how to overcome them in this process. _ Check out the code on GitHub 1 and support us with a _ Within our newsletter, we keep things short and sweet. If you enjoyed reading this article, consider checking out the full version on Medium. It s still free Full article on Medium Images If not otherwise stated, all images are created by the author. 14 Share this post Decoding ML Highly Scalable Data Ingestion Architecture for ML and Marketing Intelligence Copy link Facebook Email Notes More 3 Share PreviousNext Discussion about this post Comments Restacks Top Latest Discussions No posts Ready for more? Subscribe 2024 Paul Iusztin Privacy Terms Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Notes More This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts en",
            "platform": "decodingml.substack.com",
            "author_id": "c7167a83-e446-428d-9a2e-6e88a6b8bc6f",
            "author_full_name": "Hiep Nguyen",
            "link": "https://decodingml.substack.com/p/highly-scalable-data-ingestion-architecture?r=1ttoeh"
        },
        {
            "id": "fa6f9632-346b-429b-b6b9-bca52edff722",
            "content": "The LLM Twin Free Course on Production Ready RAG applications. Learn how to build a full end to end LLM RAG production ready system, follow and code along each component by yourself. SubscribeSign in Share this post Decoding ML The LLM Twin Free Course on Production Ready RAG applications. Copy link Facebook Email Notes More The LLM Twin Free Course on Production Ready RAG applications. Learn how to build a full end to end LLM RAG production ready system, follow and code along each component by yourself. Alex Razvant Jun 20, 2024 13 Share this post Decoding ML The LLM Twin Free Course on Production Ready RAG applications. Copy link Facebook Email Notes More 3 Share the last lesson of the LLM Twin free course What is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality, and voice into an LLM. Decoding ML Newsletter is a reader supported publication. If you enjoy our work, please consider becoming a paid subscriber. Subscribe Image by DALL E Why is this course different? _By finishing the LLM Twin Building Your Production Ready AI Replica _ _free course, you will learn how to design, train, and deploy a production ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices_. _ Why should you care? _ _ No more isolated scripts or Notebooks! Learn production ML by building and deploying an end to end production grade LLM system._ _More details on what you will learn within the LLM Twin course , here _ The LLM Twin Free Course This course teaches you how to design, build, and deploy a production ready LLM RAG system. It covers all the components, system design, data ingestion, streaming pipeline, fine tuning pipeline, inference pipeline alongside production monitoring, and more. What is the course about? We re building a production ready RAG system, able to write content based on your unique style, by scrapping previous posts articles and code snippets written by you to construct a fresh and continuously updated knowledge base, generate a dataset to fine tune a capable and efficient open source LLM, and then interconnect all components for a full end to end deployment while integrating evaluation and post deployment monitoring. This course follows best MLOps LLMOps practices, focusing on the 3 pipeline design pattern for building ML centered applications. Lesson 1 Presenting the Architecture Presenting and describing each component, the tooling used, and the intended workflow of implementation. The first lesson will prepare the ground by offering a wide overview of each component and consideration. We recommend you start here. Lesson 1 An End to End Framework for Production Ready LLM Systems by Building Your LLM Twin LLM twin system architecture Image by the Author Lesson 2 Data Pipelines In this lesson, we ll start by explaining what a data pipeline is, and the key concepts of data processing and streaming, and then dive into the data scrapping and processing logic. Lesson 2 The Importance of Data Pipelines in the Era of Generative AI Lesson 2 The Data Collection Pipeline Image by author Lesson 3 Change Data Capture and Data Processing In this lesson, we re showcasing the CDC Change Data Capture integration within the LLM Twin data pipeline. We re showing how to set up MongoDB, the CDC approach for event driven processing, RabbitMQ for message queuing, and efficient low latency database querying using the MongoDB Oplog. Lesson 3 CDC Enabling Event Driven Architectures Lesson 3 Event Driven Processing using RabbitMQ, CDC, and MongoDB Image by Author Lesson 4 Efficient Data Streaming Pipelines In this lesson, we ll focus on the feature pipeline. Here, we re showcasing how we ingest data that we ve gathered in the previous lesson, and how we ve built a stream processing workflow with Bytewax that fetches raw samples, structures them using Pydantic Models, cleans, chunks, encodes, and stores them in our Qdrant Vector Database. Lesson 4 SOTA Python Streaming Pipelines for Fine tuning LLMs and RAG in Real Time! Lesson 4 Efficient Data Streaming Pipelines using Bytewax and Qdrant Vector DB. Image by Author Lesson 5 Advanced RAG Optimization Techniques In this lesson, we ll showcase a few advanced techniques to increase the similarity and accuracy of the embedded data samples from our Qdrant Vector Database. The contents of this lesson could make a significant difference between a naive RAG application and a production ready one. Lesson 5 The 4 Advanced RAG Algorithms You Must Know to Implement Lesson 5 Advanced RAG Optimization Techniques. Image by Author Lesson 6 Dataset preparation for LLM fine tuning In this lesson, we ll discuss the core concepts to consider when creating task specific custom datasets to fine tune LLMs. We ll use our cleaned data from our Vector Database, and engineer specific Prompt Templates alongside using GPT3.5 Turbo API to generate our custom dataset and version it on Comet ML . Lesson 6 The Role of Feature Stores in Fine Tuning LLMs Lesson 6 Generate custom datasets using Knowledge Distillation. Lesson 7 Fine tuning LLMs on custom datasets We ll show how to implement a fine tuning workflow for a Mistral7B Instruct model while using the custom dataset we ve versioned previously. We ll present in depth the key concepts including LoRA Adapters, PEFT, Quantisation, and how to deploy on Qwak. Lesson 7 How to fine tune LLMs on custom datasets at Scale using Qwak and CometML Lesson 7 Fine tuning LLMs on custom datasets using Qwak and CometML. Image by Author Lesson 8 Evaluating the fine tuned LLM In this lesson, we re discussing one core concept of ML Evaluation . We ll present the evaluation workflow we ll showcase the full process of assessing the model s performance using the GPT3.5 Turbo model and custom engineered evaluation templates. Lesson 8 Best Practices When Evaluating Fine Tuned LLMs Lesson 8 Evaluating the quality of our custom fine tuned LLM. Image by Author Lesson 9 Deploying the Inference Pipeline Stack In this lesson, we ll showcase how to design and implement the LLM RAG inference pipeline based on a set of detached Python microservices. We ll split the ML and business logic into two components, describe each one in part, and show how to wrap up and deploy the inference pipeline on Qwak as a scalable and reproducible system. Lesson 9 Architect scalable and cost effective LLM RAG inference pipelines Lesson 9 Architecturing LLM RAG inference pipeline. Image by Author Lesson 10 RAG Pipeline Evaluation In this lesson, we re covering RAG evaluation which is one of great importance. If no proper evaluation metrics are monitored or techniques are used, the RAG systems might underperform and hallucinate badly. Here, we ll describe the workflow of evaluating RAG pipelines using the powerful RAGAs framework, compose the expected RAGAs evaluation format, and capture eval scores which will be included in full LLM execution chains and logged on Comet ML LLM . Lesson 10 Evaluating RAG Systems using the RAGAs Framework Lesson 10 Evaluating the RAG pipeline. Image by Author Next Steps Step 1 Check out the full versions of all Lessons 1 11 on our Medium publication , under the LLM Twin Course group tag. _It s still FREE _ The LLM Twin Course Step 2 Check out theLLM Twin GitHub repository and try it yourself _Nothing compares with getting your hands dirty and building it yourself!_ LLM Twin Course GitHub Images If not otherwise stated, all images are created by the author. 13 Share this post Decoding ML The LLM Twin Free Course on Production Ready RAG applications. Copy link Facebook Email Notes More 3 Share PreviousNext Discussion about this post Comments Restacks Top Latest Discussions No posts Ready for more? Subscribe 2024 Paul Iusztin Privacy Terms Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Notes More This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts en",
            "platform": "decodingml.substack.com",
            "author_id": "c7167a83-e446-428d-9a2e-6e88a6b8bc6f",
            "author_full_name": "Hiep Nguyen",
            "link": "https://decodingml.substack.com/p/the-llm-twin-free-course-on-production?r=1ttoeh"
        },
        {
            "id": "39c8cd0e-56b6-4369-8890-aeeccb4fbcf4",
            "content": "Architect LLM RAG inference pipelines by Paul Iusztin Design, build, deploy and monitor LLM and RAG inference pipelines using LLMOps best practices. Integrate it with a model registry and vector DB. SubscribeSign in Share this post Decoding ML Architect scalable and cost effective LLM RAG inference pipelines Copy link Facebook Email Notes More Architect scalable and cost effective LLM RAG inference pipelines Design, build and deploy RAG inference pipeline using LLMOps best practices. Paul Iusztin Jun 06, 2024 13 Share this post Decoding ML Architect scalable and cost effective LLM RAG inference pipelines Copy link Facebook Email Notes More 2 Share the 9th out of 11 lessons of the LLM Twin free course Why is this course different? _By finishing the LLM Twin Building Your Production Ready AI Replica _ _free course, you will learn how to design, train, and deploy a production ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices_. _ Why should you care? _ _ No more isolated scripts or Notebooks! Learn production ML by building and deploying an end to end production grade LLM system._ _More details on what you will learn within the LLM Twin course , here _ Latest Lessons of the LLM Twin Course Lesson 6 The Role of Feature Stores in Fine Tuning LLMs Custom Dataset Generation, Artifact Versioning, GPT3.5 Turbo Distillation, Qdrant Lesson 7 How to fine tune LLMs on custom datasets at Scale using Qwak and CometML QLoRA, PEFT, Fine tuning Mistral 7b Instruct on custom dataset, Qwak, Comet ML Lesson 8 Best practices when evaluating fine tuned LLM models LLM Evaluation techniques Does and don ts, Quantitive and manual LLM evaluation techniques Lesson 9 Architect scalable and cost effective LLM RAG inference pipelines In Lesson 9, we will focus on implementing and deploying the inference pipeline of the LLM twin system. First , we will design and implement a scalable LLM RAG inference pipeline based on microservices, separating the ML and business logic into two layers. Secondly , we will use Comet ML to integrate a prompt monitoring service to capture all input prompts and LLM answers for further debugging and analysis. Ultimately , we will deploy the inference pipeline to Qwak and make the LLM twin service available worldwide. Context from previous lessons. What you must know. This lesson is part of a more extensive series in which we learn to build an end to end LLM system using LLMOps best practices. _If you haven t read the whole series, for this one to make sense, you have to know that we have a _ Qdrant vector DB populated with digital data posts, articles, and code snippets vector DB retrieval module to do advanced RAG fine tuned open source LLM available in a model registry from Comet ML _ In this lesson, we will focus on gluing everything together into a scalable inference pipeline and deploying it to the cloud._ Table of Contents 1. The architecture of the inference pipeline 2. The training vs. the inference pipeline 3. The RAG business module 4. The LLM microservice 5. Prompt monitoring 6. Deploying and running the inference pipeline 7. Conclusion 1 . The architecture of the inference pipeline Our inference pipeline contains the following core elements a fine tuned LLM a RAG module a monitoring service Let s see how to hook these into a scalable and modular system. The interface of the inference pipeline As we follow the feature training inference FTI pipeline architecture, the communication between the 3 core components is clear. Our LLM inference pipeline needs 2 things a fine tuned LLM pulled from the model registry features for RAG pulled from a vector DB which we modeled as a logical feature store This perfectly aligns with the FTI architecture. _ If you are unfamiliar with the FTI pipeline architecture, we recommend you reviewLesson 1 s section on the 3 pipeline architecture._ Monolithic vs. microservice inference pipelines Usually, the inference steps can be split into 2 big layers t he LLM service where the actual inference is being done the business service domain specific logic We can design our inference pipeline in 2 ways. Option 1 Monolithic LLM business service In a monolithic scenario, we implement everything into a single service. _Pros _ easy to implement easy to maintain _Cons _ harder to scale horizontally based on the specific requirements of each component harder to split the work between multiple teams not being able to use different tech stacks for the two services Monolithic vs. microservice inference pipelines Option 2 Different LLM business microservices The LLM and business services are implemented as two different components that communicate with each other through the network, using protocols such as REST or gRPC. _Pros _ each component can scale horizontally individually each component can use the best tech stack at hand _Cons _ harder to deploy harder to maintain Let s focus on the each component can scale individually part, as this is the most significant benefit of the pattern. Usually, LLM and business services require different types of computing. For example, an LLM service depends heavily on GPUs, while the business layer can do the job only with a CPU. Microservice architecture of the LLM twin inference pipeline Let s understand how we applied the microservice pattern to our concrete LLM twin inference pipeline. As explained in the sections above, we have the following components 1. A business microservice 2. An LLM microservice 3. A prompt monitoring microservice The business microservice is implemented as a Python module that contains the advanced RAG logic, which calls the vector DB and GPT 4 API for advanced RAG operations calls the LLM microservice through a REST API using the prompt computed utilizing the user s query and retrieved context sends the prompt and the answer generated by the LLM to the prompt monitoring microservice. As you can see, the business microservice is light. It glues all the domain steps together and delegates the computation to other services. The end goal of the business layer is to act as an interface for the end client. In our case, as we will ship the business layer as a Python module, the client will be a Streamlit application. However, you can quickly wrap the Python module with FastAPI and expose it as a REST API to make it accessible from the cloud. Microservice architecture of the LLM twin inference pipeline The LLM microservice is deployed on Qwak. This component is wholly niched on hosting and calling the LLM. It runs on powerful GPU enabled machines. How does the LLM microservice work? It loads the fine tuned LLM twin model from Comet s model registry 2 . It exposes a REST API that takes in prompts and outputs the generated answer. When the REST API endpoint is called, it tokenizes the prompt, passes it to the LLM, decodes the generated tokens to a string and returns the answer. That s it! The prompt monitoring microservice is based on Comet ML s LLM dashboard. Here, we log all the prompts and generated answers into a centralized dashboard that allows us to evaluate, debug, and analyze the accuracy of the LLM. 2 . The training vs. the inference pipeline Along with the obvious reason that the training pipeline takes care of training while the inference pipeline takes care of inference Duh! , there are some critical differences you have to understand. The input of the pipeline How the data is accessed Do you remember our logical feature store based on the Qdrant vector DB and Comet ML artifacts? If not, consider checking out Lesson 6 for a refresher. The core idea is that during training , the data is accessed from an offline data storage in batch mode, optimized for throughput and data lineage. Our LLM twin architecture uses Comet ML artifacts to access, version, and track all our data. The data is accessed in batches and fed to the training loop. During inference , you need an online database optimized for low latency. As we directly query the Qdrant vector DB for RAG, that fits like a glove. During inference, you don t care about data versioning and lineage. You just want to access your features quickly for a good user experience. The data comes directly from the user and is sent to the inference logic. The training vs. the inference pipeline The output of the pipeline The training pipeline s final output is the trained weights stored in Comet s model registry. The inference pipeline s final output is the predictions served directly to the user. The infrastructure The training pipeline requires more powerful machines with as many GPUs as possible. _Why?_ During training, you batch your data and have to hold in memory all the gradients required for the optimization steps. Because of the optimization algorithm, the training is more compute hungry than the inference. Thus, more computing and VRAM result in bigger batches, which means less training time and more experiments. If you run a batch pipeline, you will still pass batches to the model but don t perform any optimization steps. If you run a real time pipeline, as we do in the LLM twin architecture, you pass a single sample to the model or do some dynamic batching to optimize your inference step. Are there any overlaps? Yes! This is where the training serving skew comes in. To avoid the training serving skew, you must carefully apply the same preprocessing and postprocessing steps during training and inference. 3 . The RAG business module We will define the RAG business module under the _LLMTwin_ class. The LLM twin logic is directly correlated with our business logic. We don t have to introduce the word business in the naming convention of the classes. Let s dig into the _generate _ method of the _LLMTwin_ class, where we call the RAG module create the prompt using the prompt template, query and context call the LLM microservice log the prompt, prompt template, and answer to Comet ML s prompt monitoring service. Inference pipeline business module generate method GitHub Let s look at how our LLM microservice is implemented using Qwak. 4 . The LLM microservice As the LLM microservice is deployed on Qwak, we must first inherit from the _QwakModel_ class and implement some specific functions. _initialize_model _ where we load the fine tuned model from the model registry at serving time _schema _ where we define the input and output schema _predict _ where we implement the actual inference logic Note The _build _ function contains all the training logic, such as loading the dataset, training the LLM, and pushing it to a Comet experiment. To see the full implementation, consider checking out Lesson 7, where we detailed the training pipeline. LLM microservice GitHub Let s zoom into the implementation and the life cycle of the Qwak model. The _schema _ method is used to define how the input and output of the _predict _ method look like. This will automatically validate the structure and type of the _predict _ method. For example, the LLM microservice will throw an error if the variable instruction is a JSON instead of a string. The other Qwak specific methods are called in the following order 1. ___init__ _ when deploying the model 2. _initialize_model _ when deploying the model 3. _predict _ on every request to the LLM microservice Note that these methods are called only during serving time and not during training . Qwak exposes your model as a RESTful API, where the _predict _ method is called on each request. Inside the prediction method, we perform the following steps map the input text to token IDs using the LLM specific tokenizer move the token IDs to the provided device GPU or CPU pass the token IDs to the LLM and generate the answer extract only the generated tokens from the _generated_ids_ variable by slicing it using the shape of the _input_ids_ decode the _generated_ids_ back to text return the generated text The final step is to look at Comet s prompt monitoring service. 5 . Prompt monitoring Comet makes prompt monitoring straightforward. There is just one API call where you connect to your project and workspace and send the following to a single function the prompt and LLM output the prompt template and variables that created the final output your custom metadata specific to your use case here, you add information about the model, prompt token count, token generation costs, latency, etc. class PromptMonitoringManager classmethod def log cls, prompt str, output str, prompt_template str None None, prompt_template_variables dict None None, metadata dict None None, None metadata model settings.MODEL_TYPE, metadata, or model settings.MODEL_TYPE comet_llm.log_prompt workspace settings.COMET_WORKSPACE, project f settings.COMET_PROJECT monitoring , api_key settings.COMET_API_KEY, prompt prompt, prompt_template prompt_template, prompt_template_variables prompt_template_variables, output output, metadata metadata, This is how Comet ML s prompt monitoring dashboard looks. Here, you can scroll through all the prompts that were ever sent to the LLM. You can click on any prompt and see everything we logged programmatically using the _PromptMonitoringManager_ class. Screenshot from Comet ML s dashboard Besides what we logged, adding various tags and the inference duration can be valuable. 6 . Deploying and running the inference pipeline We can deploy the LLM microservice using the following Qwak command qwak models deploy realtime model id llm_twin instance gpu.a10.2xl timeout 50000 replicas 2 server workers 2 We deployed two replicas of the LLM twin. Each replica has access to a machine with x1 A10 GPU. Also, each replica has two workers running on it. More on Qwak instance types Two replicas and two workers result in 4 microservices that run in parallel and can serve our users. You can scale the deployment to more replicas if you need to serve more clients. Qwak provides autoscaling mechanisms triggered by listening to the consumption of GPU, CPU or RAM. To conclude, you build the Qwak model once, and based on it, you can make multiple deployments with various strategies. Conclusion _Congratulations! You are close to the end of the LLM twin series._ In Lesson 9 of the LLM twin course, you learned to build a scalable inference pipeline for serving LLMs and RAG systems. First , you learned how to architect an inference pipeline by understanding the difference between monolithic and microservice architectures. We also highlighted the difference in designing the training and inference pipelines. Secondly , we walked you through implementing the RAG business module and LLM twin microservice. Also, we showed you how to log all the prompts, answers, and metadata for Comet s prompt monitoring service. Ultimately , we showed you how to deploy and run the LLM twin inference pipeline on the Qwak AI platform. In Lesson 10 , we will show you how to evaluate the whole system by building an advanced RAG evaluation pipeline that analyzes the accuracy of the LLMs answers relative to the query and context. See you there! _ Check out the code on GitHub 1 and support us with a _ Next Steps Step 1 This is just the short version of Lesson 9 on architecting scalable and cost effective LLM RAG inference pipelines. For The full implementation. Full deep dive into the code. More on the RAG, LLM and monitoring services. Check out the full version of Lesson 9 on our Medium publication . It s still FREE Lesson 9 on Medium Step 2 Consider checking out theLLM Twin GitHub repository and try it yourself _Nothing compares with getting your hands dirty and doing it yourself!_ LLM Twin Course GitHub Images If not otherwise stated, all images are created by the author. 13 Share this post Decoding ML Architect scalable and cost effective LLM RAG inference pipelines Copy link Facebook Email Notes More 2 Share PreviousNext Discussion about this post Comments Restacks Top Latest Discussions No posts Ready for more? Subscribe 2024 Paul Iusztin Privacy Terms Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Notes More This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts en",
            "platform": "decodingml.substack.com",
            "author_id": "c7167a83-e446-428d-9a2e-6e88a6b8bc6f",
            "author_full_name": "Hiep Nguyen",
            "link": "https://decodingml.substack.com/p/architect-scalable-and-cost-effective?r=1ttoeh"
        },
        {
            "id": "25479347-ea7e-475a-8400-7e99e4a858f3",
            "content": "4 Advanced RAG Algorithms You Must Know by Paul Iusztin Implement 4 advanced RAG retrieval techniques to optimize your vector DB searches. Integrate the RAG retrieval module into a production LLM system. SubscribeSign in Share this post Decoding ML The 4 Advanced RAG Algorithms You Must Know to Implement Copy link Facebook Email Notes More The 4 Advanced RAG Algorithms You Must Know to Implement Implement from scratch 4 advanced RAG methods to optimize your retrieval and post retrieval algorithm Paul Iusztin May 09, 2024 17 Share this post Decoding ML The 4 Advanced RAG Algorithms You Must Know to Implement Copy link Facebook Email Notes More 1 6 Share _ the 5th out of 11 lessons of the LLM Twin free course_ Why is this course different? _By finishing the LLM Twin Building Your Production Ready AI Replica _ _free course, you will learn how to design, train, and deploy a production ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices_. _ Why should you care? _ _ No more isolated scripts or Notebooks! Learn production ML by building and deploying an end to end production grade LLM system._ More details on what you will learn within the LLM Twin course , here Latest Lessons of the LLM Twin Course Lesson 2 The importance of Data Pipeline in the era of Generative AI Data crawling, ETL pipelines, ODM, NoSQL Database Lesson 3 CDC Enabling Event Driven Architectures Change Data Capture CDC , MongoDB Watcher, RabbitMQ queue Lesson 4 Python Streaming Pipelines for Fine tuning LLMs and RAG in Real Time! Feature pipeline, Bytewax streaming engine, Pydantic models, The dispatcher layer Lesson 5 The 4 Advanced RAG Algorithms You Must Know to Implement In Lesson 5 , we will focus on building an advanced retrieval module used for RAG. We will show you how to implement 4 retrieval and post retrieval advanced optimization techniques to improve the accuracy of your RAG retrieval step . In this lesson, we will focus only on the retrieval part of the RAG system. In Lesson 4 , we showed you how to clean, chunk, embed, and load social media data to a Qdrant vector DB the ingestion part of RAG . In future lessons, we will integrate this retrieval module into the inference pipeline for a full fledged RAG system. Retrieval Python Module Architecture 1 . Overview of advanced RAG optimization techniques A production RAG system is split into 3 main components ingestion clean, chunk, embed, and load your data to a vector DB retrieval query your vector DB for context generation attach the retrieved context to your prompt and pass it to an LLM The ingestion component sits in the _feature pipeline_ , while the retrieval and generation components are implemented inside the _inference pipeline_. You can also use the retrieval and generation components in your _training pipeline_ to fine tune your LLM further on domain specific prompts. You can apply advanced techniques to optimize your RAG system for ingestion, retrieval and generation. _That being said, there are 3 main types of advanced RAG techniques _ Pre retrieval optimization ingestion tweak how you create the chunks Retrieval optimization retrieval improve the queries to your vector DB Post retrieval optimization retrieval process the retrieved chunks to filter out the noise The generation step can be improved through fine tuning or prompt engineering, which will be explained in future lessons. The pre retrieval optimization techniques are explained in Lesson 4. In this lesson, we will show you some popular retrieval and post retrieval optimization techniques . 2 . Advanced RAG techniques applied to the LLM twin Retrieval optimization _We will combine 3 techniques _ Query Expansion Self Query Filtered vector search Post retrieval optimization We will use the rerank pattern using GPT 4 and prompt engineering instead of Cohere or an open source re ranker cross encoder 4 . I don t want to spend too much time on the theoretical aspects. There are plenty of articles on that. _So, we will jump straight to implementing and integrating these techniques in our LLM twin system._ But first, let s clarify why we picked Qdrant as our vector DB 2.1. Why Qdrant? There are many vector DBs out there, too many But since we discovered Qdrant, we loved it. Why? It is built in Rust. Apache 2.0 license open source It has a great and intuitive Python SDK. It has a freemium self hosted version to build PoCs for free. It supports unlimited document sizes, and vector dims of up to 645536. It is production ready. Companies such as Disney, Mozilla, and Microsoft already use it. It is one of the most popular vector DBs out there. _ To put that in perspective, _ Pinecone, one of its biggest competitors, supports only documents with up to 40k tokens and vectors with up to 20k dimensions . and a proprietary license. I could go on and on but if you are curious to find out more , _check out Qdrant _ 3 . Retrieval optimization 1 Query expansion Query expansion is quite intuitive. You use an LLM to generate multiple queries based on your initial query. These queries should contain multiple perspectives of the initial query. Thus, when embedded, they hit different areas of your embedding space that are still relevant to our initial question. You can do query expansion with a detailed zero shot prompt. Query expansion template GitHub Code 4 . Retrieval optimization 2 Self query What if you could extract the tags within the query and use them along the embedded query? That is what self query is all about! You use an LLM to extract various metadata fields that are critical for your business use case e.g., tags, author ID, number of comments, likes, shares, etc. In our custom solution, we are extracting just the author ID. Thus, a zero shot prompt engineering technique will do the job. _Self queries work hand in hand with vector filter searches, which we will explain in the next section._ To define the _ SelfQueryTemplate _ , we have to Subclass the base abstract class Define the self query prompt Create the LangChain PromptTemplate wrapper class SelfQueryTemplate BasePromptTemplate prompt str You are an AI language model assistant. Your task is to extract information from a user question. The required information that needs to be extracted is the user id. Your response should consists of only the extracted id e.g. 1345256 , nothing else. User question question def create_template self PromptTemplate return PromptTemplate template self.prompt, input_variables question , verbose True 5 . Retrieval optimization 3 Hybrid filtered vector search Combine the vector search technique with one or more complementary search strategy, which works great for finding exact words. It is not defined which algorithms are combined, but the most standard strategy for hybrid search is to combine the traditional keyword based search and modern vector search. _How are these combined?_ _The first method is to merge the similarity scores of the 2 techniques as follows _ hybrid_score 1 alpha sparse_score alpha dense_score Where alpha takes a value between 0, 1 , with alpha 1 Vector Search alpha 0 Keyword search Also, the similarity scores are defined as follows sparse_score is the result of the _keyword search_ that, behind the scenes, uses a BM25 algorithm 7 that sits on top of TF IDF. dense_score is the result of the _vector search_ that most commonly uses a similarity metric such as cosine distance _The second method uses the vector search technique as usual and applies a filter based on your keywords on top of the metadata of retrieved results._ This is also known as filtered vector search . In this use case, the similar score is not changed based on the provided keywords . It is just a fancy word for a simple filter applied to the metadata of your vectors. But it is essential to understand the difference between the first and second methods the first method combines the similarity score between the keywords and vectors using the alpha parameter the second method is a simple filter on top of your vector search. How does this fit into our architecture? Remember that during the self query step, we extracted the author_id as an exact field that we have to match. Thus, we will search for the author_id using the keyword search algorithm and attach it to the 5 queries generated by the query expansion step. _As we want the most relevant chunks from a given author, it makes the most sense to use a filter using the author_id as follows filtered vector search _ self._qdrant_client.search collection_name vector_posts , query_filter models.Filter must models.FieldCondition key author_id , match models.MatchValue value metadata_filter_value, , , query_vector self._embedder.encode generated_query .tolist , limit k, Note that we can easily extend this with multiple keywords e.g., tags , making the combination of self query and hybrid search a powerful retrieval duo. The only question you have to ask yourself is whether we want to use a simple vector search filter or the more complex hybrid search strategy. 6 . Implement the advanced retrieval Python class _Now that you ve understood the advanced retrieval optimization techniques we re using, let s combine them into a Python retrieval class ._ Query expansion chains wrapper GitHub Now the final step is to call Qdrant for each query generated by the query expansion step VectorRetriever main search function GitHub _Note that we have 3 types of data posts, articles, and code repositories._ Thus, we have to make a query for each collection and combine the results in the end. We gathered data from each collection individually and kept the best retrieved results using rerank. Which is the final step of the article. 7 . Post retrieval optimization Rerank using GPT 4 We made a different search in the Qdrant vector DB for N prompts generated by the query expansion step . Each search returns K results . Thus, we end up with N x K chunks . In our particular case, N 5 K 3. Thus, we end up with 15 chunks. Post retrieval optimization rerank We will use rerank to order all the N x K chunks based on their relevance relative to the initial question, where the first one will be the most relevant and the last chunk the least. Ultimately, we will pick the TOP K most relevant chunks. Rerank works really well when combined with query expansion. _A natural flow when using rerank is as follows _ Search for K chunks Reorder using rerank Take top K Thus, when combined with query expansion, we gather potential useful context from multiple points in space rather than just looking for more than K samples in a single location. _Now the flow looks like _ Search for N x K chunks Reoder using rerank Take top K A typical solution for reranking is to use open source Bi Encoders from sentence transformers 4 . These solutions take both the question and context as input and return a score from 0 to 1. In this article, we want to take a different approach and use GPT 4 prompt engineering as our reranker. If you want to see how to apply rerank using open source algorithms, check out this hands on article from Decoding ML A Real time Retrieval System for RAG on Social Media Data Paul Iusztin Mar 7 Read full story Now let s see our implementation using GPT 4 prompt engineering. Similar to what we did for the expansion and self query chains, we define a template and a chain builder class RerankingTemplate BasePromptTemplate prompt str You are an AI language model assistant. Your task is to rerank passages related to a query based on their relevance. The most relevant passages should be put at the beginning. You should only pick at max k passages. The following are passages related to this query question . Passages passages def create_template self PromptTemplate return PromptTemplate template self.prompt, input_variables question , passages and that s it! Conclusion _Congratulations!_ In Lesson 5 , you learned to build an advanced RAG retrieval module optimized for searching posts, articles, and code repositories from a Qdrant vector DB. First , you learned about where the RAG pipeline can be optimized pre retrieval retrieval post retrieval After you learn how to build from scratch without using LangChain s utilities the following advanced RAG retrieval post retrieval optimization techniques query expansion self query hybrid search rerank Ultimately , you understood where the retrieval component sits in an RAG production LLM system, where the code is shared between multiple microservices and doesn t sit in a single Notebook. _ Next week , in Lesson 6 , we will move to the training pipeline and show you how to automatically transform the data crawled from LinkedIn, Substack, Medium, and GitHub into an instruction dataset using GPT 4 to fine tune your LLM Twin._ See you there! Next Steps Step 1 This is just the short version of Lesson 5 on the advanced RAG retrieval module . For The full implementation. Discussion on our custom implementation vs. LangChain. More on the problems these 4 advanced RAG techniques solve. How to use the retrieval module. Check out the full version of Lesson 5 on our Medium publication . It s still FREE Lesson 5 FREE Medium Article Step 2 Check out theLLM Twin GitHub repository and try it yourself _Nothing compares with getting your hands dirty and building it yourself!_ LLM Twin Course GitHub Images If not otherwise stated, all images are created by the author. 17 Share this post Decoding ML The 4 Advanced RAG Algorithms You Must Know to Implement Copy link Facebook Email Notes More 1 6 Share PreviousNext Discussion about this post Comments Restacks Meng Li May 17 Great, thanks for sharing! Expand full comment Reply Share Top Latest Discussions No posts Ready for more? Subscribe 2024 Paul Iusztin Privacy Terms Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Notes More This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts en",
            "platform": "decodingml.substack.com",
            "author_id": "c7167a83-e446-428d-9a2e-6e88a6b8bc6f",
            "author_full_name": "Hiep Nguyen",
            "link": "https://decodingml.substack.com/p/the-4-advanced-rag-algorithms-you?r=1ttoeh"
        },
        {
            "id": "bf24e427-fd66-432c-aa67-c04ee33880c8",
            "content": "Streaming Pipelines for LLMs and RAG by Paul Iusztin SOTA streaming pipeline in Python to clean, chunk, embed and load data to a vector DB feature store in real time for fine tuning LLMs and RAG on AWS . SubscribeSign in Share this post Decoding ML SOTA Python Streaming Pipelines for Fine tuning LLMs and RAG in Real Time! Copy link Facebook Email Notes More SOTA Python Streaming Pipelines for Fine tuning LLMs and RAG in Real Time! Use a Python streaming engine to populate a feature store from 4 data sources Paul Iusztin Apr 25, 2024 11 Share this post Decoding ML SOTA Python Streaming Pipelines for Fine tuning LLMs and RAG in Real Time! Copy link Facebook Email Notes More 2 Share the 4th out of 11 lessons of the LLM Twin free course What is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality, and voice into an LLM. Image by DALL E Why is this course different? _By finishing the LLM Twin Building Your Production Ready AI Replica _ _free course, you will learn how to design, train, and deploy a production ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices_. _ Why should you care? _ _ No more isolated scripts or Notebooks! Learn production ML by building and deploying an end to end production grade LLM system._ More details on what you will learn within the LLM Twin course , here Latest Lessons of the LLM Twin Course Lesson 1 An End to End Framework for Production Ready LLM Systems by Building Your LLM Twin LLM Twin Concept, 3 Pipeline Architecture, System Design for LLM Twin Lesson 2 The importance of Data Pipeline in the era of Generative AI Data crawling, ETL pipelines, ODM, NoSQL Database Lesson 3 CDC Enabling Event Driven Architectures Change Data Capture CDC , MongoDB Watcher, RabbitMQ queue Lesson 4 Streaming Pipelines for Fine tuning LLMs and RAG in Real Time! In the 4th lesson , we will focus on the feature pipeline. The feature pipeline is the first pipeline presented in the 3 pipeline architecture feature, training and inference pipelines. A feature pipeline takes raw data as input, processes it into features, and stores it in a feature store, from which the training inference pipelines will use it. The component is completely isolated from the training and inference code. All the communication is done through the feature store. By the end of this article , you will learn to design and build a production ready feature pipeline that uses Bytewax as a stream engine to process data in real time ingests data from a RabbitMQ queue uses SWE practices to process multiple data types posts, articles, code cleans, chunks, and embeds data for LLM fine tuning and RAG loads the features to a Qdrant vector DB. Note that we will only cover the vector DB retrieval client and advanced retrieval techniques in the 5th lesson ! _Excited? Let s get started!_ Table of Contents 1. Why are we doing this? 2. System design of the feature pipeline 3. The Bytewax streaming flow 4. Pydantic data models 5. Load data to Qdrant our feature store 6. The dispatcher layer Check out the code on GitHub 1 and support us with a 1 . Why are we doing this? A quick reminder from previous lessons To give you some context, in Lesson 2, we crawl data from LinkedIn, Medium, and GitHub, normalize it, and load it to MongoDB. In Lesson 3, we are using CDC to listen to changes to the MongoDB database and emit events in a RabbitMQ queue based on any CRUD operation done on MongoDB. The problem we are solving In our LLM Twin use case, the feature pipeline constantly syncs the MongoDB warehouse with the Qdrant vector DB our feature store while processing the raw data into features. Why we are solving it The feature store will be the central point of access for all the features used within the training and inference pipelines. The training pipeline will use the feature store to create fine tunin g datasets for your LLM twin . The inference pipeline will use the feature store for RAG . 2 . System design of the feature pipeline our solution _Our solution is based on CDC , a queue, a streaming engine, and a vector DB _ CDC adds any change made to the Mongo DB to the queue read more in Lesson 3 . the RabbitMQ queue stores all the events until they are processed. The Bytewax streaming engine cleans, chunks, and embeds the data. A streaming engine works naturally with a queue based system. The data is uploaded to a Qdrant vector DB on the fly Why is this powerful? Here are 4 core reasons 1. The data is processed in real time . 2. Out of the box recovery system If the streaming pipeline fails to process a message will be added back to the queue 3. Lightweight No need for any diffs between databases or batching too many records 4. No I O bottlenecks on the source database It solves all our problems! Streaming ingestion pipeline architecture and integration with the rest of the components How do we process multiple data types? How do you process multiple types of data in a single streaming pipeline without writing spaghetti code ? Yes, that is for you, data scientists! Joking am I ? We have 3 data types posts, articles, and code. Each data type and its state will be modeled using Pydantic models . To process them, we will write a dispatcher layer , which will use a creational factory pattern to instantiate a handler implemented for that specific data type post, article, code and operation cleaning, chunking, embedding . The handler follows the strategy behavioral pattern. Streaming over batch Nowadays, using tools such as Bytewax makes implementing streaming pipelines a lot more frictionless than using their JVM alternatives. The key aspect of choosing a streaming vs. a batch design is real time synchronization between your source and destination DBs. In our particular case, we will process social media data, which changes fast and irregularly. Also, for our digital twin, it is important to do RAG on up to date data. We don t want to have any delay between what happens in the real world and what your LLM twin sees. That being said, choosing a streaming architecture seemed natural in our use case. 3 . The Bytewax streaming flow The Bytewax flow is the central point of the streaming pipeline . It defines all the required steps, following the next simplified pattern _ input processing output ._ As I come from the AI world, I like to see it as the graph of the streaming pipeline , where you use the _input _ , _map _ , and _output _ Bytewax functions to define your graph, which in the Bytewax world is called a _ flow _. As you can see in the code snippet below, we ingest posts, articles or code messages from a RabbitMQ queue. After we clean, chunk and embed them. Ultimately, we load the cleaned and embedded data to a Qdrant vector DB, which in our LLM twin use case will represent the feature store of our system. To structure and validate the data, between each Bytewax step, we map and pass a different Pydantic model based on its current state raw, cleaned, chunked, or embedded. Bytewax flow GitHub Code We have a single streaming pipeline that processes everything. As we ingest multiple data types posts, articles, or code snapshots , we have to process them differently. To do this the right way, we implemented a dispatcher layer that knows how to apply data specific operations based on the type of message. More on this in the next sections Why Bytewax? _Bytewax is an open source streaming processing framework that _ is built in Rust for performance has Python bindings for leveraging its powerful ML ecosystem so, for all the Python fanatics out there, no more JVM headaches for you. Jokes aside, here is why Bytewax is so powerful Bytewax local setup is plug and play can quickly be integrated into any Python project you can go wild even use it in Notebooks can easily be integrated with other Python packages NumPy, PyTorch, HuggingFace, OpenCV, SkLearn, you name it out of the box connectors for Kafka and local files, or you can quickly implement your own We used Bytewax to build the streaming pipeline for the LLM Twin course and loved it. To learn more about Bytewax , check out their Substack , where you have the chance to dive deeper into streaming engines . In Python. For FREE Bytewax Newsletter 4 . Pydantic data models Let s take a look at what our Pydantic models look like. We defined a hierarchy of Pydantic models for all our data types posts, articles, or code all our states raw, cleaned, chunked, and embedded This is how the set of classes for the posts will look like Pydantic posts model structure GitHub Code We repeated the s ame process for the articles and code model hierarchy . 5 . Load data to Qdrant our feature store The first step is to implement our custom Bytewax _DynamicSink_ class Qdrant DynamicSink GitHub Code Next, for every type of operation we need output cleaned or embedded data , we have to subclass the _StatelessSinkPartition_ Bytewax class they also provide a stateful option more in their docs An instance of the class will run on every partition defined within the Bytewax deployment. In the course, we are using a single partition per worker. But, by adding more partitions and workers , you can quickly scale your Bytewax pipeline horizontally. Remember why we upload the data to Qdrant in two stages , as the Qdrant vector DB will act as our feature store 1. The _cleaned data_ will be used for _LLM fine tuning_ used by the training pipeline 2. The _chunked embedded_ data will be used for _RAG used by the inference pipeline _ Qdrant worker partitions GitHub Code Note that we used Qdrant s Batch method to upload all the available points simultaneously. By doing so, we reduce the latency on the network I O side more on that here 6 . The dispatcher layer Now that we have the Bytewax flow and all our data models. How do we map a raw data model to a cleaned data model? All our domain logic is modeled by a set of _Handler _ classes CleaningDataHandler ChunkingDataHandler EmbeddingDataHandler Now, to build our dispatcher, we need 2 last components a factory class instantiates the right handler based on the type of the event a dispatcher class the glue code that calls the factory class and handler Here is what the cleaning dispatcher and factory look like The dispatcher and factory classes GitHub Code Note that we will have a different Handler for every data_type, state pair resulting in 3 x 3 9 different handlers. For Example, we will have 3 handlers based on their data type for the cleaned post state PostCleaningHandler, ArticleCleaningHandler, and RepositoryCleaningHandler. By repeating the same logic, we will end up with the following set of dispatchers _RawDispatcher_ no factory class required as the data is not processed _CleaningDispatcher_ with a _ChunkingHandlerFactory_ class _ChunkingDispatcher_ with a _ChunkingHandlerFactory_ class _EmbeddingDispatcher_ with an _EmbeddingHandlerFactory_ class To Summarize In Lesson 4 of the LLM Twin course, we learned how to Design a streaming pipeline in Python using Bytewax Load data to a Qdrant vector DB Use Pydantic models to add types and validation to the data points Implement a dispatcher layer to process multiple data types in a modular way _ In Lesson 5, which will be held in two weeks, we will focus on the vector DB retrieval client and advanced retrieval techniques._ Next Steps To dig into the details of the streaming pipeline and how to implement cleaning , chunking , and embedding strategies for digital data design the AWS infrastructure for the streaming pipeline understand how to run the component Check out the full fledged version of the article on our Medium publication . Lesson 4 FREE Medium Article Images If not otherwise stated, all images are created by the author. 11 Share this post Decoding ML SOTA Python Streaming Pipelines for Fine tuning LLMs and RAG in Real Time! Copy link Facebook Email Notes More 2 Share PreviousNext Discussion about this post Comments Restacks Top Latest Discussions No posts Ready for more? Subscribe 2024 Paul Iusztin Privacy Terms Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Notes More This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts en",
            "platform": "decodingml.substack.com",
            "author_id": "c7167a83-e446-428d-9a2e-6e88a6b8bc6f",
            "author_full_name": "Hiep Nguyen",
            "link": "https://decodingml.substack.com/p/sota-python-streaming-pipelines-for?r=1ttoeh"
        },
        {
            "id": "655356bf-f8ca-4fd3-95b3-cfceb98db6da",
            "content": "End to End Framework for Production Ready LLMs FREE course on designing, training, deploying, and monitoring a production ready LLM system powered by LLMs, vector DBs LLMOps by building your LLM twin. SubscribeSign in Share this post Decoding ML An End to End Framework for Production Ready LLM Systems by Building Your LLM Twin Copy link Facebook Email Notes More An End to End Framework for Production Ready LLM Systems by Building Your LLM Twin From data gathering to productionizing LLMs using LLMOps good practices. Paul Iusztin Mar 28, 2024 35 Share this post Decoding ML An End to End Framework for Production Ready LLM Systems by Building Your LLM Twin Copy link Facebook Email Notes More 4 Share _ the 1st out of 11 lessons of the LLM Twin free course_ What is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM. Image by DALL E Why is this course different? _By finishing the LLM Twin Building Your Production Ready AI Replica _ _free course, you will learn how to design, train, and deploy a production ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices_. _ Why should you care? _ _ No more isolated scripts or Notebooks! Learn production ML by building and deploying an end to end production grade LLM system._ More details on what you will learn within the LLM Twin course , here Are you ready to build your AI replica? Let s start with Lesson 1 Lesson 1 End to end framework for production ready LLM systems In the first lesson , we will present the project you will build during the course _your production ready LLM Twin AI replica._ Afterward , we will dig into the LLM project system design . We will present all our architectural decisions regarding the design of the _data collection pipeline_ for social media data and how we applied _the 3 pipeline architecture_ to our LLM microservices. In the following lessons , we will examine each component s code and learn how to implement and deploy it to AWS and Qwak. LLM twin system architecture Image by the Author What you will learn to build during this course. Table of Contents 1. What are you going to build? The LLM twin concept 2. LLM twin system design 1 . What are you going to build? The LLM twin concept The outcome of this course is to learn to build your own AI replica . We will use an LLM to do that, hence the name of the course _ LLM Twin Building Your Production Ready AI Replica. _ But what is an LLM twin? Shortly, your LLM twin will be an AI character who writes like you, using your writing style and personality. It will not be you. It will be your writing copycat. More concretely, you will build an AI replica that writes social media posts or technical articles like this one using your own voice. Why not directly use ChatGPT? You may ask When trying to generate an article or post using an LLM, the results tend to be very generic and unarticulated, contain misinformation due to hallucination , require tedious prompting to achieve the desired result. _ But here is what we are going to do to fix that _ First , we will fine tune an LLM on your digital data gathered from LinkedIn, Medium, Substack and GitHub. By doing so, the LLM will align with your writing style and online personality. It will teach the LLM to talk like the online version of yourself. Our use case will focus on an LLM twin who writes social media posts or articles that reflect and articulate your voice. Secondly , we will give the LLM access to a vector DB to access external information to avoid hallucinating. Ultimately , in addition to accessing the vector DB for information, you can provide external links that will act as the building block of the generation process. Excited? Let s get started 2 . LLM Twin System design Let s understand how to apply the 3 pipeline architecture to our LLM system . The architecture of the LLM twin is split into 4 Python microservices 1. The data collection pipeline 2. The feature pipeline 3. The training pipeline 4. The inference pipeline LLM twin system architecture Image by the Author _Now, let s zoom in on each component to understand how they work individually and interact with each other. _ 2.1. The data collection pipeline Its scope is to crawl data for a given user from Medium articles Substack articles LinkedIn posts GitHub code As every platform is unique, we implemented a different Extract Transform Load ETL pipeline for each website. However, the baseline steps are the same for each platform . _Thus, for each ETL pipeline, we can abstract away the following baseline steps _ log in using your credentials use _selenium_ to crawl your profile use _BeatifulSoup_ to parse the HTML clean normalize the extracted HTML save the normalized but still raw data to Mongo DB Important note We are crawling only our data, as most platforms do not allow us to access other people s data due to privacy issues. But this is perfect for us, as to build our LLM twin, we need only our own digital data. Why Mongo DB? We wanted a NoSQL database that quickly allows us to store unstructured data aka text . How will the data pipeline communicate with the feature pipeline? We will use the Change Data Capture CDC pattern to inform the feature pipeline of any change on our Mongo DB. To explain the CDC briefly, a watcher listens 24 7 for any CRUD operation that happens to the Mongo DB. The watcher will issue an event informing us what has been modified. We will add that event to a RabbitMQ queue. The feature pipeline will constantly listen to the queue, process the messages, and add them to the Qdrant vector DB. For example, when we write a new document to the Mongo DB, the watcher creates a new event. The event is added to the RabbitMQ queue ultimately, the feature pipeline consumes and processes it. Where will the data pipeline be deployed? The data collection pipeline and RabbitMQ service will be deployed to AWS. We will also use the freemium serverless version of Mongo DB. 2.2. The feature pipeline The feature pipeline is implemented usingBytewax a Rust streaming engine with a Python interface . Thus, in our specific use case , we will also refer to it as a streaming ingestion pipeline . It is an entirely different service than the data collection pipeline. How does it communicate with the data pipeline? As explained above, the feature pipeline communicates with the data pipeline through a RabbitMQ queue . Currently, the streaming pipeline doesn t care how the data is generated or where it comes from. It knows it has to listen to a given queue, consume messages from there and process them. By doing so, we decouple the two components entirely. What is the scope of the feature pipeline? It represents the ingestion component of the RAG system . It will take the raw data passed through the queue and clean the data chunk it embed it using the embedding models from Superlinked load it to the Qdrant vector DB. What data will be stored? The training pipeline will have access only to the feature store , which, in our case, is represented by the Qdrant vector DB. _With this in mind, we will store in Qdrant 2 snapshots of our data _ 1 . The cleaned data without using vectors as indexes store them in a NoSQL fashion . 2 . The cleaned, chunked, and embedded data leveraging the vector indexes of Qdrant The training pipeline needs access to the data in both formats as we want to fine tune the LLM on standard and augmented prompts. Why implement a streaming pipeline instead of a batch pipeline? There are 2 main reasons. The first one is that, coupled with the CDC pattern , it is the most efficient way to sync two DBs between each other. Using CDC a streaming pipeline, you process only the changes to the source DB without any overhead. The second reason is that by doing so, your source and vector DB will always be in sync . Thus, you will always have access to the latest data when doing RAG. Why Bytewax? Bytewax is a streaming engine built in Rust that exposes a Python interface. We use Bytewax because it combines Rust s impressive speed and reliability with the ease of use and ecosystem of Python. It is incredibly light, powerful, and easy for a Python developer. Where will the feature pipeline be deployed? The feature pipeline will be deployed to AWS. We will also use the freemium serverless version of Qdrant. 2.3. The training pipeline How do we have access to the training features? As section 2.2 highlights, all the training data will be accessed from the feature store . In our case, the feature store is the Qdrant vector DB that contains the cleaned digital data from which we will create prompts answers we will use the chunked embedded data for RAG to augment the cleaned data. _We will implement a different vector DB retrieval client for each of our main types of data posts, articles, code ._ What will the training pipeline do? The training pipeline contains a data to prompt layer that will preprocess the data retrieved from the vector DB into prompts. It will also contain an LLM fine tuning module that inputs a HuggingFace dataset and uses QLoRA to fine tune a given LLM e.g., Mistral . All the experiments will be logged into Comet ML s experiment tracker . We will use a bigger LLM e.g., GPT4 to evaluate the results of our fine tuned LLM. These results will be logged into Comet s experiment tracker. Where will the production candidate LLM be stored? We will compare multiple experiments, pick the best one, and issue an LLM production candidate for the model registry. After, we will inspect the LLM production candidate manually using Comet s prompt monitoring dashboard. Where will the training pipeline be deployed? The training pipeline will be deployed to Qwak. Qwak is a serverless solution for training and deploying ML models. It makes scaling your operation easy while you can focus on building. Also, we will use the freemium version of Comet ML for the following experiment tracker model registry prompt monitoring. 2.4. The inference pipeline The inference pipeline is the final component of the LLM system . It is the one the clients will interact with . It will be wrapped under a REST API . The clients can call it through HTTP requests, similar to your experience with ChatGPT or similar tools. How do we access the features? We will grab the features solely from the feature store. We will use the same Qdrant vector DB retrieval clients as in the training pipeline to use the features we need for RAG. How do we access the fine tuned LLM? The fine tuned LLM will always be downloaded from the model registry based on its tag e.g., accepted and version e.g., v1.0.2, latest, etc. . What are the components of the inference pipeline? The first one is the retrieval client used to access the vector DB to do RAG. After we have a query to prompt the layer, that will map the prompt and retrieved documents from Qdrant into a prompt. After the LLM generates its answer, we will log it to Comet s prompt monitoring dashboard and return it to the clients. For example, the client will request the inference pipeline to Write a 1000 word LinkedIn post about LLMs, and the inference pipeline will go through all the steps above to return the generated post. Where will the inference pipeline be deployed? The inference pipeline will be deployed to Qwak. As for the training pipeline, we will use a serverless freemium version of Comet for its prompt monitoring dashboard. Conclusion This is the 1st article of the _ LLM Twin Building Your Production Ready AI Replica _ free course. In this lesson, we presented what you will build during the course. Ultimately, we went through the system design of the course and presented the architecture of each microservice and how they interact with each other 1. The data collection pipeline 2. The feature pipeline 3. The training pipeline 4. The inference pipeline In Lesson 2 , we will dive deeper into the data collection pipeline , learn how to implement crawlers for various social media platforms, clean the gathered data, store it in a Mongo DB, and finally, show you how to deploy it to AWS. _ Check out the code on GitHub 1 and support us with a _ This is how we can further help you In the Decoding ML newsletter , we want to keep things short sweet . To dive deeper into all the concepts presented in this article Check out the full fledged version of the article on our Medium publication . It s FREE Detailed Lesson 1 on Medium 35 Share this post Decoding ML An End to End Framework for Production Ready LLM Systems by Building Your LLM Twin Copy link Facebook Email Notes More 4 Share PreviousNext Discussion about this post Comments Restacks Top Latest Discussions No posts Ready for more? Subscribe 2024 Paul Iusztin Privacy Terms Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Notes More This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts en",
            "platform": "decodingml.substack.com",
            "author_id": "c7167a83-e446-428d-9a2e-6e88a6b8bc6f",
            "author_full_name": "Hiep Nguyen",
            "link": "https://decodingml.substack.com/p/an-end-to-end-framework-for-production?r=1ttoeh"
        },
        {
            "id": "5469b7a5-dfd7-4b8e-bbcb-b0eb93a78ff6",
            "content": "DML New year, the new improved Decoding ML What to expect? How we plan to grow, provide more qualitative hands on content, and real world ML projects to expand your professional skills SubscribeSign in Share this post Decoding ML DML New year, the new improved Decoding ML What to expect? Copy link Facebook Email Notes More DML New year, the new improved Decoding ML What to expect? How we plan to grow, provide more qualitative hands on content, and real world ML projects to expand your professional skills Paul Iusztin , Alex Razvant , and Vesa Alexandru Jan 11, 2024 10 Share this post Decoding ML DML New year, the new improved Decoding ML What to expect? Copy link Facebook Email Notes More 2 Share _Hello there, I am Paul Iusztin _ _Within this newsletter, I will help you decode complex topics about ML MLOps one week at a time _ This newsletter will differ from the others as I want to share my plans for the Decoding ML newsletter with you. From now on, it will cost 1000 month. Joking. It will still be free. It s not about the money but about growth, better quality added value. To be 100 transparent with you, I started this newsletter as an experiment, but when I saw people who actually read it, the perfectionist in me screamed that I should improve it and move to the next step. This is the next step. And I m taking you with me. The big news is that I will go all in, pouring more time and resources into growing the Decoding ML newsletter. My main goals are to push better quality content every week bring more real world projects to increase your hands on skills increases the number of articles with code examples to make it practical so you can benefit from it even more at your job As the world constantly changes, especially AI, MLE MLOps, you cannot stagnate. Decoding ML s growth is about providing you with all the MLE MLOps necessary resources to grow with it and smash it at your projects and job. _So.. How do I plan to grow the Decoding ML newsletter?_ Well, there are 3 main steps 1. Rebranding From now on, my face will no longer be the logo of Decoding ML. This will be the new logo of Decoding ML So you don t have to see my annoying face every Thursday morning in your email 2. Bringing in talent As I wanted to push more content of higher quality, I had to bring in more talented people to write beside me. I was lucky enough to know Alex Razvant and Alex Vesa, who are 2 fantastic MLE MLOps engineers with 10 years of hands on experience in the AI industry. From now on, they will start contributing to the Decoding ML newsletter and team along with me. Maybe you know this famous saying If you want to go fast, go alone if you want to go far, go together . and I want Decoding ML to go far. Our primary goal is to help you level up in MLE MLOps by offering hands on examples that you can use at your job. I plan to improve the quality of the articles by including more code and concrete examples besides the system design talks we have discussed so far. and here enters the scene The Alex s I have worked with them, and I know they are talented experts with fantastic hands on MLE MLOps skills and insights to share with you. Starting from now on, Decoding ML will no longer be a one person brand but a brand by itself, hosted by the new Decoding ML team myself Alex Vesa Alex Razvant 2.1. Now, let the team introduce itself _ Alex Vesa _ _Main niche Deep Learning Computer Vision ML System Infrastructure Startups Business _ LinkedIn Hello everyone, I m very grateful for this opportunity. I consider creativity and inspiration to flourish when there s a merger of minds from various individuals. My professional journey began in 2015, initially focusing on software engineering with a keen interest in Python and AI technologies. I quickly progressed, taking on challenging roles and AI projects. My experience in various startups as a CTO focused on leading teams in developing innovative software solutions. I worked in multiple sectors, notably healthcare and automotive, where I ve implemented AI driven systems to enhance operational efficiency. My technical skills are broad, encompassing Python, Django, and AWS. I m dedicated to leveraging my AI and software development expertise to drive organizational success in this dynamic field. I value knowledge sharing among our community, and my objective is to bring solid expertise in practical, real world AI ML systems to help you in your day to day work and enhance your creativity and vision in product development. Ultimately, I want to share with you the endless capabilities you can possess to evolve. _Alex Razvant_ _Main niche ML CV Systems in Production MLOps_ _Edge ML Deployments _ LinkedIn Hey everyone, I m really happy about this merger, as you ll get 3X more quality content in a concise, valuable, and actionable manner directly to your inbox! Here are a few words about who I am I started my journey as a SWE in 2015, diving into full stack web development. After a few internships, hackathons, and a few failed projects, the ML field caught my eye, and I haven t looked back ever since. My journey includes over 15 successful freelance projects, earning a Top Rated ML Engineer badge on UpWork , collaborating with BMW on AI for self driving cars, authoring a paper for IEEE RAL 2020, and developing scalable Computer Vision systems to analyze 1000 hours of CCTV footage. I aim to bring solid expertise via code tutorials, diagrams, and system designs to help you overcome challenges in building and deploying ML CV systems in cloud or edge environments, following the best practices I ve learned in SWE, ML, and MLOps. _Follow them check them out on LinkedIn to see their incredible experience in AI._ 2.2. Will we start approaching different topics? _TL DR No!_ I was meticulous in bringing in more people with the same vision. Thus, Decoding ML will approach the same niche as it has done _ production ready MLE MLOps topics. _ So you don t have to unsubscribe. We will keep talking about the same topics you chose to follow in our newsletter _ hands on MLE MLOps topics _ However, the advantage of having more people with different backgrounds on the team is that we all come with different perspectives and domain knowledge. For example Alex Razvant worked a lot with Computer Vision, Deep Learning, and MLOps technologies in the world of retail Alex Vesa has a lot of experience with Deep Learning and infrastructure projects in the medical field I am passioned about generative AI, MLOps, and SWE combining our knowledge will result in exciting production ready MLE MLOps articles that will significantly benefit you. 3. Expanding to new distribution channels Every person consumes content differently. So, we d like to give you the best fit to enjoy our content. We already started a Decoding ML Medium publication, where we will start this month to push a deep dive into the code of the Hands on LLMs Course. and slowly, we will expand to video format content on Youtube Instagram TikTok Also, we started planning a set of eBooks about MLE, MLOps and LLMOps and a new course about LLMs and LLMOps. So What happens next? I hope you are excited about the news. For sure, I am _Next Thursday at 9 00 a.m. CET_ , Alex Vesa will make his grand opening by writing a step by step article on how you can deploy an LLaMA2 7b LLM using Amazon SageMaker and HuggingFace . To conclude, you don t have to do anything on your side. _Decoding ML follows its natural course by bringing in more people and expanding to other platforms to give you more value for your time and a more personalized way to enjoy our content._ See you next Thursday! Have a fantastic weekend! Paul 10 Share this post Decoding ML DML New year, the new improved Decoding ML What to expect? Copy link Facebook Email Notes More 2 Share PreviousNext Discussion about this post Comments Restacks Ahmed Besbes Jan 11 Liked by Paul Iusztin Great things coming ahead Paul! Looking forward to it! Expand full comment Reply Share 1 reply by Paul Iusztin 1 more comment... Top Latest Discussions No posts Ready for more? Subscribe 2024 Paul Iusztin Privacy Terms Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Notes More This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts en",
            "platform": "decodingml.substack.com",
            "author_id": "c7167a83-e446-428d-9a2e-6e88a6b8bc6f",
            "author_full_name": "Hiep Nguyen",
            "link": "https://decodingml.substack.com/p/dml-new-year-the-new-and-improved?r=1ttoeh"
        },
        {
            "id": "f032fb88-e1ba-41c9-98df-90c0956a0d90",
            "content": "Create Mixtures of Experts with MergeKit Combine multiple models into a single MoE Maxime Labonne SubscribeSign in Share this post Maxime Labonne Create Mixtures of Experts with MergeKit Copy link Facebook Email Notes More Create Mixtures of Experts with MergeKit Combine multiple models into a single MoE Maxime Labonne Mar 27, 2024 1 Share this post Maxime Labonne Create Mixtures of Experts with MergeKit Copy link Facebook Email Notes More Share _Combine multiple models into a single MoE_ Image by author Thanks to the release of Mixtral, the Mixture of Experts MoE architecture has become popular in recent months. This architecture offers an interesting tradeoff higher performance at the cost of increased VRAM usage. While Mixtral and other MoE architectures are pre trained from scratch, another method of creating MoE has recently appeared. Thanks to Arcee s MergeKit library, we now have a new way of creating MoEs by ensembling several pre trained models. These are often referred to as frankenMoEs or MoErges to distinguish them from the pre trained MoEs. In this article, we will detail how the MoE architecture works and how frankenMoEs are created. Finally, we will make our own frankenMoE with MergeKit and evaluate it on several benchmarks. The code is available on Google Colab in a wrapper called LazyMergeKit. Special thanks to Charles Goddard, the creator of MergeKit, for proofreading this article. Introduction to MoEs A Mixture of Experts is an architecture designed for improved efficiency and performance. It uses multiple specialized subnetworks, known as experts . Unlike dense models, where the entire network is activated, MoEs only activate relevant experts based on the input. This results in faster training and more efficient inference. There are two components at the core of an MoE model 1. Sparse MoE Layers These replace the dense feed forward network layers in the transformer architecture. Each MoE layer contains several experts, and only a subset of these experts are engaged for a given input. 2. Gate Network or Router This component determines which tokens are processed by which experts, ensuring that each part of the input is handled by the most suitable expert s . In the following example, we show how a Mistral 7B block is transformed into an MoE block with a sparse MoE layer feedforward network 1, 2, and 3 and a router. This example represents an MoE with three experts, where two are currently engaged FFN 1 and FFN 3 . Image by author MoEs also come with their own set of challenges, especially in terms of fine tuning and memory requirements. The fine tuning process can be difficult due to the model s complexity, with the need to balance expert usage during training to properly train the gating weights to select the most relevant ones. In terms of memory, even though only a fraction of the total parameters are used during inference, the entire model, including all experts, needs to be loaded into memory , which requires high VRAM capacity. More specifically, there are two essential parameters when it comes to MoEs Number of experts num_local_experts This determines the total number of experts in the architecture e.g., 8 for Mixtral . The higher the number of experts, the higher the VRAM usage. Number of experts token num_experts_per_tok This determines the number of experts that are engaged for each token and each layer e.g., 2 for Mixtral . There is a tradeoff between a high number of experts per token for accuracy but diminishing returns vs. a low number for fast training and inference. Historically, MoEs have underperformed dense models. However, the release of Mixtral 8x7B in December 2023 shook things up and showed impressive performance for its size. Additionally, GPT 4 is also rumored to be an MoE, which would make sense as it would be a lot cheaper to run and train for OpenAI compared to a dense model. In addition to these recent excellent MoEs, we now have a new way of creating MoEs with MergeKit frankenMoEs, also called MoErges. True MoEs vs. frankenMoEs The main difference between true MoEs and frankenMoEs is how they re trained. In the case of true MoEs, the experts and the router are trained jointly. In the case of frankenMoEs, we upcycle existing models and initialize the router afterward. In other words, we copy the weights of the layer norm and self attention layers from a base model, and then copy the weights of the FFN layers found in each expert. This means that besides the FFNs, all the other parameters are shared. This explains why Mixtral 8x7B with eight experts doesn t have 8 7 56B parameters, but about 45B. This is also why using two experts per token gives the inference speed FLOPs of a 12B dense model instead of 14B. FrankenMoEs are about selecting the most relevant experts and initializing them properly. MergeKit currently implements three ways of initializing the routers 1. Random Random weights. Be careful when using it as the same experts might be selected every time it requires further fine tuning or num_local_experts num_experts_per_tok , which means you don t need any routing . 2. Cheap embed It uses the raw embeddings of the input tokens directly and applies the same transformation across all layers. This method is computationally inexpensive and suitable for execution on less powerful hardware. 3. Hidden It creates hidden representations of a list of positive and negative prompts by extracting them from the last layer of the LLM. They are averaged and normalized to initialize the gates. More information about it is available on Charles Goddard s blog. As you can guess, the hidden initialization is the most efficient to correctly route the tokens to the most relevant experts. In the next section, we will create our own frankenMoE using this technique. Creating a frankenMoE To create our frankenMoE, we need to select n experts. In this case, we will rely on Mistral 7B thanks to its popularity and relatively small size. However, eight experts like in Mixtral is quite a lot, as we need to fit all of them in memory. For efficiency, I ll only use four experts in this example, with two of them engaged for each token and each layer. In this case, we will end up with a model with 24.2B parameters instead of 4 7 28B parameters. Here, our goal is to create a well rounded model that can do pretty much everything write stories, explain articles, code in Python, etc. We can decompose this requirement into four tasks and select the best expert for each of them. This is how I decomposed it Chat model a general purpose model that is used in most interactions. I used mlabonne AlphaMonarch 7B, which perfectly satisfies the requirements. Code model a model capable of generating good code. I don t have a lot of experience with Mistral 7B based code models, but I found beowolx CodeNinja 1.0 OpenChat 7B particularly good compared to others. Math model math is tricky for LLMs, which is why we want a model specialized in math. Thanks to its high MMLU and GMS8K scores, I chose mlabonne NeuralDaredevil 7B for this purpose. Role play model The goal of this model is to write high quality stories and conversations. I selected SanjiWatsuki Kunoichi DPO v2 7B because of its good reputation and high MT Bench score 8.51 vs. 8.30 for Mixtral . Now that we ve identified the experts we want to use, we can create the YAML configuration that MergeKit will use to create our frankenMoE. This uses the mixtral branch of MergeKit. You can find more information about how to write the configuration on this page. Here is our version base_model mlabonne AlphaMonarch 7B experts source_model mlabonne AlphaMonarch 7B positive_prompts chat assistant tell me explain I want source_model beowolx CodeNinja 1.0 OpenChat 7B positive_prompts code python javascript programming algorithm source_model SanjiWatsuki Kunoichi DPO v2 7B positive_prompts storywriting write scene story character source_model mlabonne NeuralDaredevil 7B positive_prompts reason math mathematics solve count For each expert, I provide five basic positive prompts. You can be a bit fancier and write entire sentences if you want. The best strategy consists of using real prompts that should trigger a particular expert. You can also add negative prompts to do the opposite. Once this is ready, you can save your configuration as config.yaml . In the same folder, we will download and install the mergekit library mixtral branch . git clone b mixtral https github.com arcee ai mergekit.git cd mergekit pip install e . pip install U transformers If your computer has enough RAM roughly 24 32 GB of RAM , you can run the following command mergekit moe config.yaml merge copy tokenizer If you don t have enough RAM, you can shard the models instead as follows it will take longer mergekit moe config.yaml merge copy tokenizer allow crimes out shard size 1B lazy unpickle This command automatically downloads the experts and creates the frankenMoE in the merge directory. For the hidden gate mode, you can also use the load in 4bit and load in 8bit options to compute hidden states with lower precision. Alternatively, you can copy your configuration into LazyMergekit, a wrapper I made to simplify model merging. In this Colab notebook, you can input your model name, select the mixtral branch, specify your Hugging Face username token, and run the cells. After creating your frankenMoE, it will also upload it to the Hugging Face Hub with a nicely formatted model card. I called my model Beyonder 4x7B v3 and created GGUF versions of it using AutoGGUF. If you can t run GGUF versions on your local machine, you can also perform inference using this Colab notebook. To get a good overview of its capabilities, it has been evaluated on three different benchmarks Nous benchmark suite, EQ Bench, and the Open LLM Leaderboard. This model is not designed to excel in traditional benchmarks, as the code and role playing models generally do not apply to those contexts. Nonetheless, it performs remarkably well thanks to strong general purpose experts. Nous Beyonder 4x7B v3 is one of the best models on Nous benchmark suite evaluation performed using LLM AutoEval and significantly outperforms the v2. See the entire leaderboard here. EQ Bench It s also the best 4x7B model on the EQ Bench leaderboard, outperforming older versions of ChatGPT and Llama 2 70b chat. Beyonder is very close to Mixtral 8x7B Instruct v0.1 and Gemini Pro, which are supposedly much bigger models. Open LLM Leaderboard Finally, it s also a strong performer on the Open LLM Leaderboard, significantly outperforming the v2 model. On top of these quantitative evaluations, I recommend checking the model s outputs in a more qualitative way using a GGUF version on LM Studio. A common way of testing these models is to gather a private set of questions and check their outputs. With this strategy, I found that Beyonder 4x7B v3 is quite robust to changes in the user and system prompts compared to other models, including AlphaMonarch 7B. This is pretty cool as it improves the usefulness of the model in general. FrankenMoEs are a promising but still experimental approach. The trade offs, like higher VRAM demand and slower inference speeds, can make it challenging to see their advantage over simpler merging techniques like SLERP or DARE TIES. Especially, when you use frankenMoEs with just two experts, they might not perform as well as if you had simply merged the two models. However, frankenMoEs excel in preserving knowledge, which can result in stronger models, as demonstrated by Beyonder 4x7B v3. With the right hardware, these drawbacks can be effectively mitigated. Conclusion In this article, we introduced the Mixture of Experts architecture. Unlike traditional MoEs that are trained from scratch, MergeKit facilitates the creation of MoEs by ensembling experts, offering an innovative approach to improving model performance and efficiency. We detailed the process of creating a frankenMoE with MergeKit, highlighting the practical steps involved in selecting and combining different experts to produce a high quality MoE. Thanks for reading this article. I encourage you to try to make your own FrankenMoEs using LazyMergeKit select a few models, create your config based Beyonder s, and run the notebook to create your own models! If you liked this article, please follow me on Hugging Face and X Twitter maximelabonne. References Mixtral of Experts by Jiang et al. 2023 Mixture of Experts for Clowns by Charles Goddard 2023 Mixture of Experts Explained by Sanseviero et al. 2023 Adaptive Mixture of Local Experts by Jacobs et al. 1991 Sparse Upcycling Training Mixture of Experts from Dense Checkpoints by Komatsuzaki et al. 2022 _Learn more about machine learning and support my work with one click become a Medium member here _ Join Medium with my referral link Maxime Labonne _As a Medium member, a portion of your membership fee goes to writers you read, and you get full access to every story _medium.com 1 Share this post Maxime Labonne Create Mixtures of Experts with MergeKit Copy link Facebook Email Notes More Share Discussion about this post Comments Restacks Top Latest Discussions No posts Ready for more? Subscribe 2024 Maxime Labonne Privacy Terms Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Notes More This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts en",
            "platform": "maximelabonne.substack.com",
            "author_id": "c7167a83-e446-428d-9a2e-6e88a6b8bc6f",
            "author_full_name": "Hiep Nguyen",
            "link": "https://maximelabonne.substack.com/p/create-mixtures-of-experts-with-mergekit-11b318c99562"
        },
        {
            "id": "29934a95-bc07-4987-af8e-c6f778515cc2",
            "content": "Merge Large Language Models with mergekit Create your own models easily, no GPU required! Maxime Labonne SubscribeSign in Share this post Maxime Labonne Merge Large Language Models with mergekit Copy link Facebook Email Notes More Merge Large Language Models with mergekit Create your own models easily, no GPU required! Maxime Labonne Jan 08, 2024 1 Share this post Maxime Labonne Merge Large Language Models with mergekit Copy link Facebook Email Notes More Share Create your own models easily, no GPU required! Image by author Model merging is a technique that combines two or more LLMs into a single model. It s a relatively new and experimental method to create new models for cheap no GPU required . Model merging works surprisingly well and produced many state of the art models on the Open LLM Leaderboard. In this tutorial, we will implement it using the mergekit library. More specifically, we will review four merge methods and provide examples of configurations. Then, we will use mergekit to create our own model, Marcoro14 7B slerp, which became the best performing model on the Open LLM Leaderboard 02 01 24 . The code is available on GitHub and Google Colab. I recommend using my automated notebook to easily run mergekit LazyMergekit. _A special thanks toCharles Goddard, the author of the mergekit library, for reviewing this article._ Image by author Merge algorithms In this section, we will focus on four methods currently implemented in mergekit. Note that there are other methods, such as linear and Task Arithmetic. If you re interested in papers on model merging, I recommend this excellent collection on Hugging Face. 1 . SLERP Spherical Linear Interpolation SLERP is a method used to smoothly interpolate between two vectors. It maintains a constant rate of change and preserves the geometric properties of the spherical space in which the vectors reside. There are several reasons to prefer SLERP over a traditional linear interpolation. For example, in high dimensional spaces, linear interpolation can lead to a decrease in the magnitude of the interpolated vector i.e., it reduces the scale of weights . Moreover, the change in direction of the weights often represents more meaningful information like feature learning and representation than the magnitude of change. SLERP is implemented using the following steps 1. Normalize the input vectors to unit length, ensuring they represent directions rather than magnitudes 2. Calculate the angle between these vectors using their dot product. 3. If the vectors are nearly collinear, it defaults to linear interpolation for efficiency. Otherwise, SLERP computing scale factors based on the interpolation factor t t 0 100 of the first vector, t 1 100 of model 2 and the angle between the vectors. 4. These factors are used to weigh the original vectors, which are then summed to obtain the interpolated vector. SLERP is currently the most popular merging method, but it is limited to combining only two models at a time. It is still possible to hierarchically combine multiple models, as shown in Mistral 7B Merge 14 v0.1. _Example of configuration _ slices sources model OpenPipe mistral ft optimized 1218 layer_range 0, 32 model mlabonne NeuralHermes 2.5 Mistral 7B layer_range 0, 32 merge_method slerp base_model OpenPipe mistral ft optimized 1218 parameters t filter self_attn value 0, 0.5, 0.3, 0.7, 1 filter mlp value 1, 0.5, 0.7, 0.3, 0 value 0.5 dtype bfloat16 This is a classic SLERP configuration, applied to every layer of both models. Note that we input a gradient of values for the interpolation factor t . The parameters for the self attention and MLP layers will use different combinations of OpenPipe mistral ft optimized 1218 and mlabonne NeuralHermes 2.5 Mistral 7B. The other layers are a 50 50 mixture of the two models. You can find the final model on the Hugging Face Hub at mlabonne NeuralPipe 7B slerp. 2 . TIES Introduced in this paper by Yadav et al., TIES Merging is designed to efficiently merge multiple task specific models into a single multitask model. It addresses two main challenges in model merging Redundancy in model parameters It identifies and eliminates redundant parameters within task specific models. This is achieved by focusing on the changes made during fine tuning, identifying the top k most significant changes, and discarding the rest. Disagreement between parameter signs Conflicts arise when different models suggest opposing adjustments to the same parameter. TIES Merging resolves these conflicts by creating a unified sign vector that represents the most dominant direction of change across all models. TIES Merging is divided into the following three steps 1. Trim Reduces redundancy in task specific models by retaining only a fraction the most significant parameters density parameter and resetting the rest to zero. 2. Elect Sign Resolves sign conflicts across different models by creating a unified sign vector based on the most dominant direction positive or negative in terms of cumulative magnitude. 3. Disjoint Merge Averages parameter values that align with the unified sign vector, excluding zero values. Unlike SLERP, TIES can merge multiple models at a time. _Example of configuration _ models model mistralai Mistral 7B v0.1 no parameters necessary for base model model OpenPipe mistral ft optimized 1218 parameters density 0.5 weight 0.5 model mlabonne NeuralHermes 2.5 Mistral 7B parameters density 0.5 weight 0.3 merge_method ties base_model mistralai Mistral 7B v0.1 parameters normalize true dtype float16 With this config, we use Mistral 7B as a base model to calculate the delta weights. We merge the same two models mistral ft optimized 1218 50 and NeuralHermes 2.5 Mistral 7B 30 with normalization. Here, the density means that we re only retaining 50 of the parameters of each model the other half comes from the base model . Note that the sum of the weights is not equal to 1 in the config, but the normalize true parameter will automatically normalize them internally. This config is inspired by the parameters provided by the author of OpenHermes 2.5 neural chat 7b v3 1 7B. You can find the final model on the Hugging Face Hub at mlabonne NeuralPipe 7B ties. 3 . DARE Introduced by Yu et al. 2023 , DARE uses an approach similar to TIES with two main differences Pruning DARE randomly reset fine tuned weights to their original values those of the base model . Rescaling DARE rescales the weights to keep the expectations of model outputs approximately unchanged. It adds the rescaled weights of both or more models to the weights of the base model with a scale factor. Mergekit s implementation of this method has two flavors with the sign election step of TIES dare_ties or without dare_linear . _Example of configuration _ models model mistralai Mistral 7B v0.1 No parameters necessary for base model model samir fama SamirGPT v1 parameters density 0.53 weight 0.4 model abacusai Slerp CM mist dpo parameters density 0.53 weight 0.3 model EmbeddedLLM Mistral 7B Merge 14 v0.2 parameters density 0.53 weight 0.3 merge_method dare_ties base_model mistralai Mistral 7B v0.1 parameters int8_mask true dtype bfloat16 In this configuration, we merge three different models based on Mistral 7B using dare_ties . This time, I chose weights that sum to 1 the sum should be between 0.9 and 1.1 . The density parameter is a little higher than what s recommended in the paper 0.5 , but it looks like it gives consistently better results see this discussion . You can find it on the Hugging Face Hub at mlabonne Daredevil 7B. It s also the best merge model in this article, outperforming even Marcoro14 7B slerp. 4 . Passthrough The passthrough method differs significantly from the previous ones. By concatenating layers from different LLMs, it can produce models with an exotic number of parameters e.g., 9B with two 7B parameter models . These models are often referred to as frankenmerges or Frankenstein models by the community. This technique is very experimental, but it managed to create impressive models, like goliath 120b using two Llama 2 70B models. The recently released SOLAR 10.7B v1.0 also uses the same idea, called depth up scaling in their paper. _Example of configuration _ slices sources model OpenPipe mistral ft optimized 1218 layer_range 0, 32 sources model mlabonne NeuralHermes 2.5 Mistral 7B layer_range 24, 32 merge_method passthrough dtype bfloat16 The resulting frankenmerge will have all the 32 layers from the first model and 8 additional layers from the second model. This creates a frankenmerge with a total of 40 layers and 8.99B parameters. This config is inspired by GML Mistral merged v1. You can find the final model on the Hugging Face Hub at mlabonne NeuralPipe 9B merged. Merge your own models In this section, we will use mergekit to load a merge configuration, run it, and upload the resulting model to the Hugging Face Hub. First of all, we install mergekit directly from source as follows !git clone https github.com cg123 mergekit.git !cd mergekit pip install q e . In the following block, we load the merge configuration in a YAML format. We also specify the name of the merged model for future use. You can copy paste any configuration from the previous section here. This time, we will use two different models Marcoroni 7B v3 and Mistral 7B Merge 14 v0.1 and merge them with the SLERP method. We save the config as a yaml file to be used as input in the merge command. import yaml MODEL_NAME Marcoro14 7B slerp yaml_config slices sources model AIDC ai business Marcoroni 7B v3 layer_range 0, 32 model EmbeddedLLM Mistral 7B Merge 14 v0.1 layer_range 0, 32 merge_method slerp base_model AIDC ai business Marcoroni 7B v3 parameters t filter self_attn value 0, 0.5, 0.3, 0.7, 1 filter mlp value 1, 0.5, 0.7, 0.3, 0 value 0.5 dtype bfloat16 Save config as yaml file with open config.yaml , w , encoding utf 8 as f f.write yaml_config We run the merge command with the following parameters copy tokenizer to copy the tokenizer from the base model allow crimes and out shard size to chunk the models into smaller shards that can be computed on a CPU with low RAM lazy unpickle to enable the experimental lazy unpickler for lower memory usage In addition, some models can require the trust_remote_code flag this is not the case with Mistral 7B . This command will download the weights of all the models listed in the merge configuration and run the selected merge method it should take 10 minutes . Merge models !mergekit yaml config.yaml merge copy tokenizer allow crimes out shard size 1B lazy unpickl The model is now merged and saved in the merge directory. Before uploading it, we can create a README file with all the information required for reproducibility. The following code block defines a Jinja template and automatically fills it with the data from the merge configuration. !pip install qU huggingface_hub from huggingface_hub import ModelCard, ModelCardData from jinja2 import Template username mlabonne template_text license apache 2.0 tags merge mergekit lazymergekit for model in models model endfor model_name model_name is a merge of the following models using mergekit https github.com cg123 mergekit for model in models model https huggingface.co model endfor Configuration yaml yaml_config Create a Jinja template object jinja_template Template template_text.strip Get list of models from config data yaml.safe_load yaml_config if models in data models data models i model for i in range len data models if parameters in data models i elif parameters in data models data slices 0 sources i model for i in range len data slices 0 sources elif slices in data models data slices i sources 0 model for i in range len data slices else raise Exception No models or slices found in yaml config Fill the template content jinja_template.render model_name MODEL_NAME, models models, yaml_config yaml_config, username username, Save the model card card ModelCard content card.save merge README.md Now that we have a model card, we can push the entire folder to the Hub. from google.colab import userdata from huggingface_hub import HfApi username mlabonne Defined in the secrets tab in Google Colab api HfApi token userdata.get HF_TOKEN api.create_repo repo_id f username MODEL_NAME , repo_type model api.upload_folder repo_id f username MODEL_NAME , folder_path merge , The model is now available on the Hugging Face Hub at mlabonne Marcoro14 7B slerp. In another notebook, we can try the model on a free T4 GPU using the following code !pip install qU transformers accelerate from transformers import AutoTokenizer import transformers import torch model mlabonne Marcoro14 7B slerp messages role user , content What is a large language model? tokenizer AutoTokenizer.from_pretrained model prompt tokenizer.apply_chat_template messages, tokenize False, add_generation_prompt True pipeline transformers.pipeline text generation , model model, torch_dtype torch.float16, device_map auto , outputs pipeline prompt, max_new_tokens 256, do_sample True, temperature 0.7, top_k 50, top_p 0.95 We re asking the question What is a Large Language Model? and received this output _A large language model is a type of artificial intelligence AI system that has been trained on vast amounts of text data. It s designed to understand and generate human like language, making predictions on what words or phrases might come next in a sentence or document. These models use complex algorithms and neural network architectures to learn from the data and improve their performance over time. Some well known large language models include GPT 3 from OpenAI and BERT from Google._ It s looking good, but we need a more comprehensive evaluation. For this kind of general purpose model, there are a few interesting benchmarks Chatbot Arena , which compiles an Elo based LLM leaderboard based on human votes. MT bench same link , which uses GPT 4 as a judge to grade model responses on a set of multi turn questions. NousResearch benchmark suite , which aggregates four benchmarks AGIEval, GPT4ALL, TruthfulQA, and Bigbench. GPT4ALL itself includes HellaSwag, OpenBookQA, Winogrande, ARC Easy, ARC Challenge, BoolQ, and PIQA. Open LLM Leaderboard , which aggregates six benchmarks ARC, HellaSwag, MMLU, Winogrande, GSM8K, and TruthfulQA. Unfortunately, we can t submit our model to the Chatbot Arena. Instead, I chose to evaluate it using the Open LLM Leaderboard and NousResearch benchmarks. I submitted our model to the Open LLM Leaderboard Submit here! tab . As shown in the introduction, it ranked as the best 7B parameter model on the leaderboard. Here are the complete results Image by author The problem with the Open LLM Leaderboard is that these benchmarks are public. It means that people can train LLMs on the test data to get better results. By merging the best models, we also contaminate our own results. It is safe to assume that Marcoro14 7B slerp is contaminated and some models used in this merge have been trained on the test set. If you want to create the best model and not hack the leaderboard, I recommend only using non merge models to create your own merges. This is why we don t want to only rely on the OpenLLM Leaderboard. For NousResearch benchmark suite, I used LLM AutoEval to compute the scores automatically with a simple Colab notebook. Here are the results compared to the excellent OpenHermes 2.5 Mistral 7B Image by author We get a significant improvement over this model on every benchmark . Note that NousResearch benchmark suite shares some tasks with the Open LLM Leaderboard ARC Challenge, TruthfulQA, HellaSwag, and Winogrande. To the best of my knowledge, Bigbench is the only benchmark that is 100 different feel free to contact me if that s not the case . However, one of the models we used in this merge could still have been trained on Bigbench. Conclusion In this article, we introduced the concept of merging LLMs with four different methods. We detailed how SLERP, TIES, DARE, and passthrough work and provided examples of configurations. Finally, we ran SLERP with mergekit to create Marcoro14 7B slerp and upload it to the Hugging Face Hub. We obtained excellent performance on two benchmark suites Open LLM Leaderboard best performing 7B model and NousResearch. If you want to create your own merges, I recommend using my automated notebook LazyMergekit. Another way of combining multiple models is to merge them in a Mixture of Experts MoE architecture. In the next article, we ll discuss how to do this in detail and create our own Mixtral like model. If you liked this article, please follow me on Medium and Twitter maximelabonne. _Learn more about machine learning and support my work with one click become a Medium member here _ Join Medium with my referral link Maxime Labonne _As a Medium member, a portion of your membership fee goes to writers you read, and you get full access to every story _medium.com 1 Share this post Maxime Labonne Merge Large Language Models with mergekit Copy link Facebook Email Notes More Share Discussion about this post Comments Restacks Top Latest Discussions No posts Ready for more? Subscribe 2024 Maxime Labonne Privacy Terms Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Notes More This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts en",
            "platform": "maximelabonne.substack.com",
            "author_id": "c7167a83-e446-428d-9a2e-6e88a6b8bc6f",
            "author_full_name": "Hiep Nguyen",
            "link": "https://maximelabonne.substack.com/p/merge-large-language-models-with-mergekit-2118fb392b54"
        },
        {
            "id": "815dfbe2-10be-420c-9fd5-086f55cd948a",
            "content": "Fine tune a Mistral 7b model with Direct Preference Optimization Boost the performance of your supervised fine tuned models Maxime Labonne SubscribeSign in Share this post Maxime Labonne Fine tune a Mistral 7b model with Direct Preference Optimization Copy link Facebook Email Notes More Fine tune a Mistral 7b model with Direct Preference Optimization Boost the performance of your supervised fine tuned models Maxime Labonne Jan 01, 2024 1 Share this post Maxime Labonne Fine tune a Mistral 7b model with Direct Preference Optimization Copy link Facebook Email Notes More Share Boost the performance of your supervised fine tuned models Image by author Pre trained Large Language Models LLMs can only perform next token prediction, making them unable to answer questions. This is why these base models are then fine tuned on pairs of instructions and answers to act as helpful assistants. However, this process can still be flawed fine tuned LLMs can be biased, toxic, harmful, etc. This is where Reinforcement Learning from Human Feedback RLHF comes into play. RLHF provides different answers to the LLM, which are ranked according to a desired behavior helpfulness, toxicity, etc. . The model learns to output the best answer among these candidates, hence mimicking the behavior we want to instill. Often seen as a way to censor models, this process has recently become popular for improving performance, as shown in neural chat 7b v3 1. In this article, we will create NeuralHermes 2.5, by fine tuning OpenHermes 2.5 using a RLHF like technique Direct Preference Optimization DPO . For this purpose, we will introduce a preference dataset, describe how the DPO algorithm works, and apply it to our model. We ll see that it significantly improves the performance of the base model on the Open LLM Leaderboard. As per usual, the code is available on GitHub and Google Colab. _ Update Jessie Davids, a reader who used this article and code, managed to create the best performing model on the Open LLM Leaderboard 7B param. Congrats to him! _ Image by author Preference datasets Preference datasets are not standardized, but they typically consist of a collection of answers that are ranked by humans. This ranking is essential, as the RLHF process fine tunes LLMs to output the preferred answer. Here is an example of Anthropic hh rlhf, a popular preference dataset Image by author The structure of the dataset is straightforward for each row, there is one chosen preferred answer, and one rejected answer. The goal of RLHF is to guide the model to output the preferred answer. Preference datasets are notoriously costly and difficult to make, as they require collecting manual feedback from humans. This feedback is also subjective and can easily be biased toward confident but wrong answers or contradict itself different annotators have different values . Over time, several solutions have been proposed to tackle these issues, such as replacing human feedback with AI feedback RLAIF . These datasets also tend to be a lot smaller than fine tuning datasets. To illustrate this, the excellent neural chat 7b v3 1 best 7B LLM on the Open LLM Leaderboard when it was released uses 518k samples for fine tuning Open Orca SlimOrca but only 12.9k samples for RLHF Intel orca_dpo_pairs . In this case, the authors generated answers with GPT 4 3.5 to create the preferred answers, and with Llama 2 13b chat to create the rejected responses. It s a smart way to bypass human feedback and only rely on models with different levels of performance. Direct Preference Optimization While the concept of RLHF has been used in robotics for a long time, it was popularized for LLMs in OpenAI s paper Fine Tuning Language Models from Human Preferences. In this paper, the authors present a framework where a reward model is trained to approximate human feedback. This reward model is then used to optimize the fine tuned model s policy using the Proximal Policy Optimization PPO algorithm. Image by author The core concept of PPO revolves around making smaller, incremental updates to the policy, as larger updates can lead to instability or suboptimal solutions. From experience, this technique is unfortunately still unstable loss diverges , difficult to reproduce numerous hyperparameters, sensitive to random seeds , and computationally expensive. This is where Direct Preference Optimization DPO comes into play. DPO simplifies control by treating the task as a classification problem. Concretely, it uses two models the trained model or policy model and a copy of it called the reference model . During training, the goal is to make sure the trained model outputs higher probabilities for preferred answers than the reference model. Conversely, we also want it to output lower probabilities for rejected answers. It means we re penalizing the LLM for bad answers and rewarding it for good ones. Image by author By using the LLM itself as a reward model and employing binary cross entropy objectives, DPO efficiently aligns the model s outputs with human preferences without the need for extensive sampling, reward model fitting, or intricate hyperparameter adjustments. It results in a more stable, more efficient, and computationally less demanding process. Formatting the data In this example, we ll fine tune the excellent OpenHermes 2.5 Mistral 7B, which is a Mistral 7b model that was only supervised fine tuned. To this end, we ll use the Intel orca_dpo_pairs dataset to align our model and improve its performance. We call this new model NeuralHermes 2.5 Mistral 7B. The first step consists of installing the required libraries as follows. pip install q datasets trl peft bitsandbytes sentencepiece wandb Once it s done, we can import the libraries. I m also using the secrets tab in Google Colab to store my Hugging Face token. import os import gc import torch import transformers from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig from datasets import load_dataset from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training from trl import DPOTrainer import bitsandbytes as bnb from google.colab import userdata import wandb Defined in the secrets tab in Google Colab hf_token userdata.get huggingface wb_token userdata.get wandb wandb.login key wb_token model_name teknium OpenHermes 2.5 Mistral 7B new_model NeuralHermes 2.5 Mistral 7B OpenHermes 2.5 Mistral 7B uses a specific chat template, called ChatML. Here is an example of a conversation formatted with this template im_start system You are a helpful chatbot assistant. im_end im_start user Hi im_end im_start assistant Hi, how can I help you? im_end As you can see, ChatML defines different roles system, user, assistant and appends special tokens im_start and im_end to separate them. Moreover, DPOTrainer also requires a specific format with three columns prompt, chosen, and rejected. Our dataset contains four columns system, question, chatgpt, and llama2 13b chat. We ll simply concatenate the system and question columns to the prompt column. We ll also map the chatgpt column to chosen and llama2 13b chat to rejected . To format the dataset in a reliable way, we ll use the tokenizer s apply_chat_template function, which already uses ChatML. def chatml_format example Format system if len example system 0 message role system , content example system system tokenizer.apply_chat_template message , tokenize False else system Format instruction message role user , content example question prompt tokenizer.apply_chat_template message , tokenize False, add_generation_prompt True Format chosen answer chosen example chosen im_end n Format rejected answer rejected example rejected im_end n return prompt system prompt, chosen chosen, rejected rejected, Load dataset dataset load_dataset Intel orca_dpo_pairs train Save columns original_columns dataset.column_names Tokenizer tokenizer AutoTokenizer.from_pretrained model_name tokenizer.pad_token tokenizer.eos_token tokenizer.padding_side left Format dataset dataset dataset.map chatml_format, remove_columns original_columns Let s print a sample of the formatted dataset to confirm that everything works as expected prompt im_start system nYou are an AI assistant. You will be given a task. You must generate a detailed and long answer. im_end n im_start user nGenerate an approximately fifteen word sentence that describes all this data Midsummer House eatType restaurant Midsummer House food Chinese Midsummer House priceRange moderate Midsummer House customer rating 3 out of 5 Midsummer House near All Bar One im_end n im_start assistant n , chosen Midsummer House is a moderately priced Chinese restaurant with a 3 5 customer rating, located near All Bar One. im_end n , rejected Sure! Here s a sentence that describes all the data you provided n n Midsummer House is a moderately priced Chinese restaurant with a customer rating of 3 out of 5, located near All Bar One, offering a variety of delicious dishes. im_end n We can see that the prompt combines system and user instructions. Thanks to the add_generation_prompt True argument, it also appends the beginning of the assistant s answer. If you want to skip this step, you can directly used the preprocessed dataset as mlabonne chatml_dpo_pairs. Training the model with DPO Next, we define the LoRA configurations to train the model. As described in Intel s blog post, we set the rank value to be equal to the lora_alpha , which is unusual 2 r as a rule of thumb . We also target all the linear modules with adapters. LoRA configuration peft_config LoraConfig r 16, lora_alpha 16, lora_dropout 0.05, bias none , task_type CAUSAL_LM , target_modules k_proj , gate_proj , v_proj , up_proj , q_proj , o_proj , down_proj We re now ready to load the model we want to fine tune with DPO. In this case, two models are required the model to fine tune as well as the reference model. This is mostly for the sake of readability, as the DPOTrainer object automatically creates a reference model if none is provided. Model to fine tune model AutoModelForCausalLM.from_pretrained model_name, torch_dtype torch.float16, load_in_4bit True model.config.use_cache False Reference model ref_model AutoModelForCausalLM.from_pretrained model_name, torch_dtype torch.float16, load_in_4bit True The final step consists of providing all the hyperparameters to TrainingArguments and DPOTrainer Among them, the beta parameter is unique to DPO since it controls the divergence from the initial policy 0.1 is a typical value for it . Compared to the values described in Intel s blog post, we lower the learning rate from 5e 4 to 5e 5 and the number of steps from 1,000 to 200 . I manually optimized these values after a few runs to stabilize training and achieve the best results. We can now start training the model. Note that it requires an A100 GPU and takes between 1 hour to complete the training. Training arguments training_args TrainingArguments per_device_train_batch_size 4, gradient_accumulation_steps 4, gradient_checkpointing True, learning_rate 5e 5, lr_scheduler_type cosine , max_steps 200, save_strategy no , logging_steps 1, output_dir new_model, optim paged_adamw_32bit , warmup_steps 100, bf16 True, report_to wandb , Create DPO trainer dpo_trainer DPOTrainer model, ref_model, args training_args, train_dataset dataset, tokenizer tokenizer, peft_config peft_config, beta 0.1, max_prompt_length 1024, max_length 1536, Fine tune model with DPO dpo_trainer.train Our model is now fine tuned. You can check the project on Weights Biases at this address. Here are some interesting metrics to analyze Image by author Interestingly, the training loss quickly drops to zero before 50 steps , despite 100 warmup steps. Meanwhile, the other metrics keep evolving. The train rewards chosen and train rewards rejected plots correspond to the mean difference between the log probabilities output by the trained and reference models. It makes sense that, over time, they diverge as our trained model learns the preferred answers. The train rewards margins plot also shows the difference between these two plots. Finally, the train reward accuracies plot shows the frequency of choosing the preferred answer. The trained model quickly reaches a perfect accuracy score, which is a good sign but could also mean that the difference between preferred and rejected answers is too obvious. Now that it s trained, we can merge the adapter with the original model. Next, we save the merged model and the tokenizer before pushing it to the Hugging Face Hub. Save artifacts dpo_trainer.model.save_pretrained final_checkpoint tokenizer.save_pretrained final_checkpoint Flush memory del dpo_trainer, model, ref_model gc.collect torch.cuda.empty_cache Reload model in FP16 instead of NF4 base_model AutoModelForCausalLM.from_pretrained model_name, return_dict True, torch_dtype torch.float16, tokenizer AutoTokenizer.from_pretrained model_name Merge base model with the adapter model PeftModel.from_pretrained base_model, final_checkpoint model model.merge_and_unload Save model and tokenizer model.save_pretrained new_model tokenizer.save_pretrained new_model Push them to the HF Hub model.push_to_hub new_model, use_temp_dir False, token hf_token tokenizer.push_to_hub new_model, use_temp_dir False, token hf_token Let s see how our model performs in a real test. We ll format the prompt to ask a basic question What is a Large Language Model? Format prompt message role system , content You are a helpful assistant chatbot. , role user , content What is a Large Language Model? tokenizer AutoTokenizer.from_pretrained new_model prompt tokenizer.apply_chat_template message, add_generation_prompt True, tokenize False Create pipeline pipeline transformers.pipeline text generation , model new_model, tokenizer tokenizer Generate text sequences pipeline prompt, do_sample True, temperature 0.7, top_p 0.9, num_return_sequences 1, max_length 200, print sequences 0 generated_text Here s the answer from the model A large language model is a type of artificial intelligence AI system that has been trained on vast amounts of text data. These models are designed to understand and generate human language, allowing them to perform various natural language processing tasks, such as text generation, language translation, and question answering. Large language models typically use deep learning techniques, like recurrent neural networks RNNs or transformers, to learn patterns and relationships in the data, enabling them to generate coherent and contextually relevant responses. The size of these models, in terms of the number of parameters and the volume of data they are trained on, plays a significant role in their ability to comprehend and produce complex language structures. Everything seems to be working, we can now evaluate the merged model. As this is a general purpose model, we can leverage the lm evaluation harness to evaluate it. As the process is quite resource intensive, we can also directly submit it for evaluation on the Open LLM Leaderboard. It took a few days, but here are the results compared to other OpenHermes models Image by author Compared to the original model, NeuralHermes 2 5 Mistral 7B model improved the average score by 6.7 points particularly on GSM8K . This is an unexpectedly large improvement, which showcases the power of Direct Preference Optimization. Conclusion In this article, we fine tuned an already supervised fine tuned model using DPO and created our own NeuralHermes 2.5 model. By leveraging a high quality preference dataset, we created a sample efficient fine tuning pipeline that produced a significant improvement on the Open LLM Leaderboard. If you want to give it a try, you can find quantized variants of this model or use this Hugging Face Space. Note that our fine tuning pipeline can still be improved in different ways. For example, the preference dataset is still quite raw and could be improved with more filtering and by using different models. In addition, numerous hyperparameters can still be tweaked to achieve better results. In particular, the learning rate can still be lowered to train the model on more steps and inject more preference data. References Fine tune Llama 2 with DPO by Kashif Rasul, Younes Belkada, and Leandro von Werra. Supervised Fine Tuning and Direct Preference Optimization on Intel Gaudi2 by Kaokao Lv, Wenxin Zhang, and Haihao Shen. llama2 fine tune by mzbac. _Learn more about machine learning and support my work with one click become a Medium member here _ Join Medium with my referral link Maxime Labonne _As a Medium member, a portion of your membership fee goes to writers you read, and you get full access to every story _medium.com 1 Share this post Maxime Labonne Fine tune a Mistral 7b model with Direct Preference Optimization Copy link Facebook Email Notes More Share Discussion about this post Comments Restacks Top Latest Discussions No posts Ready for more? Subscribe 2024 Maxime Labonne Privacy Terms Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Notes More This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts en",
            "platform": "maximelabonne.substack.com",
            "author_id": "c7167a83-e446-428d-9a2e-6e88a6b8bc6f",
            "author_full_name": "Hiep Nguyen",
            "link": "https://maximelabonne.substack.com/p/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac"
        },
        {
            "id": "f33c21af-f9ca-40e1-aa49-a45952d771a1",
            "content": "ExLlamaV2 The Fastest Library to Run LLMs Quantize and run EXL2 models Maxime Labonne SubscribeSign in Share this post Maxime Labonne ExLlamaV2 The Fastest Library to Run LLMs Copy link Facebook Email Notes More ExLlamaV2 The Fastest Library to Run LLMs Quantize and run EXL2 models Maxime Labonne Nov 20, 2023 Share this post Maxime Labonne ExLlamaV2 The Fastest Library to Run LLMs Copy link Facebook Email Notes More Share Quantize and run EXL2 models Image by author Quantizing Large Language Models LLMs is the most popular approach to reduce the size of these models and speed up inference. Among these techniques, GPTQ delivers amazing performance on GPUs. Compared to unquantized models, this method uses almost 3 times less VRAM while providing a similar level of accuracy and faster generation. It became so popular that it has recently been directly integrated into the transformers library. ExLlamaV2 is a library designed to squeeze even more performance out of GPTQ. Thanks to new kernels, it s optimized for blazingly fast inference. It also introduces a new quantization format, EXL2, which brings a lot of flexibility to how weights are stored. In this article, we will see how to quantize base models in the EXL2 format and how to run them. As usual, the code is available on GitHub and Google Colab. Quantize EXL2 models To start our exploration, we need to install the ExLlamaV2 library. In this case, we want to be able to use some scripts contained in the repo, which is why we will install it from source as follows git clone https github.com turboderp exllamav2 pip install exllamav2 Now that ExLlamaV2 is installed, we need to download the model we want to quantize in this format. Let s use the excellent zephyr 7B beta, a Mistral 7B model fine tuned using Direct Preference Optimization DPO . It claims to outperform Llama 2 70b chat on the MT bench, which is an impressive result for a model that is ten times smaller. You can try out the base Zephyr model using this space. We download zephyr 7B beta using the following command this can take a while since the model is about 15 GB git lfs install git clone https huggingface.co HuggingFaceH4 zephyr 7b beta GPTQ also requires a calibration dataset , which is used to measure the impact of the quantization process by comparing the outputs of the base model and its quantized version. We will use the wikitext dataset and directly download the test file as follows wget https huggingface.co datasets wikitext resolve 9a9e482b5987f9d25b3a9b2883fc6cc9fd8071b3 wikitext 103 v1 wikitext test.parquet Once it s done, we can leverage the convert.py script provided by the ExLlamaV2 library. We re mostly concerned with four arguments i Path of the base model to convert in HF format FP16 . o Path of the working directory with temporary files and final output. c Path of the calibration dataset in Parquet format . b Target average number of bits per weight bpw . For example, 4.0 bpw will give store weights in 4 bit precision. The complete list of arguments is available on this page. Let s start the quantization process using the convert.py script with the following arguments mkdir quant python python exllamav2 convert.py i base_model o quant c wikitext test.parquet b 5.0 Note that you will need a GPU to quantize this model. The official documentation specifies that you need approximately 8 GB of VRAM for a 7B model, and 24 GB of VRAM for a 70B model. On Google Colab, it took me 2 hours and 10 minutes to quantize zephyr 7b beta using a T4 GPU. Under the hood, ExLlamaV2 leverages the GPTQ algorithm to lower the precision of the weights while minimizing the impact on the output. You can find more details about the GPTQ algorithm in this article. So why are we using the EXL2 format instead of the regular GPTQ format? EXL2 comes with a few new features It supports different levels of quantization it s not restricted to 4 bit precision and can handle 2, 3, 4, 5, 6, and 8 bit quantization. It can mix different precisions within a model and within each layer to preserve the most important weights and layers with more bits. ExLlamaV2 uses this additional flexibility during quantization. It tries different quantization parameters and measures the error they introduce. On top of trying to minimize the error, ExLlamaV2 also has to achieve the target average number of bits per weight given as an argument. Thanks to this behavior, we can create quantized models with an average number of bits per weight of 3.5 or 4.5 for example. The benchmark of different parameters it creates is saved in the measurement.json file. The following JSON shows the measurement for one layer key model.layers.0.self_attn.q_proj , numel 16777216, options desc 0.05 3b 0.95 2b 32g s4 , bpw 2.1878662109375, total_bits 36706304.0, err 0.011161142960190773, qparams group_size 32, bits 3, 2 , bits_prop 0.05, 0.95 , scale_bits 4 , In this trial, ExLlamaV2 used 5 of 3 bit and 95 of 2 bit precision for an average value of 2.188 bpw and a group size of 32. This introduced a noticeable error that is taken into account to select the best parameters. Running ExLlamaV2 for Inference Now that our model is quantized, we want to run it to see how it performs. Before that, we need to copy essential config files from the base_model directory to the new quant directory. Basically, we want every file that is not hidden . or a safetensors file. Additionally, we don t need the out_tensor directory that was created by ExLlamaV2 during quantization. In bash, you can implement this as follows !rm rf quant out_tensor !rsync av exclude .safetensors exclude . . base_model . quant Our EXL2 model is ready and we have several options to run it. The most straightforward method consists of using the test_inference.py script in the ExLlamaV2 repo note that I don t use a chat template here python exllamav2 test_inference.py m quant p I have a dream The generation is very fast 56.44 tokens second on a T4 GPU , even compared to other quantization techniques and tools like GGUF llama.cpp or GPTQ. You can find an in depth comparison between different solutions in this excellent article from oobabooga. In my case, the LLM returned the following output Model quant Options rope_scale 1.0 , rope_alpha 1.0 Loading model... Loading tokenizer... Warmup... Generating... I have a dream. user Wow, that s an amazing speech! Can you add some statistics or examples to support the importance of education in society? It would make it even more persuasive and impactful. Also, can you suggest some ways we can ensure equal access to quality education for all individuals regardless of their background or financial status? Let s make this speech truly unforgettable! Absolutely! Here s your updated speech Dear fellow citizens, Education is not just an academic pursuit but a fundamental human right. It empowers people, opens doors Response generated in 3.40 seconds, 128 tokens, 37.66 tokens second includes prompt eval. Alternatively, you can use a chat version with the chatcode.py script for more flexibility python exllamav2 examples chatcode.py m quant mode llama If you re planning to use an EXL2 model more regularly, ExLlamaV2 has been integrated into several backends like oobabooga s text generation web UI. Note that it requires FlashAttention 2 to work properly, which requires CUDA 12.1 on Windows at the moment something you can configure during the installation process . Now that we tested the model, we re ready to upload it to the Hugging Face Hub. You can change the name of your repo in the following code snippet and simply run it. from huggingface_hub import notebook_login from huggingface_hub import HfApi notebook_login api HfApi api.create_repo repo_id f mlabonne zephyr 7b beta 5.0bpw exl2 , repo_type model api.upload_folder repo_id f mlabonne zephyr 7b beta 5.0bpw exl2 , folder_path quant , Great, the model can be found on the Hugging Face Hub. The code in the notebook is quite general and can allow you to quantize different models, using different values of bpw. This is ideal for creating models dedicated to your hardware. Conclusion In this article, we presented ExLlamaV2, a powerful library to quantize LLMs. It is also a fantastic tool to run them since it provides the highest number of tokens per second compared to other solutions like GPTQ or llama.cpp. We applied it to the zephyr 7B beta model to create a 5.0 bpw version of it, using the new EXL2 format. After quantization, we tested our model to see how it performs. Finally, it was uploaded to the Hugging Face Hub and can be found here. If you re interested in more technical content around LLMs, follow me on Medium. Articles about quantization Introduction to Weight Quantization _Reducing the size of Large Language Models with 8 bit quantization_towardsdatascience.com 4 bit Quantization with GPTQ _Quantize your own LLMs using AutoGPTQ_towardsdatascience.com _Learn more about machine learning and support my work with one click become a Medium member here _ Join Medium with my referral link Maxime Labonne _As a Medium member, a portion of your membership fee goes to writers you read, and you get full access to every story _medium.com Share this post Maxime Labonne ExLlamaV2 The Fastest Library to Run LLMs Copy link Facebook Email Notes More Share Discussion about this post Comments Restacks Top Latest Discussions No posts Ready for more? Subscribe 2024 Maxime Labonne Privacy Terms Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Notes More This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts en",
            "platform": "maximelabonne.substack.com",
            "author_id": "c7167a83-e446-428d-9a2e-6e88a6b8bc6f",
            "author_full_name": "Hiep Nguyen",
            "link": "https://maximelabonne.substack.com/p/exllamav2-the-fastest-library-to-run-llms-32aeda294d26"
        },
        {
            "id": "a5cdec3a-2c57-4e9a-8678-66f58fb37d11",
            "content": "Quantize Llama models with GGML and llama.cpp GGML vs. GPTQ vs. NF4 Maxime Labonne SubscribeSign in Share this post Maxime Labonne Quantize Llama models with GGML and llama.cpp Copy link Facebook Email Notes More Quantize Llama models with GGML and llama.cpp GGML vs. GPTQ vs. NF4 Maxime Labonne Sep 04, 2023 Share this post Maxime Labonne Quantize Llama models with GGML and llama.cpp Copy link Facebook Email Notes More Share GGML vs. GPTQ vs. NF4 Image by author Due to the massive size of Large Language Models LLMs , quantization has become an essential technique to run them efficiently. By reducing the precision of their weights, you can save memory and speed up inference while preserving most of the model s performance. Recently, 8 bit and 4 bit quantization unlocked the possibility of running LLMs on consumer hardware . Coupled with the release of Llama models and parameter efficient techniques to fine tune them LoRA, QLoRA , this created a rich ecosystem of local LLMs that are now competing with OpenAI s GPT 3.5 and GPT 4. Besides the naive approach covered in this article, there are three main quantization techniques NF4, GPTQ, and GGML. NF4 is a static method used by QLoRA to load a model in 4 bit precision to perform fine tuning. In a previous article, we explored the GPTQ method and quantized our own model to run it on a consumer GPU. In this article, we will introduce the GGML technique, see how to quantize Llama models, and provide tips and tricks to achieve the best results. You can find the code on Google Colab and GitHub. What is GGML? GGML is a C library focused on machine learning. It was created by Georgi Gerganov, which is what the initials GG stand for. This library not only provides foundational elements for machine learning, such as tensors, but also a unique binary format to distribute LLMs. This format recently changed to GGUF . This new format is designed to be extensible, so that new features shouldn t break compatibility with existing models. It also centralizes all the metadata in one file, such as special tokens, RoPE scaling parameters, etc. In short, it answers a few historical pain points and should be future proof. For more information, you can read the specification at this address. In the rest of the article, we will call GGML models all models that either use GGUF or previous formats. GGML was designed to be used in conjunction with the llama.cpp library, also created by Georgi Gerganov. The library is written in C C for efficient inference of Llama models. It can load GGML models and run them on a CPU . Originally, this was the main difference with GPTQ models, which are loaded and run on a GPU. However, you can now offload some layers of your LLM to the GPU with llama.cpp. To give you an example, there are 35 layers for a 7b parameter model. This drastically speeds up inference and allows you to run LLMs that don t fit in your VRAM. Image by author If command line tools are your thing, llama.cpp and GGUF support have been integrated into many GUIs, like oobabooga s text generation web ui, koboldcpp, LM Studio, or ctransformers. You can simply load your GGML models with these tools and interact with them in a ChatGPT like way. Fortunately, many quantized models are directly available on the Hugging Face Hub. You ll quickly notice that most of them are quantized by TheBloke, a popular figure in the LLM community. In the next section, we will see how to quantize our own models and run them on a consumer GPU. How to quantize LLMs with GGML? Let s look at the files inside of TheBloke Llama 2 13B chat GGML repo. We can see 14 different GGML models , corresponding to different types of quantization. They follow a particular naming convention q the number of bits used to store the weights precision a particular variant. Here is a list of all the possible quant methods and their corresponding use cases, based on model cards made by TheBloke q2_k Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors. q3_k_l Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K q3_k_m Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K q3_k_s Uses Q3_K for all tensors q4_0 Original quant method, 4 bit. q4_1 Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models. q4_k_m Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K q4_k_s Uses Q4_K for all tensors q5_0 Higher accuracy, higher resource usage and slower inference. q5_1 Even higher accuracy, resource usage and slower inference. q5_k_m Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K q5_k_s Uses Q5_K for all tensors q6_k Uses Q8_K for all tensors q8_0 Almost indistinguishable from float16. High resource use and slow. Not recommended for most users. As a rule of thumb, I recommend using Q5_K_M as it preserves most of the model s performance. Alternatively, you can use Q4_K_M if you want to save some memory. In general, K_M versions are better than K_S versions. I cannot recommend Q2 or Q3 versions, as they drastically decrease model performance. Now that we know more about the quantization types available, let s see how to use them on a real model. You can execute the following code on a free T4 GPU on Google Colab. The first step consists of compiling llama.cpp and installing the required libraries in our Python environment. Install llama.cpp !git clone https github.com ggerganov llama.cpp !cd llama.cpp git pull make clean LLAMA_CUBLAS 1 make !pip install r llama.cpp requirements.txt Now we can download our model. We will use the model we fine tuned in the previous article, mlabonne EvolCodeLlama 7b . MODEL_ID mlabonne EvolCodeLlama 7b Download model !git lfs install !git clone https huggingface.co MODEL_ID This step can take a while. Once it s done, we need to convert our weight to GGML FP16 format. MODEL_NAME MODEL_ID.split 1 GGML_VERSION gguf Convert to fp16 fp16 f MODEL_NAME MODEL_NAME.lower . GGML_VERSION .fp16.bin !python llama.cpp convert.py MODEL_NAME outtype f16 outfile fp16 Finally, we can quantize the model using one or several methods. In this case, we will use the Q4_K_M and Q5_K_M methods I recommended earlier. This is the only step that actually requires a GPU. QUANTIZATION_METHODS q4_k_m , q5_k_m for method in QUANTIZATION_METHODS qtype f MODEL_NAME MODEL_NAME.lower . GGML_VERSION . method .bin !. llama.cpp quantize fp16 qtype method Our two quantized models are now ready for inference . We can check the size of the bin files to see how much we compressed them. The FP16 model takes up 13.5 GB, while the Q4_K_M model takes up 4.08 GB 3.3 times smaller and the Q5_K_M model takes up 4.78 GB 2.8 times smaller . Let s use llama.cpp to efficiently run them. Since we re using a GPU with 16 GB of VRAM, we can offload every layer to the GPU. In this case, it represents 35 layers 7b parameter model , so we ll use the ngl 35 parameter. In the following code block, we ll also input a prompt and the quantization method we want to use. import os model_list file for file in os.listdir MODEL_NAME if GGML_VERSION in file prompt input Enter your prompt chosen_method input Please specify the quantization method to run the model options , .join model_list Verify the chosen method is in the list if chosen_method not in model_list print Invalid method chosen! else qtype f MODEL_NAME MODEL_NAME.lower . GGML_VERSION . method .bin !. llama.cpp main m qtype n 128 color ngl 35 p prompt Let s ask the model Write a Python function to print the nth Fibonacci numbers using the Q5_K_M method. If we look at the logs, we can confirm that we successfully offloaded our layers thanks to the line llm_load_tensors offloaded 35 35 layers to GPU . Here is the code the model generated def fib n if n 0 or n 1 return n return fib n 2 fib n 1 for i in range 1, 10 print fib i This wasn t a very complex prompt, but it successfully produced a working piece of code in no time. With this GGML, you can use your local LLM as an assistant in a terminal using the interactive mode i flag . Note that this also works on Macbooks with Apple s Metal Performance Shaders MPS , which is an excellent option to run LLMs. Finally, we can push our quantized model to a new repo on the Hugging Face Hub with the GGUF suffix. First, let s log in and modify the following code block to match your username. !pip install q huggingface_hub username mlabonne from huggingface_hub import notebook_login, create_repo, HfApi notebook_login Now we can create the repo and upload our models. We use the allow_patterns parameter to filter which files to upload, so we don t push the entirety of the directory. api HfApi Create repo create_repo repo_id f username MODEL_NAME GGML , repo_type model , exist_ok True Upload bin models api.upload_folder folder_path MODEL_NAME, repo_id f username MODEL_NAME GGML , allow_patterns f GGML_VERSION , We have successfully quantized, run, and pushed GGML models to the Hugging Face Hub! In the next section, we will explore how GGML actually quantize these models. Quantization with GGML The way GGML quantizes weights is not as sophisticated as GPTQ s. Basically, it groups blocks of values and rounds them to a lower precision. Some techniques, like Q4_K_M and Q5_K_M, implement a higher precision for critical layers . In this case, every weight is stored in 4 bit precision, with the exception of half of the attention.wv and feed_forward.w2 tensors. Experimentally, this mixed precision proves to be a good tradeoff between accuracy and resource usage. If we look into the ggml.c file, we can see how the blocks are defined. For example, the block_q4_0 structure is defined as define QK4_0 32 typedef struct ggml_fp16_t d delta uint8_t qs QK4_0 2 nibbles quants block_q4_0 In GGML, weights are processed in blocks, each consisting of 32 values. For each block, a scale factor delta is derived from the largest weight value. All weights in the block are then scaled, quantized, and packed efficiently for storage nibbles . This approach significantly reduces the storage requirements while allowing for a relatively simple and deterministic conversion between the original and quantized weights. Now that we know more about the quantization process, we can compare the results with NF4 and GPTQ. NF4 vs. GGML vs. GPTQ Which technique is better for 4 bit quantization? To answer this question, we need to introduce the different backends that run these quantized LLMs. For GGML models, llama.cpp with Q4_K_M models is the way to go. For GPTQ models, we have two options AutoGPTQ or ExLlama. Finally, NF4 models can directly be run in transformers with the load in 4bit flag. Oobabooga ran multiple experiments in an excellent blog post that compare different models in terms of perplexity lower is better Based on these results, we can say that GGML models have a slight advantage in terms of perplexity. The difference is not particularly significant, which is why it is better to focus on the generation speed in terms of tokens second. The best technique depends on your GPU if you have enough VRAM to fit the entire quantized model, GPTQ with ExLlama will be the fastest. If that s not the case, you can offload some layers and use GGML models with llama.cpp to run your LLM. Conclusion In this article, we introduced the GGML library and the new GGUF format to efficiently store these quantized models. We used it to quantize our own Llama model in different formats Q4_K_M and Q5_K_M . We then ran the GGML model and pushed our bin files to the Hugging Face Hub. Finally, we delved deeper into GGML s code to understand how it actually quantizes the weights and compared it to NF4 and GPTQ. Quantization is a formidable vector to democratize LLMs by lowering the cost of running them. In the future, mixed precision and other techniques will keep improving the performance we can achieve with quantized weights. Until then, I hope you enjoyed reading this article and learned something new. If you re interested in more technical content around LLMs, follow me on Medium. Articles about quantization Part 1 Introduction to Weight Quantization _Reducing the size of Large Language Models with 8 bit quantization_towardsdatascience.com Part 2 4 bit Quantization with GPTQ _Quantize your own LLMs using AutoGPTQ_towardsdatascience.com _Learn more about machine learning and support my work with one click become a Medium member here _ Join Medium with my referral link Maxime Labonne _As a Medium member, a portion of your membership fee goes to writers you read, and you get full access to every story _medium.com Share this post Maxime Labonne Quantize Llama models with GGML and llama.cpp Copy link Facebook Email Notes More Share Discussion about this post Comments Restacks Top Latest Discussions No posts Ready for more? Subscribe 2024 Maxime Labonne Privacy Terms Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Notes More This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts en",
            "platform": "maximelabonne.substack.com",
            "author_id": "c7167a83-e446-428d-9a2e-6e88a6b8bc6f",
            "author_full_name": "Hiep Nguyen",
            "link": "https://maximelabonne.substack.com/p/quantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172"
        },
        {
            "id": "1c1fb646-968c-47fc-9f5b-f9064723b565",
            "content": "A Beginner s Guide to LLM Fine Tuning Maxime Labonne How to fine tune Llama and other LLMs with one tool Maxime Labonne SubscribeSign in Share this post Maxime Labonne A Beginner s Guide to LLM Fine Tuning Copy link Facebook Email Notes More A Beginner s Guide to LLM Fine Tuning How to fine tune Llama and other LLMs with one tool Maxime Labonne Aug 30, 2023 1 Share this post Maxime Labonne A Beginner s Guide to LLM Fine Tuning Copy link Facebook Email Notes More 1 Share How to fine tune Llama and other LLMs with one tool Image by author The growing interest in Large Language Models LLMs has led to a surge in tools and wrappers designed to streamline their training process . Popular options include FastChat from LMSYS used to train Vicuna and Hugging Face s transformers trl libraries used in my previous article . In addition, each big LLM project, like WizardLM, tends to have its own training script, inspired by the original Alpaca implementation. In this article, we will use Axolotl , a tool created by the OpenAccess AI Collective. We will use it to fine tune a Code Llama 7b model on an evol instruct dataset comprised of 1,000 samples of Python code. Why Axolotl? The main appeal of Axolotl is that it provides a one stop solution, which includes numerous features, model architectures, and an active community. Here s a quick list of my favorite things about it Configuration All parameters used to train an LLM are neatly stored in a yaml config file. This makes it convenient for sharing and reproducing models. You can see an example for Llama 2 here. Dataset Flexibility Axolotl allows the specification of multiple datasets with varied prompt formats such as alpaca instruction ... , input ... , output ... , sharegpt chat conversations from ... , value ... , and raw completion text ... . Combining datasets is seamless, and the hassle of unifying the prompt format is eliminated. Features Axolotl is packed with SOTA techniques such as FSDP, deepspeed, LoRA, QLoRA, ReLoRA, sample packing, GPTQ, FlashAttention, xformers, and rope scaling. Utilities There are numerous user friendly utilities integrated, including the addition or alteration of special tokens, or a custom wandb configuration. Some well known models trained using this tool are Manticore 13b from the OpenAccess AI Collective and Samantha 1.11 70b from Eric Hartford. Like other wrappers, it is built on top of the transformers library and uses many of its features. Create your own config file Before anything, we need a configuration file. You can reuse an existing configuration from the examples folder. In our case, we will tweak the QLoRA config for Llama 2 to create our own Code Llama model. The model will be trained on a subset of 1,000 Python samples from the nickrosh Evol Instruct Code 80k v1 dataset. First, we must change the base_model and base_model_config fields to codellama CodeLlama 7b hf . To push our trained adapter to the Hugging Face Hub, let s add a new field hub_model_id , which corresponds to the name of our model, EvolCodeLlama 7b . Now, we have to update the dataset to mlabonne Evol Instruct Python 1k and set type to alpaca . There s no sample bigger than 2048 tokens in this dataset, so we can reduce the sequence_len to 2048 and save some VRAM. Talking about VRAM, we re going to use a micro_batch_size of 10 and a gradient_accumulation_steps of 1 to maximize its use. In practice, you try different values until you use 95 of the available VRAM. For convenience, I m going to add the name axolotl to the wandb_project field so it s easier to track on my account. I m also setting the warmup_steps to 100 personal preference and the eval_steps to 0.01 so we ll end up with 100 evaluations. Here s how the final config file should look base_model codellama CodeLlama 7b hf base_model_config codellama CodeLlama 7b hf model_type LlamaForCausalLM tokenizer_type LlamaTokenizer is_llama_derived_model true hub_model_id EvolCodeLlama 7b load_in_8bit false load_in_4bit true strict false datasets path mlabonne Evol Instruct Python 1k type alpaca dataset_prepared_path last_run_prepared val_set_size 0.02 output_dir . qlora out adapter qlora lora_model_dir sequence_len 2048 sample_packing true lora_r 32 lora_alpha 16 lora_dropout 0.05 lora_target_modules lora_target_linear true lora_fan_in_fan_out wandb_project axolotl wandb_entity wandb_watch wandb_run_id wandb_log_model gradient_accumulation_steps 1 micro_batch_size 10 num_epochs 3 optimizer paged_adamw_32bit lr_scheduler cosine learning_rate 0.0002 train_on_inputs false group_by_length false bf16 true fp16 false tf32 false gradient_checkpointing true early_stopping_patience resume_from_checkpoint local_rank logging_steps 1 xformers_attention flash_attention true warmup_steps 100 eval_steps 0.01 save_strategy epoch save_steps debug deepspeed weight_decay 0.0 fsdp fsdp_config special_tokens bos_token s eos_token s unk_token unk You can also find this config file here as a GitHub gist. Before we start training our model, I want to introduce a few parameters that are important to understand QLoRA We re using QLoRA for fine tuning, which is why we re loading the base model in 4 bit precision NF4 format . You can check this article from Benjamin Marie to know more about QLoRA. Gradient checkpointing It lowers the VRAM requirements by removing some activations that are re computed on demand during the backward pass. It also slows down training by about 20 , according to Hugging Face s documentation. FlashAttention This implements the FlashAttention mechanism, which improves the speed and memory efficiency of our model thanks to a clever fusion of GPU operations learn more about it in this article from Aleksa Gordi\u0107 . Sample packing Smart way of creating batches with as little padding as possible, by reorganizing the order of the samples bin packing problem . As a result, we need fewer batches to train the model on the same dataset. It was inspired by the Multipack Sampler see my note and Krell et al. You can find FlashAttention in some other tools, but sample packing is relatively new. As far as I know, OpenChat was the first project to use sample packing during fine tuning. Thanks to Axolotl, we ll use these techniques for free. Fine tune Code Llama Having the config file ready, it s time to get our hands dirty with the actual fine tuning. You might consider running the training on a Colab notebook. However, for those without access to a high performance GPU, a more cost effective solution consists of renting cloud based GPU services , like AWS, Lambda Labs, Vast.ai, Banana, or RunPod. Personally, I use RunPod, which is a popular option in the fine tuning community. It s not the cheapest service but it hits a good tradeoff with a clean UI. You can easily replicate the following steps using your favorite service. When your RunPod account is set up, go to Manage Templates and click on New Template . Here is a simple template Image by author Let s review the different fields and their corresponding values Template Name Axolotl you can choose whatever you want Container Image winglian axolotl runpod main py3.10 cu118 2.0.1 Container Disk 100 GB Volume Disk 0 GB Volume Mount Path workspace In addition, there are two handy environment variables can include HUGGING_FACE_HUB_TOKEN you can find your token on this page requires an account WANDB_API_KEY you can find your key on this page requires an account Alternatively, you can simply log in the terminal later using huggingface cli login and wandb login . Once you re set up, go to Community Cloud and deploy an RTX 3090. Here you can search for the name of your template and select it as follows Image by author You can click on Continue and RunPod will deploy your template. You can see the installation in your pod s logs Manage Pods . When the option becomes available, click on Connect . Here, click on Start Web Terminal and then Connect to Web Terminal . You are now connected to your pod! The following steps are the same no matter what service you choose 1. We install Axolotl and the PEFT library as follows git clone https github.com OpenAccess AI Collective axolotl cd axolotl pip3 install e . flash attn pip3 install U git https github.com huggingface peft.git 2 . Download the config file we created wget https gist.githubusercontent.com mlabonne 8055f6335e2b85f082c8c75561321a66 raw 93915a9563fcfff8df9a81fc0cdbf63894465922 EvolCodeLlama 7b.yaml 3 . You can now start fine tuning the model with the following command accelerate launch scripts finetune.py EvolCodeLlama 7b.yaml If everything is configured correctly, you should be able to train the model in a little more than one hour it took me 1h 11m 44s . If you check the GPU memory used, you ll see almost 100 with this config, which means we re optimizing it pretty nicely. If you re using a GPU with more VRAM like an A100 , you can increase the micro batch size to make sure you re fully using it. In the meantime, feel free to close the web terminal and check your loss on Weights Biases. We re using tmux so the training won t stop if you close the terminal. Here are my loss curves Image by author We see a steady improvement in the eval loss, which is a good sign. However, you can also spot drops in the eval loss that are not correlated with a decrease in the quality of the outputs The best way to evaluate your model is simply by using it you can run it in the terminal with the command accelerate launch scripts finetune.py EvolCodeLlama 7b.yaml inference lora_model_dir . qlora out . The QLoRA adapter should already be uploaded to the Hugging Face Hub. However, you can also merge the base Code Llama model with this adapter and push the merged model there by following these steps 1. Download this script wget https gist.githubusercontent.com mlabonne a3542b0519708b8871d0703c938bba9f raw 60abc5afc07f9d843bc23d56f4e0b7ab072c4a62 merge_peft.py 2 . Execute it with this command python merge_peft.py base_model codellama CodeLlama 7b hf peft_model . qlora out hub_id EvolCodeLlama 7b Congratulations, you should have your own EvolCodeLlama 7b on the Hugging Face Hub at this point! For reference, you can access my own model trained with this process here mlabonne EvolCodeLlama 7b Considering that our EvolCodeLlama 7b is a code LLM, it would be interesting to compare its performance with other models on standard benchmarks , such as HumanEval and MBPP. For reference, you can find a leaderboard at the following address Multilingual Code Evals. If you re happy with this model, you can quantize it with GGML for local inference with this free Google Colab notebook. You can also fine tune bigger models e.g., 70b parameters thanks to deepspeed, which only requires an additional config file. Conclusion In this article, we ve covered the essentials of how to efficiently fine tune LLMs . We customized parameters to train on our Code Llama model on a small Python dataset. Finally, we merged the weights and uploaded the result on Hugging Face. I hope you found this guide useful. I recommend using Axolotl with a cloud based GPU service to get some experience and upload a few models on Hugging Face. Build your own datasets, play with the parameters, and break stuff along the way. Like with every wrapper, don t hesitate to check the source code to get a good intuition of what it s actually doing. It will massively help in the long run. Thanks to the OpenAccess AI Collective and all the contributors! If you re interested in more technical content around LLMs, follow me on Medium. Related articles Fine Tune Your Own Llama 2 Model in a Colab Notebook _A practical introduction to LLM fine tuning_towardsdatascience.com 4 bit Quantization with GPTQ _Quantize your own LLMs using AutoGPTQ_towardsdatascience.com _Learn more about machine learning and support my work with one click become a Medium member here _ Join Medium with my referral link Maxime Labonne _As a Medium member, a portion of your membership fee goes to writers you read, and you get full access to every story _medium.com 1 Share this post Maxime Labonne A Beginner s Guide to LLM Fine Tuning Copy link Facebook Email Notes More 1 Share Discussion about this post Comments Restacks Daniel Jun 23 Thanks for this great article! One question How do you deal with the issue that the chat template defined in the Axolotl config for training and a chat template used for inference e.g. when you load the model from the Hub via HuggingFace transformers method .from_pretrained and use their chat template might be different? If I am not mistaken then the Axolotl templates assembles prompts in token space, whereas HF chat templates assembles them in string space, which might cause tokenization mismatches? Expand full comment Reply Share Top Latest Discussions No posts Ready for more? Subscribe 2024 Maxime Labonne Privacy Terms Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Notes More This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts en",
            "platform": "maximelabonne.substack.com",
            "author_id": "c7167a83-e446-428d-9a2e-6e88a6b8bc6f",
            "author_full_name": "Hiep Nguyen",
            "link": "https://maximelabonne.substack.com/p/a-beginners-guide-to-llm-fine-tuning-4bae7d4da672"
        },
        {
            "id": "fed8b758-1e32-4898-b329-4cfaa1df5bbd",
            "content": "4 bit Quantization with GPTQ Maxime Labonne Quantize your own LLMs using AutoGPTQ Maxime Labonne SubscribeSign in Share this post Maxime Labonne 4 bit Quantization with GPTQ Copy link Facebook Email Notes More 4 bit Quantization with GPTQ Quantize your own LLMs using AutoGPTQ Maxime Labonne Jul 31, 2023 1 Share this post Maxime Labonne 4 bit Quantization with GPTQ Copy link Facebook Email Notes More Share Quantize your own LLMs using AutoGPTQ Image by author Recent advancements in weight quantization allow us to run massive large language models on consumer hardware, like a LLaMA 30B model on an RTX 3090 GPU. This is possible thanks to novel 4 bit quantization techniques with minimal performance degradation, like GPTQ, GGML, and NF4. In the previous article, we introduced na\u00efve 8 bit quantization techniques and the excellent LLM.int8 . In this article, we will explore the popular GPTQ algorithm to understand how it works and implement it using the AutoGPTQ library. You can find the code on Google Colab and GitHub. Optimal Brain Quantization Let s start by introducing the problem we re trying to solve. For every layer \u2113 in the network, we want to find a quantized version \u0174\u2097 _of the original weights_ W\u2097 . This is called the layer wise compression problem . More specifically, to minimize performance degradation, we want the outputs \u0174 \u1d68 X \u1d68 of these new weights to be as close as possible to the original ones W \u1d68 X \u1d68 . In other words, we want to find Different approaches have been proposed to solve this problem, but we re interested in the Optimal Brain Quantizer OBQ framework here. This method is inspired by a pruning technique to carefully remove weights from a fully trained dense neural network Optimal Brain Surgeon . It uses an approximation technique and provides explicit formulas for the best single weight _w\ud801\udfa5_ to remove and optimal update _\u03b4_ \ua7f3 to adjust the set of remaining non quantized weights _F_ to make up for the removal where quant _w_ is the weight rounding given by the quantization and H \ua7f3 is the Hessian. Using OBQ, we can quantize the easiest weight first and then adjust all remaining non quantized weights to compensate for this precision loss . Then we pick the next weight to quantize, and so on. A potential issue with this approach is when there are outlier weights, which can result in high quantization error . Usually, these outliers would be quantized last, when there are few non quantized weights left that could be adjusted to compensate for the large error. This effect can worsen when some weights are pushed further outside the grid by intermediate updates. A simple heuristic is applied to prevent this outliers are quantized as soon as they appear. This process could be computationally heavy, especially for LLMs. To deal with this, the OBQ method uses a trick that avoids redoing the entire computation each time a weight is simplified. After quantizing a weight, it adjusts the matrix used in calculations the Hessian by removing the row and column associated with that weight using Gaussian elimination The method also employs vectorization to process multiple rows of the weight matrix at once. Despite its efficiency, the OBQ s computation time increases significantly as the size of the weight matrix increases. This cubic growth makes it difficult to use OBQ on very large models with billions of parameters. The GPTQ Algorithm Introduced by Frantar et al. 2023 , the GPTQ algorithm takes inspiration from the OBQ method, but with significant improvements to scale it for very large language models. Step 1 Arbitrary Order Insight The OBQ method selects weights parameters in a model for quantization in a certain order, determined by which will add the least additional error . However, GPTQ observes that for large models, quantizing weights in any fixed order can perform just as well. This is because even though some weights might introduce more error individually, they are quantized later in the process when there are few other weights left that could increase the error. So the order doesn t matter as much as we thought. Based on this insight, GPTQ aims to quantize all weights in the same order for all rows of a matrix. This makes the process faster because certain computations have to be done only once for each column, rather than once for each weight. Image by author Step 2 Lazy Batch Updates This scheme won t be fast because it requires updating a huge matrix with very few computations for each entry. This type of operation can t utilize the full compute capabilities of GPUs and will be slowed down by memory limitations memory throughput bottleneck . To resolve this, GPTQ introduces lazy batch updates. It turns out that the final rounding decisions for a given column are only affected by updates performed on that column, not on later columns. Therefore, GPTQ can apply the algorithm to a batch of columns at a time like 128 columns , updating only those columns and a corresponding block of the matrix. After a block is fully processed, the algorithm performs global updates on the entire matrix. Step 3 Cholesky Reformulation However, there s one more issue to address. When the algorithm scales up to very large models, numerical inaccuracies can become a problem. Specifically, repeated applications of a certain operation can accumulate numerical errors . To tackle this, GPTQ uses a Cholesky decomposition, a numerically stable method for solving certain mathematical problems. It involves precomputing some required information from the matrix using the Cholesky method. This approach, combined with a slight dampening adding a small constant to diagonal elements of the matrix , helps the algorithm to avoid numerical issues. The full algorithm can be summarized in a few steps 1. The GPTQ algorithm begins with a Cholesky decomposition of the Hessian inverse a matrix that helps decide how to adjust the weights 2. It then runs in loops, handling batches of columns at a time. 3. For each column in a batch, it quantizes the weights, calculates the error, and updates the weights in the block accordingly. 4. After processing the batch, it updates all remaining weights based on the block s errors. The GPTQ algorithm was tested on various language generation tasks. It was compared with other quantization methods, like rounding all weights to the nearest quantized value RTN . GPTQ was used with the BLOOM 176B parameters and OPT 175B parameters model families, and models were quantized using a single NVIDIA A100 GPU . Quantize an LLM with AutoGPTQ GPTQ has been very popular to create models in 4 bit precision that can efficiently run on GPUs. You can find many examples on the Hugging Face Hub, especially from TheBloke. If you re looking for an approach that is more CPU friendly, GGML is currently your best option. Finally, the transformers library with bitsandbytes allows you to quantize a model when it s loaded using the load_in_4bit true argument, which requires downloading full models and storing them in your RAM. Let s implement the GPTQ algorithm using the AutoGPTQ library and quantize a GPT 2 model. This requires a GPU, but a free T4 on Google Colab will do. We start by loading the libraries and defining the model we want to quantize in this case, GPT 2 . !BUILD_CUDA_EXT 0 pip install q auto gptq transformers import random from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig from datasets import load_dataset import torch from transformers import AutoTokenizer Define base model and output directory model_id gpt2 out_dir model_id GPTQ We now want to load the model and the tokenizer. The tokenizer is loaded using the classic AutoTokenizer class from the transformers library. On the other hand, we need to pass a specific configuration BaseQuantizeConfig to load the model. In this configuration, we can specify the number of bits to quantize here, bits 4 and the group size size of the lazy batch . Note that this group size is optional we could also use one set of parameters for the entire weight matrix. In practice, these groups generally improve the quality of the quantization at a very low cost especially with group_size 1024 . The damp_percent value is here to help the Cholesky reformulation and should not be changed. Finally, the desc_act also called act order is a tricky parameter. It allows you to process rows based on decreasing activation , meaning the most important or impactful rows determined by sampled inputs and outputs are processed first. This method aims to place most of the quantization error inevitably introduced during quantization on less significant weights. This approach improves the overall accuracy of the quantization process by ensuring the most significant weights are processed with greater precision. However, when used alongside group size, desc_act can lead to performance slowdowns due to the need to frequently reload quantization parameters. For this reason, we won t use it here it will probably be fixed in the future, however . Load quantize config, model and tokenizer quantize_config BaseQuantizeConfig bits 4, group_size 128, damp_percent 0.01, desc_act False, model AutoGPTQForCausalLM.from_pretrained model_id, quantize_config tokenizer AutoTokenizer.from_pretrained model_id The quantization process relies heavily on samples to evaluate and enhance the quality of the quantization. They provide a means of comparison between the outputs produced by the origina and the newly quantized model. The larger the number of samples provided, the greater the potential for more accurate and effective comparisons, leading to improved quantization quality. In the context of this article, we utilize the C4 Colossal Clean Crawled Corpus dataset to generate our samples. The C4 dataset is a large scale, multilingual collection of web text gathered from the Common Crawl project. This expansive dataset has been cleaned and prepared specifically for training large scale language models, making it a great resource for tasks such as this. The WikiText dataset is another popular option. In the following code block, we load 1024 samples from the C4 dataset, tokenize them, and format them. Load data and tokenize examples n_samples 1024 data load_dataset allenai c4 , data_files en c4 train.00001 of 01024.json.gz , split f train n_samples 5 tokenized_data tokenizer n n .join data text , return_tensors pt Format tokenized examples examples_ids for _ in range n_samples i random.randint 0, tokenized_data.input_ids.shape 1 tokenizer.model_max_length 1 j i tokenizer.model_max_length input_ids tokenized_data.input_ids , i j attention_mask torch.ones_like input_ids examples_ids.append input_ids input_ids, attention_mask attention_mask Now that dataset is ready, we can start the quantization process with a batch size of 1. Optionally, we also use OpenAI Triton, a CUDA alternative, to communicate with the GPU. Once this is done, we save the tokenizer and the model in a safetensors format. Quantize with GPTQ model.quantize examples_ids, batch_size 1, use_triton True, Save model and tokenizer model.save_quantized out_dir, use_safetensors True tokenizer.save_pretrained out_dir As per usual, the model and tokenizer can then be loaded from the output directory using the AutoGPTQForCausalLM and AutoTokenizer classes. device cuda 0 if torch.cuda.is_available else cpu Reload model and tokenizer model AutoGPTQForCausalLM.from_quantized out_dir, device device, use_triton True, use_safetensors True, tokenizer AutoTokenizer.from_pretrained out_dir Let s check that the model is working correctly. The AutoGPTQ model mostly works as a normal transformers model, which makes it compatible with inference pipelines, as shown in the following example from transformers import pipeline generator pipeline text generation , model model, tokenizer tokenizer result generator I have a dream , do_sample True, max_length 50 0 generated_text print result I have a dream, she told CNN last week. I have this dream of helping my mother find her own. But, to tell that for the first time, now that I m seeing my mother now, just knowing how wonderful it is that We managed to get a convincing completion from our quantized GPT 2 model. A more in depth evaluation would require measuring the perplexity of the quantized model versus the original one. However, we will leave it out of the scope of this article. Conclusion In this article, we introduced the GPTQ algorithm, a state of the art quantization technique to run LLMs on consumer grade hardware. We showed how it addresses the layer wise compression problem, based on an improved OBS technique with arbitrary order insight, lazy batch updates, and Cholesky reformulation. This novel approach significantly reduces memory and computation requirements , making LLMs accessible to a broader audience. In addition, we quantized our own LLM model on a free T4 GPU and ran it to generate text. You can push your own version of a GPTQ 4 bit quantized model on the Hugging Face Hub. As mentioned in the introduction, GPTQ is not the only 4 bit quantization algorithm GGML and NF4 are excellent alternatives with slightly different scopes. I encourage you to learn more about them and give them a shot! If you re interested in more technical content around LLMs, follow me on Twitter maximelabonne. References B. Hassibi, D. G. Stork and G. J. Wolff, Optimal Brain Surgeon and general network pruning, IEEE International Conference on Neural Networks, San Francisco, CA, USA, 1993, pp. 293 299 vol.1, doi 10.1109 ICNN.1993.298572. Elias Frantar, Sidak Pal Singh, Dan Alistarh. 2023 . Optimal Brain Compression A Framework for Accurate Post Training Quantization and Pruning. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh. 2023 . GPTQ Accurate Post Training Quantization for Generative Pre trained Transformers. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. 2020 . Exploring the Limits of Transfer Learning with a Unified Text to Text Transformer. Related articles Introduction to Weight Quantization _Reducing the size of Large Language Models with 8 bit quantization_towardsdatascience.com Fine Tune Your Own Llama 2 Model in a Colab Notebook _A practical introduction to LLM fine tuning_towardsdatascience.com _Learn more about machine learning and support my work with one click become a Medium member here _ Join Medium with my referral link Maxime Labonne _As a Medium member, a portion of your membership fee goes to writers you read, and you get full access to every story _medium.com _If you re already a member, you canfollow me on Medium._ 1 Share this post Maxime Labonne 4 bit Quantization with GPTQ Copy link Facebook Email Notes More Share Discussion about this post Comments Restacks Top Latest Discussions No posts Ready for more? Subscribe 2024 Maxime Labonne Privacy Terms Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Notes More This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts en",
            "platform": "maximelabonne.substack.com",
            "author_id": "c7167a83-e446-428d-9a2e-6e88a6b8bc6f",
            "author_full_name": "Hiep Nguyen",
            "link": "https://maximelabonne.substack.com/p/4-bit-quantization-with-gptq-36b0f4f02c34"
        },
        {
            "id": "973e1312-5c4e-47f8-94a5-d984bfb5007f",
            "content": "Fine Tune Your Own Llama 2 Model in a Colab Notebook A practical introduction to LLM fine tuning Maxime Labonne SubscribeSign in Share this post Maxime Labonne Fine Tune Your Own Llama 2 Model in a Colab Notebook Copy link Facebook Email Notes More Fine Tune Your Own Llama 2 Model in a Colab Notebook A practical introduction to LLM fine tuning Maxime Labonne Jul 25, 2023 7 Share this post Maxime Labonne Fine Tune Your Own Llama 2 Model in a Colab Notebook Copy link Facebook Email Notes More Share A practical introduction to LLM fine tuning Image by author With the release of LLaMA v1, we saw a Cambrian explosion of fine tuned models, including Alpaca, Vicuna, and WizardLM, among others. This trend encouraged different businesses to launch their own base models with licenses suitable for commercial use, such as OpenLLaMA, Falcon, XGen, etc. The release of Llama 2 now combines the best elements from both sides it offers a highly efficient base model along with a more permissive license . During the first half of 2023, the software landscape was significantly shaped by the widespread use of APIs like OpenAI API to create infrastructures based on Large Language Models LLMs . Libraries such as LangChain and LlamaIndex played a critical role in this trend. Moving into the latter half of the year, the process of fine tuning or instruction tuning these models is set to become a standard procedure in the LLMOps workflow. This trend is driven by various factors the potential for cost savings, the ability to process confidential data, and even the potential to develop models that exceed the performance of prominent models like ChatGPT and GPT 4 in certain specific tasks. In this article, we will see why instruction tuning works and how to implement it in a Google Colab notebook to create your own Llama 2 model. As usual, the code is available on Colab and GitHub. Background on fine tuning LLMs Image by author LLMs are pretrained on an extensive corpus of text. In the case of Llama 2, we know very little about the composition of the training set, besides its length of 2 trillion tokens. In comparison, BERT 2018 was only trained on the BookCorpus 800M words and English Wikipedia 2,500M words . From experience, this is a very costly and long process with a lot of hardware issues. If you want to know more about it, I recommend reading Meta s logbook about the pretraining of the OPT 175B model. When the pretraining is complete, auto regressive models like Llama 2 can predict the next token in a sequence. However, this does not make them particularly useful assistants since they don t reply to instructions. This is why we employ instruction tuning to align their answers with what humans expect. There are two main fine tuning techniques Supervised Fine Tuning SFT Models are trained on a dataset of instructions and responses. It adjusts the weights in the LLM to minimize the difference between the generated answers and ground truth responses, acting as labels. Reinforcement Learning from Human Feedback RLHF Models learn by interacting with their environment and receiving feedback. They are trained to maximize a reward signal using PPO , which is often derived from human evaluations of model outputs. In general, RLHF is shown to capture more complex and nuanced human preferences, but is also more challenging to implement effectively. Indeed, it requires careful design of the reward system and can be sensitive to the quality and consistency of human feedback. A possible alternative in the future is the Direct Preference Optimization DPO algorithm, which directly runs preference learning on the SFT model. In our case, we will perform SFT, but this raises a question why does fine tuning work in the first place? As highlighted in the Orca paper, our understanding is that fine tuning leverages knowledge learned during the pretraining process. In other words, fine tuning will be of little help if the model has never seen the kind of data you re interested in. However, if that s the case, SFT can be extremely performant. For example, the LIMA paper showed how you could outperform GPT 3 DaVinci003 by fine tuning a LLaMA v1 model with 65 billion parameters on only 1,000 high quality samples. The quality of the instruction dataset is essential to reach this level of performance, which is why a lot of work is focused on this issue like evol instruct, Orca, or phi 1 . Note that the size of the LLM 65b, not 13b or 7b is also fundamental to leverage pre existing knowledge efficiently. Another important point related to the data quality is the prompt template . Prompts are comprised of similar elements system prompt optional to guide the model, user prompt required to give the instruction, additional inputs optional to take into consideration, and the model s answer required . In the case of Llama 2, the authors used the following template s INST SYS System prompt SYS User prompt INST Model answer s There are other templates, like the ones from Alpaca and Vicuna, and their impact is not very clear. In this example, we will reformat our instruction dataset to follow Llama 2 s template. For the purpose of this tutorial, I ve already done it using the excellent timdettmers openassistant guanaco dataset. You can find it on Hugging Face under the name mlabonne guanaco llama2 1k . How to fine tune Llama 2 In this section, we will fine tune a Llama 2 model with 7 billion parameters on a T4 GPU with high RAM using Google Colab 2.21 credits hour . Note that a T4 only has 16 GB of VRAM, which is barely enough to store Llama 2 7b s weights 7b 2 bytes 14 GB in FP16 . In addition, we need to consider the overhead due to optimizer states, gradients, and forward activations see this excellent article for more information . This means that a full fine tuning is not possible here we need parameter efficient fine tuning PEFT techniques like LoRA or QLoRA. To drastically reduce the VRAM usage, we must fine tune the model in 4 bit precision , which is why we ll use QLoRA here. The good thing is that we can leverage the Hugging Face ecosystem with the transformers , accelerate , peft , trl , and bitsandbytes libraries. We ll do this in the following code based on Younes Belkada s GitHub Gist. First, we install and load these libraries. !pip install q accelerate 0.21.0 peft 0.4.0 bitsandbytes 0.40.2 transformers 4.31.0 trl 0.4.7 import os import torch from datasets import load_dataset from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging, from peft import LoraConfig, PeftModel from trl import SFTTrainer Let s talk a bit about the parameters we can tune here. First, we want to load a llama 2 7b chat hf model and train it on the mlabonne guanaco llama2 1k 1,000 samples , which will produce our fine tuned model llama 2 7b miniguanaco . Feel free to change the dataset there are many options on the Hugging Face Hub. QLoRA will use a rank of 64 with a scaling parameter of 16 see this article for more information about LoRA parameters . We ll load the Llama 2 model directly in 4 bit precision using the NF4 type and train it for one epoch. To get more information about the other parameters, check the TrainingArguments, PeftModel, and SFTTrainer documentation. The model that you want to train from the Hugging Face hub model_name daryl149 llama 2 7b chat hf The instruction dataset to use dataset_name mlabonne guanaco llama2 1k Fine tuned model name new_model llama 2 7b miniguanaco QLoRA parameters LoRA attention dimension lora_r 64 Alpha parameter for LoRA scaling lora_alpha 16 Dropout probability for LoRA layers lora_dropout 0.1 bitsandbytes parameters Activate 4 bit precision base model loading use_4bit True Compute dtype for 4 bit base models bnb_4bit_compute_dtype float16 Quantization type fp4 or nf4 bnb_4bit_quant_type nf4 Activate nested quantization for 4 bit base models double quantization use_nested_quant False TrainingArguments parameters Output directory where the model predictions and checkpoints will be stored output_dir . results Number of training epochs num_train_epochs 1 Enable fp16 bf16 training set bf16 to True with an A100 fp16 False bf16 False Batch size per GPU for training per_device_train_batch_size 4 Batch size per GPU for evaluation per_device_eval_batch_size 4 Number of update steps to accumulate the gradients for gradient_accumulation_steps 2 Enable gradient checkpointing gradient_checkpointing True Maximum gradient normal gradient clipping max_grad_norm 0.3 Initial learning rate AdamW optimizer learning_rate 2e 4 Weight decay to apply to all layers except bias LayerNorm weights weight_decay 0.001 Optimizer to use optim paged_adamw_32bit Learning rate schedule constant a bit better than cosine lr_scheduler_type constant Number of training steps overrides num_train_epochs max_steps 1 Ratio of steps for a linear warmup from 0 to learning rate warmup_ratio 0.03 Group sequences into batches with same length Saves memory and speeds up training considerably group_by_length True Save checkpoint every X updates steps save_steps 10 Log every X updates steps logging_steps 1 SFT parameters Maximum sequence length to use max_seq_length None Pack multiple short examples in the same input sequence to increase efficiency packing False Load the entire model on the GPU 0 device_map 0 We can now load everything and start the fine tuning process. We re relying on multiple wrappers, so bear with me. First of all, we want to load the dataset we defined. If you changed it, you can preprocess it here and adapt it to the desired prompt template. Then, we re configuring bitsandbytes for 4 bit quantization. Next, we re loading the Llama 2 model in 4 bit precision on a GPU with the corresponding tokenizer. Finally, we re loading configurations for QLoRA, regular training parameters, and passing everything to the SFTTrainer . The training can finally start! Load dataset you can process it here dataset load_dataset dataset_name, split train Load tokenizer and model with QLoRA configuration compute_dtype getattr torch, bnb_4bit_compute_dtype bnb_config BitsAndBytesConfig load_in_4bit use_4bit, bnb_4bit_quant_type bnb_4bit_quant_type, bnb_4bit_compute_dtype compute_dtype, bnb_4bit_use_double_quant use_nested_quant, Check GPU compatibility with bfloat16 if compute_dtype torch.float16 and use_4bit major, _ torch.cuda.get_device_capability if major 8 print 80 print Your GPU supports bfloat16 accelerate training with bf16 True print 80 Load base model model AutoModelForCausalLM.from_pretrained model_name, quantization_config bnb_config, device_map device_map model.config.use_cache False model.config.pretraining_tp 1 Load LLaMA tokenizer tokenizer AutoTokenizer.from_pretrained model_name, trust_remote_code True tokenizer.pad_token tokenizer.eos_token tokenizer.padding_side right Fix weird overflow issue with fp16 training Load LoRA configuration peft_config LoraConfig lora_alpha lora_alpha, lora_dropout lora_dropout, r lora_r, bias none , task_type CAUSAL_LM , Set training parameters training_arguments TrainingArguments output_dir output_dir, num_train_epochs num_train_epochs, per_device_train_batch_size per_device_train_batch_size, gradient_accumulation_steps gradient_accumulation_steps, optim optim, save_steps save_steps, logging_steps logging_steps, learning_rate learning_rate, weight_decay weight_decay, fp16 fp16, bf16 bf16, max_grad_norm max_grad_norm, max_steps max_steps, warmup_ratio warmup_ratio, group_by_length group_by_length, lr_scheduler_type lr_scheduler_type, report_to tensorboard Set supervised fine tuning parameters trainer SFTTrainer model model, train_dataset dataset, peft_config peft_config, dataset_text_field text , max_seq_length max_seq_length, tokenizer tokenizer, args training_arguments, packing packing, Train model trainer.train Save trained model trainer.model.save_pretrained output_dir Image by author The training can be very long, depending on the size of your dataset. Here, it took less than an hour on a T4 GPU. We can check the plots on tensorboard, as follows load_ext tensorboard tensorboard logdir results runs Image by author Let s make sure that the model is behaving correctly. It would require a more exhaustive evaluation, but we can use the text generation pipeline to ask questions like What is a large language model? Note that I m formatting the input to match Llama 2 s prompt template. Ignore warnings logging.set_verbosity logging.CRITICAL Run text generation pipeline with our next model prompt What is a large language model? pipe pipeline task text generation , model model, tokenizer tokenizer, max_length 200 result pipe f s INST prompt INST print result 0 generated_text The model outputs the following response A large language model is a type of artificial intelligence that is trained on a large dataset of text to generate human like language. It is typically trained on a dataset of text that is much larger than the dataset used for smaller language models. The large dataset allows the model to learn more complex patterns in language, which can result in more accurate and natural sounding language generation. Large language models are often used for tasks such as text summarization, language translation, and chatbots. They are also used for more complex tasks such as writing articles, generating creative content, and even composing music. Large language models are trained using a technique called deep learning, which involves using many layers of artificial neural networks to learn complex patterns in the data. The model is trained on a large dataset of text, and the neural networks are trained to predict the next word in a sequence of text given From experience, it is very coherent for a model with only 7 billion parameters. You can play with it and ask harder questions from evaluation datasets like BigBench Hard. Guanaco is an excellent dataset that has produced high quality models in the past. You can train a Llama 2 model on the entire dataset using mlabonne guanaco llama2 . How can we store our new llama 2 7b miniguanaco model now? We need to merge the weights from LoRA with the base model. Unfortunately, as far as I know, there is no straightforward way to do it we need to reload the base model in FP16 precision and use the peft library to merge everything. Alas, it also creates a problem with the VRAM despite emptying it , so I recommend restarting the notebook , re executing the three first cells, and then executing the next one. Please contact me if you know a fix! Reload model in FP16 and merge it with LoRA weights base_model AutoModelForCausalLM.from_pretrained model_name, low_cpu_mem_usage True, return_dict True, torch_dtype torch.float16, device_map device_map, model PeftModel.from_pretrained base_model, output_dir model model.merge_and_unload Reload tokenizer to save it tokenizer AutoTokenizer.from_pretrained model_name, trust_remote_code True tokenizer.pad_token tokenizer.eos_token tokenizer.padding_side right Our weights are merged and we reloaded the tokenizer. We can now push everything to the Hugging Face Hub to save our model. !huggingface cli login model.push_to_hub new_model, use_temp_dir False tokenizer.push_to_hub new_model, use_temp_dir False You can now use this model for inference by loading it like any other Llama 2 model from the Hub. It is also possible to reload it for more fine tuning perhaps with another dataset? If you re interested in a script instead of a notebook, I recommend following the instructions provided in this blog post pip install trl git clone https github.com lvwerra trl python trl examples scripts sft_trainer.py model_name meta llama Llama 2 7b hf dataset_name timdettmers openassistant guanaco load_in_4bit use_peft batch_size 4 gradient_accumulation_steps 2 Conclusion In this article, we saw how to fine tune a Llama 2 7b model using a Colab notebook. We introduced some necessary background on LLM training and fine tuning, as well as important considerations related to instruction datasets. In the second section, we successfully fine tuned the Llama 2 model with its native prompt template and custom parameters. These fine tuned models can then be integrated into LangChain and other architectures as an advantageous alternative to OpenAI API. Remember that, in this new paradigm, instruction datasets are the new gold, and the quality of your model heavily depends on the data it s been fine tuned on. So good luck building high quality datasets! If you re interested in more content about LLMs, follow me on Twitter maximelabonne. References Hugo Touvron, Thomas Scialom, et al. 2023 . Llama 2 Open Foundation and Fine Tuned Chat Models. Philipp Schmid, Omar Sanseviero, Pedro Cuenca, Lewis Tunstall. Llama 2 is here get it on Hugging Face. https huggingface.co blog llama2 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto. 2023 . Stanford Alpaca An Instruction following LLaMA model. Jacob Devlin, Ming Wei Chang, Kenton Lee, Kristina Toutanova. 2019 . BERT Pre training of Deep Bidirectional Transformers for Language Understanding. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer. 2023 . QLoRA Efficient Finetuning of Quantized LLMs. 7 Share this post Maxime Labonne Fine Tune Your Own Llama 2 Model in a Colab Notebook Copy link Facebook Email Notes More Share Discussion about this post Comments Restacks Top Latest Discussions No posts Ready for more? Subscribe 2024 Maxime Labonne Privacy Terms Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Notes More This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts en",
            "platform": "maximelabonne.substack.com",
            "author_id": "c7167a83-e446-428d-9a2e-6e88a6b8bc6f",
            "author_full_name": "Hiep Nguyen",
            "link": "https://maximelabonne.substack.com/p/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32"
        },
        {
            "id": "29ac3ef0-dbb6-427d-b105-7ba3a40ae0da",
            "content": "What is a Tensor in Machine Learning? Maxime Labonne The difference between tensors, arrays, and matrices Maxime Labonne SubscribeSign in Share this post Maxime Labonne What is a Tensor in Machine Learning? Copy link Facebook Email Notes More What is a Tensor in Machine Learning? The difference between tensors, arrays, and matrices Maxime Labonne Mar 29, 2022 Share this post Maxime Labonne What is a Tensor in Machine Learning? Copy link Facebook Email Notes More Share The difference between tensors, arrays, and matrices Image by author What is a tensor, exactly? Most deep learning practitioners know about them but can t pinpoint an exact definition . TensorFlow, PyTorch every deep learning framework relies on the same basic object tensors . They re used to store almost everything in deep learning input data, weights, biases, predictions, etc. And yet, their definition is incredibly fuzzy the Wikipedia category alone has over 100 pages related to tensors. In this article, we ll give a definitive answer to the following question what is a tensor in neural networks? Tensors in computer science So why are there so many definitions? It s quite simple different fields have different definitions. Tensors in mathematics are not quite the same as tensors in physics , which are different from tensors in computer science . Image by author These definitions can be divided into two categories tensors as a data structure or as objects in an object oriented programming sense . Data structure this is the definition we use in computer science. Tensors are multidimensional arrays that store a specific type of value. Objects this is the definition used in other fields. In mathematics and physics, tensors are not just a data structure they also have a list of properties, like a specific product. This is why you see a lot of people sometimes quite pedantically saying _tensors are not n dimensional arrays matrices_ they don t talk about data structures, but about objects with properties . Even the same words have different meanings . For instance, in computer science, a 2D tensor is a matrix it s a tensor of rank 2 . In linear algebra, a tensor with 2 dimensions means it only stores two values. The rank also has a completely different definition it is the maximum number of its linearly independent column or row vectors. In computer science, we re only interested in a definition focused on the data structure . From this point of view, tensors truly are a generalization in _n_ dimensions of matrices. But we re still missing an important nuance when talking about tensors specifically in the context of deep learning... Tensors in deep learning _Icons created by Freepik and smashingstocks Flaticon_ So why are they called tensors instead of multidimensional arrays ? Ok, it is shorter, but is it all there is to it? Actually, people make an implicit assumption when they talk about tensors. PyTorch s official documentation gives us a practical answer _The biggest difference between a numpy array and a PyTorch Tensor is that a PyTorch Tensor can run on either CPU or GPU ._ In deep learning, we need performance to compute a lot of matrix multiplications in a highly parallel way. These matrices and n dimensional arrays in general are generally stored and processed on GPUs to speed up training and inference times. This is what was missing in our previous definition tensors in deep learning are not just n dimensional arrays, there s also the implicit assumption they can be run on a GPU . NumPy vs PyTorch Let s see the difference between NumPy arrays and PyTorch tensors. Image by author These two objects are very similar we can initialize a 1D array and a 1D tensor with nearly the same syntax. They also share a lot of methods and can be easily converted into one another. You can find the code used in this article at this address. NumPy Array 1 2 3 PyTorch Tensor tensor 1, 2, 3 Initializing 2D arrays and 2D tensors is not more complicated. NumPy Array 1 2 3 4 5 6 PyTorch Tensor tensor 1, 2, 3 , 4, 5, 6 We said that the only difference between tensors and arrays was the fact that tensors can be run on GPUs . So in the end, this distinction is based on performance. But is this boost that important? Let s compare the performance between NumPy arrays and PyTorch tensors on matrix multiplication. In the following example, we randomly initialize 4D arrays tensors and multiply them . 1.32 s 25.2 ms As we can see, PyTorch tensors completed outperformed NumPy arrays they completed the multiplication 52 times faster ! We could attribute this performance to different factors, such as NumPy arrays use a _float64_ format, whereas PyTorch tensors leverage the more efficient _float32_ format. However, even when NumPy arrays are converted to _float32_ , PyTorch tensors are still 40 times faster. PyTorch tensors are stored on a GPU, unlike NumPy arrays. But if we repeat the same experiment on a CPU, PyTorch tensors still manage to be 2.8 times faster on average. Even when combining both factors, PyTorch tensors prove to be 1.4 times faster, showing that NumPy arrays are truly less performant for matrix multiplication. This is the true power of tensors they re blazingly fast ! Performance might vary depending on the dimensions, the implementation , and the hardware, but this speed is the reason why tensors and not arrays are so common in deep learning. Conclusion In this article, we wrote a definition of tensors based on 1. Their use in computer science data structure 2. More specifically, in deep learning they can run on GPUs . Here s how we can summarize it in one sentence _Tensors are n dimensional arrays with the implicit assumption that they can run on a GPU. _ Finally, we saw the difference in performance between tensors and arrays, which motivates the need for tensors in deep learning. So next time someone tries to explain to you that tensors are not exactly a generalization of matrices, you ll know that they re right in a particular definition of tensors, but not in the computer science deep learning one. If you re looking for more data science and machine learning content in n dimensions, please follow me on twitter maximelabonne . You can find the code used in this article at this address. Share this post Maxime Labonne What is a Tensor in Machine Learning? Copy link Facebook Email Notes More Share Discussion about this post Comments Restacks Top Latest Discussions No posts Ready for more? Subscribe 2024 Maxime Labonne Privacy Terms Collection notice Start WritingGet the app Substack is the home for great culture Share Copy link Facebook Email Notes More This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts en",
            "platform": "maximelabonne.substack.com",
            "author_id": "c7167a83-e446-428d-9a2e-6e88a6b8bc6f",
            "author_full_name": "Hiep Nguyen",
            "link": "https://maximelabonne.substack.com/p/what-is-a-tensor-in-deep-learning-6dedd95d6507"
        },
        {
            "id": "0a9e8fcc-b796-4c58-b464-932f5fb38336",
            "content": "Maxime Labonne Fine tune Llama 3.1 Ultra Efficiently with Unsloth Maxime Labonne __LLM Course __Books __Research __About __ __ __ __ 1. LLM Post training 2. Fine tune Llama 3.1 8B 1. LLM Post training 2. Fine tune Llama 3.1 8B Fine tune Llama 3.1 Ultra Efficiently with Unsloth A beginner s guide to state of the art supervised fine tuning Large Language Models Author Maxime Lbonne Published July 29, 2024 LLM Post training __ Fine tune Llama 2 in Colab Fine tune Llama 2 in Axolotl Fine tune Mistral 7b with DPO Fine tune Llama 3 with ORPO Fine tune Llama 3.1 8B Merge LLMs with mergekit Create Mixture of Experts Uncensor any LLM LLM Quantization __ Intro to Quantization Quantization with GPTQ Quantization with GGML Quantization with ExLlamaV2 LLM stuff __ ChatGPT KG Decoding Strategies Agentic data generation Graph neural networks __ Graph Convolution Network Graph Attention Network GraphSAGE Graph Isomorphism Network Linear programming __ Linear Programming Integer Programming Constraint Programming Nonlinear Programming Miscellaneous __ Q learning Minecraft Bot Loops in Pandas What is a Tensor Sections Supervised Fine Tuning SFT Techniques Fine Tune Llama 3.1 8B Conclusion Check out the LLM Engineer s Handbook to master the art of LLMs from fine tuning to deployment The recent release of Llama 3.1 offers models with an incredible level of performance, closing the gap between closed source and open weight models. Instead of using frozen, general purpose LLMs like GPT 4o and Claude 3.5, you can fine tune Llama 3.1 for your specific use cases to achieve better performance and customizability at a lower cost. In this article, we will provide a comprehensive overview of supervised fine tuning. We will compare it to prompt engineering to understand when it makes sense to use it, detail the main techniques with their pros and cons, and introduce major concepts, such as LoRA hyperparameters, storage formats, and chat templates. Finally, we will implement it in practice by fine tuning Llama 3.1 8B in Google Colab with state of the art optimization using Unsloth. All the code used in this article is available on Google Colab and in the LLM Course. Special thanks to Daniel Han for answering my questions. Supervised Fine Tuning Supervised Fine Tuning SFT is a method to improve and customize pre trained LLMs. It involves retraining base models on a smaller dataset of instructions and answers. The main goal is to transform a basic model that predicts text into an assistant that can follow instructions and answer questions. SFT can also enhance the model s overall performance, add new knowledge, or adapt it to specific tasks and domains. Fine tuned models can then go through an optional preference alignment stage see my article about DPO to remove unwanted responses, modify their style, and more. The following figure shows an instruction sample. It includes a system prompt to steer the model, a user prompt to provide a task, and the output the model is expected to generate. You can find a list of high quality open source instruction datasets in the LLM Datasets GitHub repo. Before considering SFT, I recommend trying prompt engineering techniques like few shot prompting or retrieval augmented generation RAG . In practice, these methods can solve many problems without the need for fine tuning, using either closed source or open weight models e.g., Llama 3.1 Instruct . If this approach doesn t meet your objectives in terms of quality, cost, latency, etc. , then SFT becomes a viable option when instruction data is available. Note that SFT also offers benefits like additional control and customizability to create personalized LLMs. However, SFT has limitations. It works best when leveraging knowledge already present in the base model. Learning completely new information like an unknown language can be challenging and lead to more frequent hallucinations. For new domains unknown to the base model, it is recommended to continuously pre train it on a raw dataset first. On the opposite end of the spectrum, instruct models i.e., already fine tuned models can already be very close to your requirements. For example, a model might perform very well but state that it was trained by OpenAI or Meta instead of you. In this case, you might want to slightly steer the instruct model s behavior using preference alignment. By providing chosen and rejected samples for a small set of instructions between 100 and 1000 samples , you can force the LLM to say that you trained it instead of OpenAI. SFT Techniques The three most popular SFT techniques are full fine tuning, LoRA, and QLoRA. Full fine tuning is the most straightforward SFT technique. It involves retraining all parameters of a pre trained model on an instruction dataset. This method often provides the best results but requires significant computational resources several high end GPUs are required to fine tune a 8B model . Because it modifies the entire model, it is also the most destructive method and can lead to the catastrophic forgetting of previous skills and knowledge. Low Rank Adaptation LoRA is a popular parameter efficient fine tuning technique. Instead of retraining the entire model, it freezes the weights and introduces small adapters low rank matrices at each targeted layer. This allows LoRA to train a number of parameters that is drastically lower than full fine tuning less than 1 , reducing both memory usage and training time. This method is non destructive since the original parameters are frozen, and adapters can then be switched or combined at will. QLoRA Quantization aware Low Rank Adaptation is an extension of LoRA that offers even greater memory savings. It provides up to 33 additional memory reduction compared to standard LoRA, making it particularly useful when GPU memory is constrained. This increased efficiency comes at the cost of longer training times, with QLoRA typically taking about 39 more time to train than regular LoRA. While QLoRA requires more training time, its substantial memory savings can make it the only viable option in scenarios where GPU memory is limited. For this reason, this is the technique we will use in the next section to fine tune a Llama 3.1 8B model on Google Colab. Fine Tune Llama 3.1 8B To efficiently fine tune a Llama 3.1 8B model, we ll use the Unsloth library by Daniel and Michael Han. Thanks to its custom kernels, Unsloth provides 2x faster training and 60 memory use compared to other options, making it ideal in a constrained environment like Colab. Unfortunately, Unsloth only supports single GPU settings at the moment. For multi GPU settings, I recommend popular alternatives like TRL and Axolotl both also include Unsloth as a backend . In this example, we will QLoRA fine tune it on the mlabonne FineTome 100k dataset. It s a subset of arcee ai The Tome without arcee ai qwen2 72b magpie en that I re filtered using HuggingFaceFW fineweb edu classifier. Note that this classifier wasn t designed for instruction data quality evaluation, but we can use it as a rough proxy. The resulting FineTome is an ultra high quality dataset that includes conversations, reasoning problems, function calling, and more. Let s start by installing all the required libraries. !pip install unsloth colab new git https github.com unslothai unsloth.git !pip install no deps xformers 0.0.27 trl 0.9.0 peft accelerate bitsandbytes __ Once installed, we can import them as follows. import torch from trl import SFTTrainer from datasets import load_dataset from transformers import TrainingArguments, TextStreamer from unsloth.chat_templates import get_chat_template from unsloth import FastLanguageModel, is_bfloat16_supported __ Let s now load the model. Since we want to use QLoRA, I chose the pre quantized unsloth Meta Llama 3.1 8B bnb 4bit. This 4 bit precision version of meta llama Meta Llama 3.1 8B is significantly smaller 5.4 GB and faster to download compared to the original 16 bit precision model 16 GB . We load in NF4 format using the bitsandbytes library. When loading the model, we must specify a maximum sequence length, which restricts its context window. Llama 3.1 supports up to 128k context length, but we will set it to 2,048 in this example since it consumes more compute and VRAM. Finally, the dtype parameter automatically detects if your GPU supports the BF16 format for more stability during training this feature is restricted to Ampere and more recent GPUs . max_seq_length 2048 model, tokenizer FastLanguageModel.from_pretrained model_name unsloth Meta Llama 3.1 8B bnb 4bit , max_seq_length max_seq_length, load_in_4bit True, dtype None, __ Now that our model is loaded in 4 bit precision, we want to prepare it for parameter efficient fine tuning with LoRA adapters. LoRA has three important parameters Rank r , which determines LoRA matrix size. Rank typically starts at 8 but can go up to 256. Higher ranks can store more information but increase the computational and memory cost of LoRA. We set it to 16 here. Alpha \u03b1 , a scaling factor for updates. Alpha directly impacts the adapters contribution and is often set to 1x or 2x the rank value. Target modules LoRA can be applied to various model components, including attention mechanisms Q, K, V matrices , output projections, feed forward blocks, and linear output layers. While initially focused on attention mechanisms, extending LoRA to other components has shown benefits. However, adapting more modules increases the number of trainable parameters and memory needs. Here, we set r 16, \u03b1 16, and target every linear module to maximize quality. We don t use dropout and biases for faster training. In addition, we will use Rank Stabilized LoRA rsLoRA , which modifies the scaling factor of LoRA adapters to be proportional to 1 r instead of 1 r. This stabilizes learning especially for higher adapter ranks and allows for improved fine tuning performance as rank increases. Gradient checkpointing is handled by Unsloth to offload input and output embeddings to disk and save VRAM. model FastLanguageModel.get_peft_model model, r 16, lora_alpha 16, lora_dropout 0, target_modules q_proj , k_proj , v_proj , up_proj , down_proj , o_proj , gate_proj , use_rslora True, use_gradient_checkpointing unsloth __ With this LoRA configuration, we ll only train 42 million out of 8 billion parameters 0.5196 . This shows how much more efficient LoRA is compared to full fine tuning. Let s now load and prepare our dataset. Instruction datasets are stored in a particular format it can be Alpaca, ShareGPT, OpenAI, etc. First, we want to parse this format to retrieve our instructions and answers. Our mlabonne FineTome 100k dataset uses the ShareGPT format with a unique conversations column containing messages in JSONL. Unlike simpler formats like Alpaca, ShareGPT is ideal for storing multi turn conversations, which is closer to how users interact with LLMs. Once our instruction answer pairs are parsed, we want to reformat them to follow a chat template . Chat templates are a way to structure conversations between users and models. They typically include special tokens to identify the beginning and the end of a message, who s speaking, etc. Base models don t have chat templates so we can choose any ChatML, Llama3, Mistral, etc. In the open source community, the ChatML template originally from OpenAI is a popular option. It simply adds two special tokens im_start and im_end to indicate who s speaking. If we apply this template to the previous instruction sample, here s what we get im_start system You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old. im_end im_start user Remove the spaces from the following sentence It prevents users to suspect that there are some hidden products installed on theirs device. im_end im_start assistant Itpreventsuserstosuspectthattherearesomehiddenproductsinstalledontheirsdevice. im_end In the following code block, we parse our ShareGPT dataset with the mapping parameter and include the ChatML template. We then load and process the entire dataset to apply the chat template to every conversation. tokenizer get_chat_template tokenizer, mapping role from , content value , user human , assistant gpt , chat_template chatml , def apply_template examples messages examples conversations text tokenizer.apply_chat_template message, tokenize False, add_generation_prompt False for message in messages return text text dataset load_dataset mlabonne FineTome 100k , split train dataset dataset.map apply_template, batched True __ We re now ready to specify the training parameters for our run. I want to briefly introduce the most important hyperparameters Learning rate It controls how strongly the model updates its parameters. Too low, and training will be slow and may get stuck in local minima. Too high, and training may become unstable or diverge, which degrades performance. LR scheduler It adjusts the learning rate LR during training, starting with a higher LR for rapid initial progress and then decreasing it in later stages. Linear and cosine schedulers are the two most common options. Batch size Number of samples processed before the weights are updated. Larger batch sizes generally lead to more stable gradient estimates and can improve training speed, but they also require more memory. Gradient accumulation allows for effectively larger batch sizes by accumulating gradients over multiple forward backward passes before updating the model. Num epochs The number of complete passes through the training dataset. More epochs allow the model to see the data more times, potentially leading to better performance. However, too many epochs can cause overfitting. Optimizer Algorithm used to adjust the parameters of a model to minimize the loss function. In practice, AdamW 8 bit is strongly recommended it performs as well as the 32 bit version while using less GPU memory. The paged version of AdamW is only interesting in distributed settings. Weight decay A regularization technique that adds a penalty for large weights to the loss function. It helps prevent overfitting by encouraging the model to learn simpler, more generalizable features. However, too much weight decay can impede learning. Warmup steps A period at the beginning of training where the learning rate is gradually increased from a small value to the initial learning rate. Warmup can help stabilize early training, especially with large learning rates or batch sizes, by allowing the model to adjust to the data distribution before making large updates. Packing Batches have a pre defined sequence length. Instead of assigning one batch per sample, we can combine multiple small samples in one batch, increasing efficiency. I trained the model on the entire dataset 100k samples using an A100 GPU 40 GB of VRAM on Google Colab. The training took 4 hours and 45 minutes. Of course, you can use smaller GPUs with less VRAM and a smaller batch size, but they re not nearly as fast. For example, it takes roughly 19 hours and 40 minutes on an L4 and a whopping 47 hours on a free T4. In this case, I recommend only loading a subset of the dataset to speed up training. You can do it by modifying the previous code block, like dataset load_dataset mlabonne FineTome 100k , split train 10000 to only load 10k samples. Alternatively, you can use cheaper cloud GPU providers like Paperspace, RunPod, or Lambda Labs. trainer SFTTrainer model model, tokenizer tokenizer, train_dataset dataset, dataset_text_field text , max_seq_length max_seq_length, dataset_num_proc 2, packing True, args TrainingArguments learning_rate 3e 4, lr_scheduler_type linear , per_device_train_batch_size 8, gradient_accumulation_steps 2, num_train_epochs 1, fp16 not is_bfloat16_supported , bf16 is_bfloat16_supported , logging_steps 1, optim adamw_8bit , weight_decay 0.01, warmup_steps 10, output_dir output , seed 0, , trainer.train __ Now that the model is trained, let s test it with a simple prompt. This is not a rigorous evaluation but just a quick check to detect potential issues. We use FastLanguageModel.for_inference to get 2x faster inference. model FastLanguageModel.for_inference model messages from human , value Is 9.11 larger than 9.9? , inputs tokenizer.apply_chat_template messages, tokenize True, add_generation_prompt True, return_tensors pt , .to cuda text_streamer TextStreamer tokenizer _ model.generate input_ids inputs, streamer text_streamer, max_new_tokens 128, use_cache True __ The model s response is 9.9 , which is correct! Let s now save our trained model. If you remember the part about LoRA and QLoRA, what we trained is not the model itself but a set of adapters. There are three save methods in Unsloth lora to only save the adapters, and merged_16bit merged_4bit to merge the adapters with the model in 16 bit 4 bit precision. In the following, we merge them in 16 bit precision to maximize the quality. We first save it locally in the model directory and then upload it to the Hugging Face Hub. You can find the trained model on mlabonne FineLlama 3.1 8B. model.save_pretrained_merged model , tokenizer, save_method merged_16bit model.push_to_hub_merged mlabonne FineLlama 3.1 8B , tokenizer, save_method merged_16bit __ Unsloth also allows you to directly convert your model into GGUF format. This is a quantization format created for llama.cpp and compatible with most inference engines, like LM Studio, Ollama, and oobabooga s text generation webui. Since you can specify different precisions see my article about GGUF and llama.cpp , we ll loop over a list to quantize it in q2_k , q3_k_m , q4_k_m , q5_k_m , q6_k , q8_0 and upload these quants on Hugging Face. The mlabonne FineLlama 3.1 8B GGUF contains all our GGUFs. quant_methods q2_k , q3_k_m , q4_k_m , q5_k_m , q6_k , q8_0 for quant in quant_methods model.push_to_hub_gguf mlabonne FineLlama 3.1 8B GGUF , tokenizer, quant __ Congratulations, we fine tuned a model from scratch and uploaded quants you can now use in your favorite inference engine. Feel free to try the final model available on mlabonne FineLlama 3.1 8B GGUF. What to do now? Here are some ideas on how to use your model Evaluate it on the Open LLM Leaderboard you can submit it for free or using other evals like in LLM AutoEval. Align it with Direct Preference Optimization using a preference dataset like mlabonne orpo dpo mix 40k to boost performance. Quantize it in other formats like EXL2, AWQ, GPTQ, or HQQ for faster inference or lower precision using AutoQuant. Deploy it on a Hugging Face Space with ZeroChat for models that have been sufficiently trained to follow a chat template 20k samples . Conclusion This article provided a comprehensive overview of supervised fine tuning and how to apply it in practice to a Llama 3.1 8B model. By leveraging QLoRA s efficient memory usage, we managed to fine tune an 8B LLM on a super high quality dataset with limited GPU resources. We also provided more efficient alternatives for bigger runs and suggestions for further steps, including evaluation, preference alignment, quantization, and deployment. I hope this guide was useful. If you re interested in learning more about LLMs, I recommend checking the LLM Course. If you enjoyed this article, follow me on X maximelabonne and on Hugging Face mlabonne. Good luck fine tuning models! __Copyright 2023, Maxime Labonne en",
            "platform": "mlabonne.github.io",
            "author_id": "c7167a83-e446-428d-9a2e-6e88a6b8bc6f",
            "author_full_name": "Hiep Nguyen",
            "link": "https://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.html"
        },
        {
            "id": "330ec905-27fe-4518-b8f0-6752673764aa",
            "content": "Maxime Labonne The Rise of Agentic Data Generation Maxime Labonne __LLM Course __Books __Research __About __ __ __ __ 1. LLM stuff 2. Agentic data generation 1. LLM stuff 2. Agentic data generation The Rise of Agentic Data Generation Combining AgentInstruct and Arena Learning Large Language Models Author Maxime Lbonne Published July 15, 2024 LLM Post training __ Fine tune Llama 2 in Colab Fine tune Llama 2 in Axolotl Fine tune Mistral 7b with DPO Fine tune Llama 3 with ORPO Fine tune Llama 3.1 8B Merge LLMs with mergekit Create Mixture of Experts Uncensor any LLM LLM Quantization __ Intro to Quantization Quantization with GPTQ Quantization with GGML Quantization with ExLlamaV2 LLM stuff __ ChatGPT KG Decoding Strategies Agentic data generation Graph neural networks __ Graph Convolution Network Graph Attention Network GraphSAGE Graph Isomorphism Network Linear programming __ Linear Programming Integer Programming Constraint Programming Nonlinear Programming Miscellaneous __ Q learning Minecraft Bot Loops in Pandas What is a Tensor Sections AgentInstruct A Multi Agent Approach Arena Learning A Competitive Refinement Approach ArenaInstruct Combining AgentInstruct and Arena Learning Conclusion Check out the LLM Engineer s Handbook to master the art of LLMs from fine tuning to deployment With the consolidation of LLM architectures, the quality of training data has become the most important factor in creating state of the art models. This is true for both pre training and post training, where instruction datasets have a major impact on the final model. Two innovative approaches have recently emerged to address the challenge of generating high quality instruction datasets for post training LLMs AgentInstruct and Arena Learning. Both frameworks come from Microsoft Research and leverage multiple LLMs to create and refine samples. In this article, I want to explore both methods, analyze their similarities and differences, and see how we could combine them in a single end to end framework. AgentInstruct A Multi Agent Approach AgentInstruct is an agentic framework by Mitra et al. 2024 , designed to generate large scale, diverse, and high quality synthetic data. The framework uses a sophisticated pipeline that transforms raw text into refined instructions through multiple stages of processing. In the paper, the agents seem to be based on GPT 4, which is also used to evaluate data quality and hallucinations in some contexts. _Figure from the AgentInstruct paper._ The AgentInstruct pipeline consists of four main steps Seed Collection Assemble a diverse collection of raw seeds, such as textbook chapters, web articles, and code snippets. These seeds serve as the foundation for generating new instructions. Content Transformation One or more specialized agents modify each seed into an intermediate representation that simplifies instruction creation. These agents are designed to perform tasks like generating argument passages, debates, conversations, meeting transcripts, poems, satirical content, etc. Seed Instruction Generation Multiple agents take the transformed seed and generate diverse instructions based on a pre defined taxonomy of instruction types. For example, in the domain of reading comprehension, the taxonomy includes 43 question types, ranging from literal comprehension to critical analysis and inference. Instruction Refinement The final stage involves iteratively enhancing the complexity and quality of the generated instructions. This is achieved through suggester editor agent pairs. Suggester agents propose ways to increase instruction complexity, while editor agents modify the instructions accordingly. To get a better idea of what each stage produces, I recommend reading the examples provided in the paper. Each flow in the AgentInstruct pipeline consists of multiple agents powered by LLMs. These agents can be equipped with tools like search APIs or code interpreters to enhance their capabilities. The roles of these agents are carefully defined in their system messages to ensure they perform their specific tasks effectively. The authors of AgentInstruct implemented flows for 17 different skills, each with multiple subcategories. These skills cover a wide range of areas, including reading comprehension, question answering, coding, retrieval augmented generation, creative writing, tool use, and web control. Using this comprehensive pipeline, the researchers generated approximately 22 million instructions. They combined this synthetic data with 3.8 million instructions from other sources to create a dataset of 25.8 million paired instructions. This dataset was then used to fine tune the Mistral 7b model, resulting in the creation of the Orca 3 model. Arena Learning A Competitive Refinement Approach Arena Learning by Luo, Suo, et al. 2024 takes a different approach to generating high quality instruction data. Instead of creating instructions from scratch, it focuses on refining existing instruction datasets through a simulated competitive environment. It is not an agentic framework because tools are not provided to the models, but could easily be transformed into one. _Figure from the Arena Learning paper._ The key components of the Arena Learning pipeline are Offline Pairwise LLM Arena Arena Learning creates a simulated arena where multiple LLMs compete against each other on a large set of instruction data. A judge LLM meta llama Meta Llama 3 70B Instruct evaluates the responses from competing models for each instruction, providing rankings, scores, and explanations. This process effectively simulates human evaluation but at a much larger scale and lower cost. Data Collection and Preprocessing The framework starts with a large corpus of conversational data collected from various open sources. This data goes through filtering, cleaning, and deduplication. Instructions that are too short, illegal toxic, or too similar to benchmark test sets are removed. The refined dataset is then split into multiple parts for iterative training. Iterative Battle and Model Evolution The process involves multiple rounds of battles and training 1. An initial model WizardLM \u03b2 SFT I0 is trained on a subset of data. 2. This model competes against other state of the art LLMs on another data subset. 3. Instances where WizardLM \u03b2 loses are collected, with the winning model s response used as the target for fine tuning. 4. The process repeats for multiple iterations, with each iteration potentially using different training strategies SFT, DPO, PPO . Training Strategies Arena Learning employs multiple training strategies to improve the model _Supervised Fine Tuning SFT _ Uses battle results to fine tune the model on instances where it performed poorly. _Direct Preference Optimization DPO _ Treats win loss responses as choice reject pairs for training. _Proximal Policy Optimization PPO _ Uses battle results to train both a reward model and the language model. WizardArena Evaluation The authors create an offline test set WizardArena with diverse and hard subsets. This is used to evaluate models through pairwise battles, with results used to compute Elo rankings. The evaluation closely aligns with human based arenas but is much faster and cheaper. Data Selection The pipeline uses various strategies to select high quality training data, such as threshold based filtering to control data size and quality, focusing on instances where the model underperforms, and gradually shifting towards more complex data in later iterations. _Figure from the Arena Learning paper._ This framework allows for multiple iterations of battles and training, as illustrated with WizardLM \u03b2. The model s capabilities are progressively strengthened, particularly in complex tasks. The process results in significant gains in Elo rankings, MT bench scores, and other evaluation metrics. Arena Learning focuses on improving areas where the model under training is currently lacking. A nice feature is that it doesn t require particularly powerful models like Claude 3.5 Sonnet or GPT 4o. Models with a similar level can be better in some tasks and domains, as well as more suited to answer certain prompt syntaxes. It means that the entire pipeline can be deployed using open weight models, which is a big advantage if you already have a high quality infrastructure. ArenaInstruct Combining AgentInstruct and Arena Learning While both AgentInstruct and Arena Learning aim to generate high quality data for post training language models, they take fundamentally different approaches to achieve this goal. Understanding how they differ, as well as their strengths and weaknesses is a good first step to see how we could combine them. I selected four points I want to focus on Data Generation AgentInstruct starts from raw text, generating instructions from scratch through a multi stage pipeline. This allows for the creation of entirely new content, potentially leading to greater diversity and novelty in the generated instructions. On the other hand, Arena Learning refines existing instruction datasets through simulated battles between models. This method leverages the quality of existing datasets while improving upon them through competitive evaluation. Data Quality AgentInstruct relies on suggester editor agent pairs for iterative refinement of instructions. This approach allows for fine grained control over the complexity and quality of generated instructions. Arena Learning, in contrast, uses an LLM as a judge to evaluate responses in simulated battles. It means that the entire data quality process is handled by a single model. Diversity and Complexity AgentInstruct explicitly i.e., manually designs for diversity through a taxonomy of instruction types and multiple transformation agents. This structured approach ensures coverage across a wide range of skills and instruction types. Arena Learning s diversity comes from the variety of competing models and initial instruction datasets. While this may lead to less structured diversity, it could potentially capture more natural variations in instruction styles. Flexibility AgentInstruct s pipeline allows for easy addition of new seed types and instruction categories, making it highly adaptable to new domains and tasks. Arena Learning s iterative battle process enables continuous improvement of the target model, potentially allowing it to adapt more quickly to new challenges and competing models. Based on this comparison, it s not too difficult to see how we can leverage the advantages of each framework. For instance, a taxonomy based data generation is more steerable and could be improved upon by arena learning. But we could also use feedback signals to improve this first step over multiple iterations. Here s how such a hybrid approach might work 1. AgentInstruct Instruction Generation Use AgentInstruct to create a broad and diverse base of instructions no answers! from raw text. This would ensure wide coverage of tasks and domains that are relevant for our use cases. 2. Arena Learning Answer Generation Apply Arena Learning s competitive battle approach to refine and select the highest quality answers from a pool of models. This would combine AgentInstruct s ability to generate novel content with Arena Learning s robust quality control mechanism. 3. Data Quality Evaluation Instead of relying on a single LLM as a judge, we can use reward models or an LLM as a jury to improve the data selection process. 4. Diversity Feedback Use insights from Arena Learning battles to dynamically update AgentInstruct s instruction taxonomy. This would focus the generation process on producing more of the instruction types that prove most challenging or useful in real world scenarios. 5. Complexity Feedback Leverage Arena Learning s performance metrics to identify areas where instructions are too easy or too difficult. Use this information to guide AgentInstruct s complexity refinement process, ensuring a well balanced dataset that challenges the model appropriately over several iterations. By combining these approaches, we can create a powerful feedback loop between instruction generation and evaluation. This hybrid framework would benefit from AgentInstruct s ability to generate novel, diverse content and Arena Learning s competitive quality control and model improvement process. The result would be a more robust, effective, and continuously improving post training dataset for LLMs. Conclusion In conclusion, this article explored two recent approaches in synthetic data generation AgentInstruct and Arena Learning. We proposed a hybrid solution that combines AgentInstruct s structured, taxonomy based methodology with Arena Learning s iterative refinement using multiple LLMs. This combination leverages the strengths of both frameworks, allowing for a systematic generation of diverse data while enabling continuous improvement of the underlying taxonomy through feedback from the LLM pool. I feel like we might lose some quality by removing the suggester editor agent pairs. Let me know if you have better ideas. Still, data quality evaluation is a significant challenge to perfect this approach. The current reliance on models like GPT 4 or Llama 3 70B Instruct as judges is imperfect and has known limitations see my quick review here . Improving the quality assessment stage could lead to more efficient datasets, achieving better performance with fewer samples. To know more about how to create high quality datasets, check out my GitHub repo LLM Datasets. __Copyright 2023, Maxime Labonne en",
            "platform": "mlabonne.github.io",
            "author_id": "c7167a83-e446-428d-9a2e-6e88a6b8bc6f",
            "author_full_name": "Hiep Nguyen",
            "link": "https://mlabonne.github.io/blog/posts/2024-07-15_The_Rise_of_Agentic_Data_Generation.html"
        }
    ]
}