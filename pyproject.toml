[tool.poetry]
name = "llmops-practice"
version = "0.1.0"
description = "Study LLM Engineering Handbook"
authors = ["Hiep Nguyen"]

[tool.poetry.dependencies]
python = "^3.11"
requests = "^2.25.1"
numpy = "^1.19.5"
poethepoet = "^0.31.1"

[tool.poetry.dev-dependencies]

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"

[tool.poe.tasks]
# Data pipelines
run-digital-data-etl = "poetry run python -m tools.run --run-etl --no-cache --etl-config-filename digital_data_etl.yaml"
run-feature-engineering-pipeline = "poetry run python -m tools.run --no-cache --run-feature-engineering"

# Utility pipelines
run-export-artifact-to-json-pipeline = "poetry run python -m tools.run --no-cache --run-export-artifact-to-json"
run-export-data-warehouse-to-json = "poetry run python -m tools.data_warehouse --export-raw-data"
run-import-data-warehouse-from-json = "poetry run python -m tools.data_warehouse --import-raw-data"

# Training pipelines
run-training-pipeline = "poetry run python -m tools.run --no-cache --run-training"
run-evaluation-pipeline = "poetry run python -m tools.run --no-cache --run-evaluation"

# Inference
call-rag-retrieval-module = "poetry run python -m tools.rag"
run-inference-ml-service = "poetry run uvicorn tools.ml_service:app --host 0.0.0.0 --port 8000 --reload"
call-inference-ml-service = "curl -X POST 'http://127.0.0.1:8000/rag' -H 'Content-Type: application/json' -d '{\"query\": \"My name is Hiep Nguyen. Could you draft a LinkedIn post discussing RAG systems? I am particularly interested in how RAG works and how it is integrated with vector DBs and LLMs.\"}'"

# Infrastructure
## Local infrastructure
local-docker-up = "docker compose up -d"
local-docker-down = "docker compose stop"
local-zenml-up = "zenml login --local"
local-zenml-down = "zenml down"

local-infrastructure-up = [
    "local-docker-up",
    "local-zenml-up",
]

local-infrastructure-down = [
    "local-docker-down",
    "local-zenml-down",
]

## SageMaker
create-sagemaker-role = "poetry run python -m llm_twin.infrastructure.aws.roles.create_sagemaker_role"
create-sagemaker-execution-role = "poetry run python -m llm_twin.infrastructure.aws.roles.create_sagemaker_execution_role"
deploy-inference-endpoint = "poetry run python -m llm_twin.infrastructure.aws.deploy.huggingface.run"
test-sagemaker-endpoint = "poetry run python -m llm_twin.model.inference.test"
delete-inference-endpoint = "poetry run python -m llm_twin.infrastructure.aws.deploy.delete_sagemaker_endpoint"
