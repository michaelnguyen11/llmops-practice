from functools import cached_property
from pathlib import Path
from typing import Optional

import numpy as np
from numpy.typing import NDArray
from loguru import logger

from sentence_transformers.SentenceTransformer import SentenceTransformer
from sentence_transformers.cross_encoder import CrossEncoder
from transformers import AutoTokenizer

from llm_twin.settings import settings

from .base import SingletonMeta


class EmdeddingModelSingleton(metaclass=SingletonMeta):
    """
    A Singleton class that provides a pre-trained transformer model for generating embeddings of text.
    """

    def __init__(
        self,
        model_id: str = settings.TEXT_EMBEDDING_MODEL_ID,
        device: str = settings.RAG_MODEL_DEVICE,
        cached_dir: Optional[Path] = None,
    ) -> None:

        self._model_id = model_id
        self._device = device

        self._model = SentenceTransformer(
            self._model_id,
            device=self._device,
            cache_folder=str(cached_dir) if cached_dir else None,
        )

        self._model.eval()

    @property
    def model_id(self) -> str:
        """
        Return the identifier of the pre-trained transformer model to use.
        """
        return self._model_id

    @cached_property
    def embedding_size(self) -> int:
        """
        Return the size of the embeddings generated by the pre-trained transformer model.
        """

        dummy_embedding = self._model.encode("")
        return dummy_embedding.shape[0]

    @property
    def max_input_length(self) -> int:
        """
        Return the maximum length of input text to tokenize.
        """

        return self._model.max_seq_length

    @property
    def tokenizer(self) -> AutoTokenizer:
        """
        Return the tokenizer used to tokenize the input text.
        """
        return self._model.tokenizer

    def __call__(
        self, input_text: str | list[str], to_list: bool = True
    ) -> NDArray[np.float32] | list[float] | list[list[float]]:
        """
        Generates embeddings for the input text using the pre-trained transformer model.

        Args:
            input_text (str or list of str): The input text to generate embeddings for.
            to_list (bool, optional): If True, returns a list of embeddings. Otherwise, returns a numpy array. Defaults to True.

        Returns:
            NDArray[np.float32] or list[float] or list[list[float]]: The generated embeddings for the input text.
        """

        try:
            embeddings = self._model.encode(input_text)
        except Exception:
            logger.error(
                f"Error generating embeddings for {self._model_id=} and {input_text=}"
            )
            return [] if to_list else np.array([])

        if to_list:
            embeddings = embeddings.tolist()

        return embeddings


class CrossEncoderModelSingleton(metaclass=SingletonMeta):
    """
    A Singleton class that provides a pre-trained cross-encoder model for scoring pairs of input text.
    """

    def __init__(
        self,
        model_id: str = settings.RERANKING_CROSS_ENCODER_MODEL_ID,
        device: str = settings.RAG_MODEL_DEVICE,
    ) -> None:
        self._model_id = model_id
        self._device = device

        self._model = CrossEncoder(
            model_name=self._model_id,
            device=self._device,
        )
        self._model.model.eval()

    def __call__(
        self, pairs: list[tuple[str, str]], to_list: bool = True
    ) -> NDArray[np.float32] | list[float]:
        """
        Scores pairs of input text using the pre-trained cross-encoder model.
        """
        scores = self._model.predict(pairs)

        if to_list:
            scores = scores.tolist()

        return scores
